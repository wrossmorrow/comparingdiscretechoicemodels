{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Several Discrete Choice Models\n",
    "\n",
    "- - -\n",
    "\n",
    "W. Ross Morrow ([wrossmorrow@stanford.edu](mailto:wrossmorrow@stanford.edu), [wrossmorrow.com](https://wrossmorrow.com))\n",
    "\n",
    "Research Analytics Consultant, Stanford GSB\n",
    "\n",
    "February 5th, 2019\n",
    "\n",
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "\n",
    "- - - - \n",
    "\n",
    "In this notebook we'll compare several Discrete Choice Models in a simple situation: single-feature binary choice data. Specifically, the data will be composed of a number of observations, $N$, a number of individuals, $I$, an individual index per observations $i_n$, and a \"yes/no\" dummy $y_n \\in \\{\\pm 1\\}$ standing in for random (but structured) choices. We can draw those choices with a variety of data generating processes, as discussed below. But the basic setup is that we see some number of repeated trials for different individuals in which they answer \"yes\" or \"no\" to some question, and our broad goal is to model the frequency with which respondents say \"yes\" or \"no\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, lets do our basic imports and function definitions. We'll discuss the models for data $(N,I,\\mathbf{i},\\mathbf{y})$ below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import time\n",
    "from multiprocessing import Pool , Queue , Process\n",
    "from multiprocessing.pool import ThreadPool\n",
    "\n",
    "import string\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import rand , randn , randint , choice\n",
    "\n",
    "import cvxpy as cp\n",
    "import ecos\n",
    "\n",
    "from scipy.sparse import csc_matrix , csr_matrix , coo_matrix , issparse\n",
    "from scipy.optimize import minimize , LinearConstraint , BFGS\n",
    "from scipy.stats import norm as gaussian , normaltest\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Colormap\n",
    "%matplotlib notebook\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are some wrappers around random number generation that are useful here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def randi(A,N) : return randint(0,high=A,size=N) \n",
    "\n",
    "def rands(N) : return np.sign( 2.0*rand(N) - 1.0 )\n",
    "\n",
    "def randc(C,N,p) : return choice( C , size=N , p=p )\n",
    "\n",
    "def randp(A,N) : \n",
    "    L , R = randi(A,N) , randi(A-1,N)\n",
    "    R[ np.where( R >= L )[0] ] += 1\n",
    "    return L , R\n",
    "\n",
    "def random_string( l ) : \n",
    "    return ''.join( random.choice(string.ascii_letters) for m in range(l) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a nice simple way to plot sparsity patterns for not-too-large sparse matrices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def spy( A , b=None ) : \n",
    "    plt.figure( figsize=(10,3) )\n",
    "    plt.xticks([]) ; plt.yticks([])\n",
    "    if issparse(A) : A = A.toarray()\n",
    "    if b is None : \n",
    "        plt.imshow( A != 0 , cmap='Greys' , interpolation='nearest', aspect='auto' )\n",
    "    else : \n",
    "        X = np.zeros( ( A.shape[0] , A.shape[1]+4 ) )\n",
    "        X[:,:A.shape[1]] = A\n",
    "        X[:,A.shape[1]+3] = b\n",
    "        plt.imshow( X != 0 , cmap='Greys' , interpolation='nearest', aspect='auto' )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "- - -\n",
    "\n",
    "Below we review Logit, Latent Class Logit, Random Coefficient Logit, and `idLogit` models and provide code for this specific (and rather simple) situation. \n",
    "\n",
    "First, however, we define a general class `FittableModel` each model-specific class will be derived from. This class has a timeout-enabled fitting method as well as derivative checking routines for gradients and Hessian-vector products. The timeout-enabled fitting method is optional, but is helpful given that the models we try to fit can have dramatically different fit times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FittableModel( object ) : \n",
    "    \n",
    "    def __init__( self ) : \n",
    "        pass\n",
    "    \n",
    "    def fit( self , p0=None , maxtime=None ) : \n",
    "        \n",
    "        if p0 is None : \n",
    "            try : \n",
    "                p0 = self.draw_initial_condition(  )\n",
    "            except AttributeError as e : \n",
    "                p0 = randn( self.Nvars )\n",
    "        else : \n",
    "            p0 = p0.flatten()\n",
    "            \n",
    "        start = time.time()\n",
    "        \n",
    "        if ( maxtime is None ) or ( maxtime <= 0.0 ) : \n",
    "            \n",
    "            self.soln = self.wrapped_solve( p0=p0 )\n",
    "            self.soln['timeout'] = False\n",
    "            \n",
    "        else : \n",
    "            \n",
    "            q = Queue()\n",
    "            p = Process( target=self.wrapped_solve , kwargs={ 'p0' : p0 , 'queue' : q } )\n",
    "            \n",
    "            p.start()\n",
    "            while time.time() - start < maxtime :\n",
    "                p.join( timeout=1 )\n",
    "                if not p.is_alive() : break\n",
    "            if p.is_alive():\n",
    "                p.terminate()\n",
    "                self.soln = { 'timeout' : True }\n",
    "            else : \n",
    "                self.soln = q.get()\n",
    "                self.soln['timeout'] = False\n",
    "                \n",
    "        self.soln['solvertime'] = time.time() - start\n",
    "        self.soln['x0'] = p0\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def wrapped_solve( self , p0=None , queue=None ) :\n",
    "        try : \n",
    "            self.soln = self.solve( p0=p0 )\n",
    "            self.soln['error'] = False\n",
    "        except Exception as e : \n",
    "            print( \"caught solve exception: \" , e )\n",
    "            self.soln = { 'error' : True , 'message' : e }\n",
    "        if queue is not None : queue.put( self.soln )\n",
    "        return self.soln\n",
    "    \n",
    "    def grad_check( self , p=None , verbose=True ) : \n",
    "\n",
    "        \"\"\"\n",
    "        gradient check of an object that has certain attributes (Nvars, obj, and grad)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if p is None : p = rand( self.Nvars )\n",
    "\n",
    "        f0 , g0 = self.obj( p ) , self.grad( p ) # original objective and gradient\n",
    "\n",
    "        Hs = 10.0**np.arange( -10 , 0 , 1 )[::-1] # perturbation sizes\n",
    "        pP = p.copy() # copy for perturbed betas\n",
    "\n",
    "        df = np.zeros( self.Nvars ) # space for dinite differences\n",
    "        ds = np.zeros( Hs.size ) # differences between gradient and finite differences\n",
    "        \n",
    "        # iterations\n",
    "        for h in range( Hs.size ) : \n",
    "            H = Hs[h] # actual perturbation\n",
    "            for k in range( self.Nvars ) : \n",
    "                pP[k] += H # perturb on coordinate k\n",
    "                fk = self.obj( pP ) # evalute objective at perturbed argument\n",
    "                df[k] = ( fk - f0 ) / H # compute finite difference\n",
    "                pP[k] -= H # perturb on coordinate k\n",
    "            ds[h] = np.max( np.abs( g0 - df ) ) # compute difference\n",
    "\n",
    "        if( verbose ) : \n",
    "            for h in range( Hs.size ) : \n",
    "                print( \"%0.16f , %0.16f\" % ( Hs[h] , ds[h] ) )\n",
    "\n",
    "        return Hs , ds\n",
    "\n",
    "    def hessp_check( self , p=None , v=None , verbose=True ) :  \n",
    "\n",
    "        \"\"\"\n",
    "        hessian product check of an object that has certain attributes (Nvars, grad, and hessp)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if p is None : p = rand( self.Nvars )\n",
    "        if v is None : v = rand( self.Nvars )\n",
    "\n",
    "        g0 , h0 = self.grad( p ) , self.hessp( p , v ) # original gradient\n",
    "\n",
    "        Hs = 10.0**np.arange( -10 , 1 , 1 )[::-1] # perturbation sizes\n",
    "        pP = p.copy() # copy for perturbed betas\n",
    "\n",
    "        ds = np.zeros( Hs.size ) # differences between gradient and finite differences\n",
    "\n",
    "        # iterations\n",
    "        for h in range( Hs.size ) : \n",
    "            H = Hs[h] # actual perturbation\n",
    "            pP = p + H * v # perturb by H in direction v\n",
    "            gP = self.grad( pP ) # evalute objective gradient at perturbed argument\n",
    "            df = ( gP - g0 ) / H\n",
    "            ds[h] = np.max( np.abs( df - h0 ) ) # compute difference\n",
    "\n",
    "        if( verbose ) : \n",
    "            for h in range( Hs.size ) : \n",
    "                print( \"%0.16f , %0.16f\" % ( Hs[h] , ds[h] ) )\n",
    "\n",
    "        return Hs , ds\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have a specific way of testing derivative accuracy: Compare finite differences (for gradients or for hessian-vector products) for a _decreasing sequence_ of perturbation sizes. Regardless of the function evaluation accuracy, we should see a \"V\" shape in the associated relative error magnitude between computed derivatives and finite differences. The finite difference approximation should be innaccurate for \"large\" perturbations but get more accurate as the perturbation decreases, that is until floating point errors start to accumulate and reduce the approximation accuracy. \n",
    "\n",
    "This is obviously more computationally intensive that comparing derivatives at a single perturbation size and for large enough problems can be quite burdensome. However single-point approximations give no indication of what a \"reasonable\" error should be, and thus are somewhat useless when we don't know how much error exists in our function and derivative evaluations to begin with. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logit\n",
    "\n",
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Logit is the simplest model, with \"yes\" probability\n",
    "$$\n",
    "    \\frac{e^\\beta}{1+e^\\beta}\n",
    "$$\n",
    "and MLE problem\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\min &\\quad \\frac{1}{N} \\sum_{n=1}^N \\log( 1 + e^{-y_n\\beta} ) \\\\\n",
    "    \\text{w.r.t.} &\\quad \\beta \\in \\mathbb{R}\n",
    "\\end{aligned}\n",
    "$$\n",
    "The derivative of the objective is easily derived as\n",
    "$$\n",
    "    D^\\beta = \\frac{1}{N} \\sum_{n=1}^N (-y_n)\\left( \\frac{ e^{-y_n\\beta} }{ 1 + e^{-y_n\\beta} } \\right)\n",
    "$$\n",
    "Moreover the Hessian is even easily derived as\n",
    "$$\n",
    "    \\frac{1}{N} \\sum_{n=1}^N \\left( \\frac{ e^{-y_n\\beta} }{ 1 + e^{-y_n\\beta} } \\right)\\left( 1 - \\frac{ e^{-y_n\\beta} }{ 1 + e^{-y_n\\beta} } \\right)\n",
    "$$\n",
    "The following class, derived from `FittableModel`, implements the Logit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Logit( FittableModel ) : \n",
    "    \n",
    "    type = \"Logit\"\n",
    "    \n",
    "    def __init__( self , N , y ) : \n",
    "        self.Nvars , self.N , self.y , self.ny = 1 , N , y , - y\n",
    "\n",
    "    def obj( self , p ) :\n",
    "        return np.sum( np.log1p( np.exp( self.ny * p ) ) ) / self.N\n",
    "\n",
    "    def grad( self , p ) :  \n",
    "        eU = np.exp( self.ny * p ) ; PL = np.divide( eU , 1.0 + eU )\n",
    "        return np.array( [ np.sum( PL * self.ny ) / self.N ] )\n",
    "    \n",
    "    def hess( self , p ) :  \n",
    "        eU = np.exp( self.ny * p ) ; PL = eU / ( 1.0 + eU )\n",
    "        return np.array( [ np.sum( PL * ( 1.0 - PL ) ) / self.N ] )\n",
    "    \n",
    "    def solve( self , p0=None ) : \n",
    "        self.soln = minimize( self.obj , p0 , jac=self.grad , \\\n",
    "                              hess=self.hess , method='trust-constr' )\n",
    "        return self.soln\n",
    "    \n",
    "    def printx( self ) : \n",
    "        if self.soln is None : return \"(no solution)\"\n",
    "        return \"%0.2f\" % self.soln['x'][0]\n",
    "    \n",
    "    def getx( self ) : \n",
    "        if self.soln is None : return None\n",
    "        else : return self.soln['x'][0]\n",
    "    \n",
    "    def kld( self , PT ) : \n",
    "        if self.soln['x'][0] > 0 : \n",
    "            P = 1.0 / ( 1.0 + np.exp( -self.soln['x'][0] ) )\n",
    "        else : \n",
    "            P = np.exp( self.soln['x'][0] ) ; P = P / ( 1.0 + P )\n",
    "        return PT * np.log( PT/P ) + (1.0-PT) * np.log( (1.0-PT)/(1.0-P) )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Class Logit\n",
    "\n",
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we presume that each individual is \"drawn\" from one of $C$ classes each with its own coefficient. Our job is to estimate the class coefficients and the mass function for the classes. The simplest version of this problem is: \n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\\\\n",
    "    \\min &\\quad - \\frac{1}{N} \\sum_{i=1}^I \\log \\left( \\sum_{c=1}^C \\rho_c e^{L_i(\\beta_c)} \\right)    \n",
    "        \\quad\\quad\\text{where}\\quad \n",
    "        L_i(\\theta) = - \\sum_{ n \\in \\mathcal{O}_i } \\log \\Big( 1 + e^{-y_n\\theta} \\Big) \n",
    "        \\\\\n",
    "    \\text{w.r.t.} &\\quad 0 \\leq \\rho_c \\leq 1 \\;\\; , \\;\\; \\beta_c \\in \\mathbb{R} \n",
    "                        \\;\\; \\text{for all} \\;\\; c = 1,\\dotsc,C \\\\\n",
    "    \\text{s.to} &\\quad \\sum_{c=1}^C \\rho_c = 1 \\\\\n",
    "   \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "The derivatives are\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    D_c^\\rho \n",
    "        &= - \\frac{1}{N} \\sum_{i=1}^I \\frac{ e^{L_i(\\beta_c)} }{ \\sum_{d=1}^C \\rho_d e^{L_i(\\beta_d)} }\n",
    "    \\\\\n",
    "    D_c^\\beta \n",
    "        &= - \\frac{1}{N} \\sum_{i=1}^I \\frac{ \\rho_c e^{L_i(\\beta_c)} L_i^\\prime(\\beta_c) }{ \\sum_{d=1}^C \\rho_d e^{L_i(\\beta_d)} }\n",
    "    \\quad\\quad\\text{where}\\quad \n",
    "    L_i^\\prime(\\theta) \n",
    "        = - \\sum_{ n \\in \\mathcal{O}_i } (-y_n)\\frac{ e^{-y_n\\theta} }{ 1 + e^{-y_n\\theta} }\n",
    "        = \\sum_{ n \\in \\mathcal{O}_i } y_n\\frac{ e^{-y_n\\theta} }{ 1 + e^{-y_n\\theta} }\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stabilization\n",
    "\n",
    "We have to consider an important stabilization technique. If $\\max_c L_i( \\beta_c ) \\ll 0$, then $e^{ L_i( \\beta_c ) } \\approx 0$ and \n",
    "$$\n",
    "    \\log \\left( \\sum_{c=1}^C \\rho_c e^{ L_i( \\beta_c ) } \\right )\n",
    "$$ \n",
    "will not be computable. Specifically, if \n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\\\\n",
    "    \\max_c L_i( \\beta_c ) \\leq -745.15\n",
    "    \\quad\\quad\\text{then}\\quad\\quad\n",
    "    \\texttt{float}( e^{ L_i( \\beta_c ) } ) = 0\n",
    "    \\\\\n",
    "    \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "where $\\texttt{float}$ is the floating point representation of the exponential (in double precision). This has to be handled carefully if we want to allow for sufficient exploration of $\\beta_1,\\dotsc,\\beta_C$ for arbitrary data. \n",
    "\n",
    "In particular, note that \n",
    "$$\n",
    "    L_i(\\theta) = - |\\{n:y_n=+1\\}| \\log\\left( 1 + e^{-\\theta} \\right) - |\\{n:y_n=-1\\}| \\log\\left( 1 + e^{\\theta} \\right)\n",
    "$$\n",
    "This is, in fact, a formula we could use to dramatically improve the scalability of the fitting methods we explore, but at the cost of _complete_ loss of generality. What is worth noting here is that as the number of observations increase, the size of both sets goes to infinity; moreover the $\\log$ terms are both positive, having arguments larger than one. Thus, for any finite $\\theta$, there is _some_ number of observations such that $L_i$ is too negative to effectively compute with. It is probably even possible to analyze this situation further, but knowing exactly _when_ we get into trouble isn't helpful. \n",
    "\n",
    "A simple trick sidesteps this potential problem: Let \n",
    "$$\n",
    "L_i^*(\\boldsymbol{\\beta}) = \\max_c L_i( \\beta_c )\n",
    "$$\n",
    "and then\n",
    "$$\n",
    "   -\\frac{1}{N} \\sum_{i=1}^I \\log \\left( \\sum_{c=1}^C \\rho_c e^{ L_i( \\beta_c ) } \\right )\n",
    "        = -\\frac{1}{N} \\sum_{i=1}^I L_i^*(\\boldsymbol{\\beta}) -\\frac{1}{N} \\sum_{i=1}^I  \\log \\left( \\sum_{c=1}^C \\rho_c e^{ L_i( \\beta_c ) - L_i^*(\\boldsymbol{\\beta}) } \\right )\n",
    "$$ \n",
    "Note also that the derivatives can be evaluated with $L_i( \\beta_c ) - L_i^*(\\boldsymbol{\\beta})$: \n",
    "$$\n",
    "\\begin{aligned}\n",
    "    D_c^\\rho \n",
    "        &= - \\frac{1}{N} \\sum_{i=1}^I \\frac{ e^{L_i(\\beta_c)} }{ \\sum_{d=1}^C \\rho_d e^{L_i(\\beta_d)} }\n",
    "        = - \\frac{1}{N} \\sum_{i=1}^I \\frac{ e^{L_i^*(\\boldsymbol{\\beta})} e^{L_i(\\beta_c)-L_i^*(\\boldsymbol{\\beta})} }{ e^{L_i^*(\\boldsymbol{\\beta})} \\sum_{d=1}^C \\rho_d e^{L_i(\\beta_d)-L_i^*(\\boldsymbol{\\beta})} }\n",
    "        = - \\frac{1}{N} \\sum_{i=1}^I \\frac{ e^{L_i(\\beta_c)-L_i^*(\\boldsymbol{\\beta})} }{\\sum_{d=1}^C \\rho_d e^{L_i(\\beta_d)-L_i^*(\\boldsymbol{\\beta})} }\n",
    "    \\\\\n",
    "    D_c^\\beta \n",
    "        &= - \\frac{1}{N} \\sum_{i=1}^I \\frac{ \\rho_c e^{L_i(\\beta_c)} L_i^\\prime(\\beta_c) }{ \\sum_{d=1}^C \\rho_d e^{L_i(\\beta_d)} }\n",
    "        = - \\frac{1}{N} \\sum_{i=1}^I \\frac{ e^{L_i^*(\\boldsymbol{\\beta})} \\rho_c e^{L_i(\\beta_c)-L_i^*(\\boldsymbol{\\beta})} L_i^\\prime(\\beta_c) }{ e^{L_i^*(\\boldsymbol{\\beta})} \\sum_{d=1}^C \\rho_d e^{L_i(\\beta_d)-L_i^*(\\boldsymbol{\\beta})} }\n",
    "        = - \\frac{1}{N} \\sum_{i=1}^I \\frac{ \\rho_c e^{L_i(\\beta_c)-L_i^*(\\boldsymbol{\\beta})} L_i^\\prime(\\beta_c) }{ \\sum_{d=1}^C \\rho_d e^{L_i(\\beta_d)-L_i^*(\\boldsymbol{\\beta})} }\n",
    "    \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LatentClassLogit( FittableModel ) : \n",
    "    \n",
    "    type = \"Latent Class Logit\"\n",
    "    \n",
    "    def __init__( self , N , I , i , y , C , ordered=True ) : \n",
    "        \n",
    "        if C <= 1 : \n",
    "            raise ArgumentError( \"Trivial number of classes: should be at least two.\" )\n",
    "        \n",
    "        self.N , self.I , self.i , self.y , self.C = N , I , i , -y , C\n",
    "        self.Nvars , self.Ncons = 2*C , 2*C if ordered else C+1\n",
    "        \n",
    "        # this matrix \"assigns\" observations to individuals, facilitating \n",
    "        # sums over observations within individuals\n",
    "        self.reducer = csr_matrix( (-np.ones(N),(i,np.arange(N))) , shape=(I,N) )\n",
    "        self.yeducer = csr_matrix( (y,(i,np.arange(N))) , shape=(I,N) )\n",
    "        \n",
    "        # constraints: \n",
    "        #  \n",
    "        #     p[0] , p[1] , ... , p[C] >= 0.0\n",
    "        #     p[0] + p[1] + ... + p[C] = 1.0\n",
    "        #     p[0] >= p[1] >= ... >= p[C] if ordered\n",
    "        # \n",
    "        \n",
    "        lo = np.zeros( self.Ncons )\n",
    "        up = np.inf * np.ones( self.Ncons )\n",
    "        lo[C] , up[C] = 1.0 , 1.0\n",
    "        Cmtrx = np.zeros( ( self.Ncons , 2*C ) )\n",
    "        for c in range(self.C) : \n",
    "            Cmtrx[c,c] = 1.0 # p[c] >= 0\n",
    "            Cmtrx[C,c] = 1.0 # sum_c p[c] = 1\n",
    "        if ordered : \n",
    "            for c in range(1,self.C) : \n",
    "                Cmtrx[C+c,c-1] = 1.0\n",
    "                Cmtrx[C+c,c] = - 1.0\n",
    "            \n",
    "        self.cons = LinearConstraint( Cmtrx , lo , up )\n",
    "        \n",
    "        # workspace\n",
    "        self.yp = np.zeros((self.N,self.C),dtype=np.float)\n",
    "        self.eU = np.zeros((self.N,self.C),dtype=np.float)\n",
    "        self.ll = np.zeros((self.N,self.C),dtype=np.float)\n",
    "        self.Li = np.zeros((self.I,self.C),dtype=np.float)\n",
    "        self.EL = np.zeros((self.I,self.C),dtype=np.float)\n",
    "        self.PL = np.zeros((self.N,self.C),dtype=np.float)\n",
    "        self.LP = np.zeros((self.I,self.C),dtype=np.float)\n",
    "        self.j  = np.zeros((self.I,),dtype=np.float)\n",
    "        self.LM = np.zeros((self.I,),dtype=np.float)\n",
    "        \n",
    "    def basics( self , p ) : \n",
    "        np.outer( self.y , p[self.C:] , out=self.yp )\n",
    "        np.exp( self.yp , out=self.eU )\n",
    "        np.log1p( self.eU , out=self.ll )\n",
    "        self.Li = self.reducer @ self.ll\n",
    "        np.max( self.Li , axis=1 , out=self.LM )\n",
    "        self.Li = self.Li - np.tile( self.LM.reshape((self.I,1)) , (1,self.C) )\n",
    "        np.exp( self.Li , out=self.EL )\n",
    "        self.j = self.EL @ p[:self.C]\n",
    "        \n",
    "    def obj( self , p ) :\n",
    "        self.basics( p )\n",
    "        np.log( self.j , out=self.j )\n",
    "        return - np.sum( self.LM + self.j ) / self.N\n",
    "\n",
    "    def grad( self , p ) :  \n",
    "        self.basics( p ) \n",
    "        np.divide( self.eU , 1.0 + self.eU , out=self.PL )\n",
    "        self.LP = self.yeducer @ self.PL\n",
    "        self.j  = 1.0 / self.j\n",
    "        \n",
    "        # gradient terms\n",
    "        g = np.zeros( self.Nvars )\n",
    "        g[:self.C] = - self.EL.T @ self.j / self.N\n",
    "        g[self.C:] = - p[:self.C] * ( ( self.EL * self.LP ).T @ self.j ) / self.N\n",
    "        \n",
    "        return g\n",
    "    \n",
    "    def draw_initial_condition( self ) : \n",
    "        p0 = randn( self.Nvars ) \n",
    "        p0[:self.C] = np.abs( p0[:self.C] )\n",
    "        p0[:self.C] = p0[:self.C] / p0[:self.C].sum()\n",
    "        return p0\n",
    "    \n",
    "    def solve( self , p0=None ) : \n",
    "        self.soln = minimize( self.obj , p0 , jac=self.grad , hess=BFGS() , \\\n",
    "                            method='trust-constr' , constraints=self.cons , \\\n",
    "                           options={ 'maxiter' : 100000 } )\n",
    "        return self.soln\n",
    "    \n",
    "    def printx( self ) : \n",
    "        if self.soln is None : return \"(no solution)\"\n",
    "        s = \" ) , ( \".join( [ \"%0.2f , %0.2f\" % ( self.soln['x'][c+self.C] , self.soln['x'][c] ) \\\n",
    "                                    for c in range(self.C) ] )\n",
    "        return \"( %s )\" % s\n",
    "    \n",
    "    def getx( self ) : \n",
    "        if self.soln is None : return None\n",
    "        else : \n",
    "            return self.soln['x'][self.C:]\n",
    "    \n",
    "    def kld( self , PT ) : \n",
    "        PL = np.exp( self.soln['x'][self.C:] ) ; PL = PL / ( 1.0 + PL )\n",
    "        P  = np.sum( PL * self.soln['x'][:self.C] )\n",
    "        return PT * np.log( PT/P ) + (1.0-PT) * np.log( (1.0-PT)/(1.0-P) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Classification\" Latent Class Logit\n",
    "\n",
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we \"classify\" individuals to classes, instead of presume each individual was drawn from a class at random. For this we introduce individual-specific class membership probabilities and solve\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\min &\\quad - \\frac{1}{N} \\sum_{i=1}^I \\log \\left( \\sum_{c=1}^C \\rho_{i,c} e^{L_i(\\beta_c)} \\right) \\\\\n",
    "    \\text{w.r.t.} &\\quad 0 \\leq \\rho_{i,c} \\leq 1 \\;\\; , \\;\\; \\beta_c \\in \\mathbb{R} \n",
    "                        \\;\\; \\text{for all} \\;\\; c = 1,\\dotsc,C \\\\\n",
    "    \\text{s.to} &\\quad \\sum_{c=1}^C \\rho_{i,c} = 1 \\text{ for all } i \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "The derivatives here are simpler, if more numerous: \n",
    "$$\n",
    "\\begin{aligned}\n",
    "    D_{i,c}^\\rho \n",
    "        &= - \\frac{1}{N} \\frac{ e^{L_i(\\beta_c)} }{ \\sum_{d=1}^C \\rho_{i,d} e^{L_i(\\beta_d)} }\n",
    "        \\\\\n",
    "    D_{c}^\\beta \n",
    "        &= - \\frac{1}{N} \\sum_{i=1}^I \\frac{ \\rho_{i,c} e^{L_i(\\beta_c)} }{ \\sum_{d=1}^C \\rho_{i,d} e^{L_i(\\beta_d)} }\n",
    "            L_i^\\prime(\\beta_c)\n",
    "        \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "This problem is also not always well-determined, as the large number of variables ($(I+1)C$) can exceed the number of observations ($N$). \n",
    "\n",
    "We stabilize in the same way as described above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ClassClassLogit( FittableModel ) : \n",
    "    \n",
    "    type = \"Class Class Logit\"\n",
    "    \n",
    "    def __init__( self , N , I , i , y , C , ordered=True ) : \n",
    "        \n",
    "        if C <= 1 : \n",
    "            raise ArgumentError( \"Trivial number of classes: should be at least two.\" )\n",
    "        \n",
    "        self.N , self.I , self.i , self.y , self.C = N , I , i , -y , C\n",
    "        self.Nvars , self.IC , self.Ncons = (I+1)*C , I*C , I*(C+1)\n",
    "        \n",
    "        if self.N - self.Nvars <= 0 : \n",
    "            raise ArgumentError( \"Not enough observations for the number of individuals and classes.\" )\n",
    "        \n",
    "        # this matrix \"assigns\" observations to individuals, facilitating \n",
    "        # sums over observations within individuals\n",
    "        self.reducer = csr_matrix( (-np.ones(N),(i,np.arange(N))) , shape=(I,N) )\n",
    "        self.yeducer = csr_matrix( (y,(i,np.arange(N))) , shape=(I,N) )\n",
    "        \n",
    "        # constraints: \n",
    "        #  \n",
    "        #     p[0] , p[1] , ... , p[IC-1] >= 0.0                          IC equations\n",
    "        #     p[i,0] + p[i,1] + ... + p[i,C-1] = 1.0 for all I            I equations\n",
    "        #     p[i,0] - p[i,1] , ... p[i,C] - p[i,C-1] >= 0 if ordered ?   I(C-1) equations\n",
    "        # \n",
    "        \n",
    "        lo , up = np.zeros( self.Ncons ) , np.inf * np.ones( self.Ncons )\n",
    "        lo[I*C:I*(C+1)] , up[I*C:I*(C+1)] = 1.0 , 1.0\n",
    "        \n",
    "        Cnnzs = 2*I*C\n",
    "        Crows , Ccols = np.zeros( Cnnzs , dtype=np.int ) , np.zeros( Cnnzs , dtype=np.int )\n",
    "        Cdata = np.ones( Cnnzs )\n",
    "        \n",
    "        Crows[:I*C] , Ccols[:I*C] = np.arange(I*C) , np.arange(I*C)\n",
    "        Crows[I*C:] , Ccols[I*C:] = I*C + np.repeat( np.arange(I) , C ) , np.arange(I*C)\n",
    "        \n",
    "        Cmtrx = csr_matrix( (Cdata,(Crows,Ccols)) , shape=( self.Ncons , self.Nvars ) )\n",
    "            \n",
    "        self.cons = LinearConstraint( Cmtrx , lo , up )\n",
    "        \n",
    "        # workspace\n",
    "        self.yp = np.zeros((self.N,self.C),dtype=np.float)\n",
    "        self.eU = np.zeros((self.N,self.C),dtype=np.float)\n",
    "        self.ll = np.zeros((self.N,self.C),dtype=np.float)\n",
    "        self.Li = np.zeros((self.I,self.C),dtype=np.float)\n",
    "        self.EL = np.zeros((self.I,self.C),dtype=np.float)\n",
    "        self.TL = np.zeros((self.I,self.C),dtype=np.float)\n",
    "        self.PL = np.zeros((self.N,self.C),dtype=np.float)\n",
    "        self.LP = np.zeros((self.I,self.C),dtype=np.float)\n",
    "        self.j  = np.zeros((self.I,),dtype=np.float)\n",
    "        self.LM = np.zeros((self.I,),dtype=np.float)\n",
    "\n",
    "    def basics( self , p ) : \n",
    "        np.outer( self.y , p[self.IC:] , out=self.yp )\n",
    "        np.exp( self.yp , self.eU )\n",
    "        np.log1p( self.eU , out=self.ll )\n",
    "        self.Li = self.reducer @ self.ll\n",
    "        np.max( self.Li , axis=1 , out=self.LM )\n",
    "        self.Li = self.Li - np.tile( self.LM.reshape((self.I,1)) , (1,self.C) )\n",
    "        np.exp( self.Li , out=self.EL )\n",
    "        np.multiply( self.EL , p[:self.IC].reshape((self.I,self.C)) , out=self.TL )\n",
    "        self.j = np.sum( self.TL , axis=1 )\n",
    "        \n",
    "    def obj( self , p ) :\n",
    "        self.basics( p )\n",
    "        np.log( self.j , out=self.j )\n",
    "        return - np.sum( self.LM + self.j ) / self.N\n",
    "\n",
    "    def grad( self , p ) :  \n",
    "        \n",
    "        self.basics( p )\n",
    "        \n",
    "        np.divide( self.eU , 1.0 + self.eU , out=self.PL )\n",
    "        self.LP = self.yeducer @ self.PL\n",
    "        self.j = 1.0 / self.j\n",
    "        \n",
    "        # gradient terms\n",
    "        g = np.zeros( self.Nvars )\n",
    "        g[:self.IC] = - self.EL.flatten() * np.repeat( self.j , self.C ) / self.N\n",
    "        g[self.IC:] = - ( ( self.TL * self.LP ).T @ self.j ) / self.N\n",
    "        \n",
    "        return g\n",
    "    \n",
    "    def draw_initial_condition( self ) : \n",
    "        p0 = randn(self.Nvars)\n",
    "        p0[:self.IC] = np.abs( p0[:self.IC] )\n",
    "        for i in range( self.I ) : \n",
    "            p0[i*self.C:i*self.C+1] = p0[i*self.C:i*self.C+1] / p0[i*self.C:i*self.C+1].sum()\n",
    "        return p0\n",
    "        \n",
    "    def solve( self , p0=None ) : \n",
    "        self.soln = minimize( self.obj , p0 , jac=self.grad , hess=BFGS() , \\\n",
    "                            method='trust-constr' , constraints=self.cons , \\\n",
    "                           options={ 'maxiter' : 100000 } )\n",
    "        return self.soln\n",
    "    \n",
    "    def printx( self ) : \n",
    "        if self.soln is None : return \"(no solution)\"\n",
    "        rho = self.soln['x'][:self.IC].reshape((self.I,self.C)).mean( axis=0 )\n",
    "        s = \" ) , ( \".join( [ \"%0.2f , %0.2f\" % ( self.soln['x'][self.IC+c] , rho[c] ) for c in range(self.C) ] )\n",
    "        return \"( %s )\" % s\n",
    "    \n",
    "    def getx( self ) : \n",
    "        if self.soln is None : return None\n",
    "        else : \n",
    "            w = self.soln['x'][:self.IC].reshape( (self.I,self.C) ).mean( axis=0 ) # average class likelihoods\n",
    "            return self.soln['x'][self.IC:]\n",
    "    \n",
    "    def kld( self , PT ) : \n",
    "        PL = np.exp( self.soln['x'][self.IC:] ) ; PL = PL / ( 1.0 + PL )\n",
    "        w  = self.soln['x'][:self.IC].reshape( (self.I,self.C) ).mean( axis=0 ) # average class likelihoods\n",
    "        P  = np.sum( PL * w )\n",
    "        return PT * np.log( PT/P ) + (1.0-PT) * np.log( (1.0-PT)/(1.0-P) )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Gaussian) Random Coefficients\n",
    "\n",
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random (gaussian) coefficient Logit for this situation is\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\min &\\quad -\\frac{1}{N} \\sum_{i=1}^I \\log \\left( \n",
    "                        \\int e^{ L_i( \\mu + \\sigma v ) } \\phi(v) dv\n",
    "                    \\right) \\\\\n",
    "    \\text{w.r.t.} &\\quad \\mu \\in \\mathbb{R} \\; , \\; \\sigma \\geq 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "where $L_i$ has the same definition as above. Broadly speaking, the derivatives are\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    D^\\mu \n",
    "        &= -\\frac{1}{N} \\sum_{i=1}^I \n",
    "                            \\frac{ \\int e^{ L_i( \\mu + \\sigma v ) } L_i^\\prime( \\mu + \\sigma v ) \\phi(v) dv }\n",
    "                                { \\int e^{ L_i( \\mu + \\sigma v ) } \\phi(v) dv } \n",
    "                    \\\\\n",
    "    D^\\sigma\n",
    "        &= -\\frac{1}{N} \\sum_{i=1}^I \n",
    "                            \\frac{ \\int e^{ L_i( \\mu + \\sigma v ) } L_i^\\prime( \\mu + \\sigma v ) v \\phi(v) dv }\n",
    "                                { \\int e^{ L_i( \\mu + \\sigma v ) } \\phi(v) dv }\n",
    "                    \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "we can approximate these directly, or approximate them by differentiating our approximation to the actual integral. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we approximate the integrals with a sequence of $S$ \"standardized\" samples $v_s$ and associated weights $w_s$ that we hold fix over all individuals: \n",
    "$$\n",
    "    \\int e^{ L_i( \\mu + \\sigma v ) } \\phi(v) dv\n",
    "        \\approx \\sum_{s=1}^S w_s e^{ L_i( \\mu + \\sigma v_s ) }. \n",
    "$$\n",
    "This makes the negative log likelihood approximation \n",
    "$$\n",
    "    -\\frac{1}{N} \\sum_{i=1}^I \\log\\left( \\sum_{s=1}^S w_s e^{ L_i( \\mu + \\sigma v_s ) } \\right) .\n",
    "$$\n",
    "The associated (identically sampled and weighted) derivative approximations are\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    D^\\mu \n",
    "        &= -\\frac{1}{N} \\sum_{i=1}^I \n",
    "                            \\frac{ \\sum_{s=1}^S w_s e^{ L_i( \\mu + \\sigma v_s ) } L_i^\\prime( \\mu + \\sigma v_s ) }\n",
    "                                { \\sum_{s=1}^S  w_s e^{ L_i( \\mu + \\sigma v_s ) }  } \n",
    "                    \\\\\n",
    "    D^\\sigma\n",
    "        &= -\\frac{1}{N} \\sum_{i=1}^I \n",
    "                            \\frac{ \\sum_{s=1}^S w_s e^{ L_i( \\mu + \\sigma v_s ) } L_i^\\prime( \\mu + \\sigma v_s ) v_s}\n",
    "                                { \\sum_{s=1}^S w_s  e^{ L_i( \\mu + \\sigma v_s ) } } \n",
    "                    \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "It is important that we either hold such approximations fixed over the course of an estimation attempt or ensure that the approximation is so good that changing approximation error does not interfere with optimizer progress. \n",
    "\n",
    "Given $\\mathbf{v}$ and $\\mathbf{w}$, we can compute the negative log likelihood $f$ and gradient $\\mathbf{g}$ as\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    & \\\\\n",
    "    &\\quad \\boldsymbol{\\theta} \\leftarrow \\mu + \\sigma \\mathbf{v} \\in \\mathbb{R}^{S} \\\\\n",
    "    &\\quad \\boldsymbol{\\eta} \\leftarrow \\exp\\{ - \\mathbf{y} \\boldsymbol{\\theta}^\\top \\} \\in \\mathbb{R}^{N \\times S} \\\\\n",
    "    &\\quad \\boldsymbol{\\ell} \\leftarrow \\log( 1 + \\boldsymbol{\\eta} ) \\in \\mathbb{R}^{N \\times S} \\\\\n",
    "    &\\quad \\mathbf{L} \\leftarrow - \\mathbf{R}\\boldsymbol{\\ell} \\in \\mathbb{R}^{I \\times S} \\\\\n",
    "    &\\quad \\mathbf{P} \\leftarrow \\boldsymbol{\\eta} \\; / \\; (1+\\boldsymbol{\\eta}) \\in \\mathbb{R}^{N \\times S} \\\\\n",
    "    &\\quad \\mathbf{L}^\\prime \\leftarrow \\mathbf{R}(\\mathrm{diag}(\\mathbf{y})\\mathbf{P}) \\in \\mathbb{I \\times S} \\\\\n",
    "    &\\quad \\mathbf{E}_1 \\leftarrow \\exp\\{ \\mathbf{L} \\} \\in \\mathbb{R}^{I \\times S} \\\\\n",
    "    &\\quad \\mathbf{E}_2 \\leftarrow \\mathbf{E}_1 * \\mathbf{L}^\\prime \\in \\mathbb{R}^{I \\times S} \\\\\n",
    "    &\\quad\\begin{aligned}\n",
    "        &\\texttt{for } i=1,\\dotsc,I \\\\\n",
    "        &\\quad\\quad \\mathbf{E}_3[i,:] \\leftarrow \\mathbf{E}_2[i,:] * \\mathbf{v}[:] \\\\\n",
    "     \\end{aligned} \\quad\\quad\\text{or}\\quad\\quad\n",
    "     \\begin{aligned}\n",
    "        &\\texttt{for } s=1,\\dotsc,S \\\\\n",
    "        &\\quad\\quad \\mathbf{E}_3[:,s] \\leftarrow \\mathbf{v}[s]\\;\\mathbf{E}_2[:,s] \\\\\n",
    "     \\end{aligned}\n",
    "    \\\\\n",
    "    &\\quad\\begin{aligned}\n",
    "        &\\texttt{for } k=1,2,3 \\\\\n",
    "        &\\quad\\quad \\mathbf{j}_k \\leftarrow \\mathbf{E}_k \\mathbf{w} \\in \\mathbb{R}^{I} \\\\\n",
    "     \\end{aligned}\n",
    "     \\\\\n",
    "    &\\quad f \\leftarrow - \\; \\texttt{sum}( \\; \\log( \\; \\mathbf{j}_1 \\; ) \\; ) \\; / \\; N \\\\\n",
    "    &\\quad \\mathbf{g} \\leftarrow \n",
    "        - \\frac{1}{N} \\begin{pmatrix} \n",
    "            \\texttt{sum}( \\; \\mathbf{j}_2 \\; / \\; \\mathbf{j}_1 \\; ) \\\\\n",
    "            \\texttt{sum}( \\; \\mathbf{j}_3 \\; / \\; \\mathbf{j}_1 \\; ) \\\\\n",
    "        \\end{pmatrix}\n",
    "     \\\\\n",
    "     & \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "After defining a class for this approach we review two ways of computing sample points and weights: sample average approximations and Gauss-Legendre quadrature. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stabilization\n",
    "\n",
    "We have to again consider our stabilization. If $\\max_s L_i( \\mu + \\sigma v_s ) \\ll 0$, then $e^{ L_i( \\mu + \\sigma v_s ) } \\approx 0$; if the latter holds, the resulting $\\log$ will fail. Specifically, if \n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\\\\n",
    "    \\max_s L_i( \\mu + \\sigma v_s ) \\leq -745.15\n",
    "    \\quad\\quad\\text{then}\\quad\\quad\n",
    "    \\texttt{float}( e^{ L_i( \\mu + \\sigma v_s ) } ) = 0\n",
    "    \\\\\n",
    "    \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "where $\\texttt{float}$ is the floating point representation of the exponential (in double precision). This has to be handled carefully if we want to allow for sufficient exploration of $\\mu,\\sigma$ for arbitrary data. \n",
    "\n",
    "As above, let \n",
    "$$\n",
    "L_i^*(\\mu,\\sigma) = \\max_s L_i( \\mu + \\sigma v_s )\n",
    "$$\n",
    "and note that\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    -\\frac{1}{N} \\sum_{i=1}^I \\log\\left( \\sum_{s=1}^S w_s e^{ L_i( \\mu + \\sigma v_s ) } \\right) \n",
    "        &= -\\frac{1}{N} \\sum_{i=1}^I \\log\\left( e^{L_i^*(\\mu,\\sigma)} \\sum_{s=1}^S w_s e^{ L_i( \\mu + \\sigma v_s ) - L_i^*(\\mu,\\sigma) } \\right) \\\\\n",
    "        &= -\\frac{1}{N} \\sum_{i=1}^I L_i^*(\\mu,\\sigma) - \\frac{1}{N} \\sum_{i=1}^I \\log\\left( \\sum_{s=1}^S w_s e^{ L_i( \\mu + \\sigma v_s ) - L_i^*(\\mu,\\sigma) } \\right) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "Here at least one of the exponents, for any $i$, is zero by definition and thus its corresponding exponentiated value is one. Thus the sum is at least as large as the smallest weight, which is nonzero if we carefully construct the weights. \n",
    "\n",
    "If we want, we can absorb the weights too: \n",
    "$$\n",
    "\\begin{aligned}\n",
    "    -\\frac{1}{N} \\sum_{i=1}^I \\log\\left( \\sum_{s=1}^S w_s e^{ L_i( \\mu + \\sigma v_s ) } \\right) \n",
    "        &= -\\frac{1}{N} \\sum_{i=1}^I \\log\\left( \\sum_{s=1}^S e^{ W_{i,s}( \\mu + \\sigma v_s ) } \\right) \n",
    "            &&\\quad W_{i,s}(\\theta) = L_i(\\theta) + \\log w_s\\\\\n",
    "        &= -\\frac{1}{N} \\sum_{i=1}^I \\log\\left( e^{W_i^*(\\mu,\\sigma)} \\sum_{s=1}^S e^{ W_{i,s}( \\mu + \\sigma v_s ) - W_i^*(\\mu,\\sigma) } \\right) \n",
    "            &&\\quad W_i^*(\\mu,\\sigma) = \\max_s W_{i,s}(\\mu+\\sigma v_s) \\\\\n",
    "        &= -\\frac{1}{N} \\sum_{i=1}^I W_i^*(\\mu,\\sigma) - \\frac{1}{N} \\sum_{i=1}^I \\log\\left( \\sum_{s=1}^S e^{ W_{i,s}( \\mu + \\sigma v_s ) - W_i^*(\\mu,\\sigma) } \\right) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "This form would ensure that the $\\log$ is of a term that is always greater than one. \n",
    "\n",
    "Presuming the first form, the associated derivative approximations can be evaluated the same way with shifted values for $L_i(\\mu+\\sigma v_s)$: \n",
    "$$\n",
    "\\begin{aligned}\n",
    "    D^\\mu \n",
    "        &= -\\frac{1}{N} \\sum_{i=1}^I \n",
    "                            \\frac{ \\sum_{s=1}^S w_s e^{ L_i( \\mu + \\sigma v_s ) } L_i^\\prime( \\mu + \\sigma v_s ) }\n",
    "                                { \\sum_{s=1}^S  w_s e^{ L_i( \\mu + \\sigma v_s ) }  } \n",
    "                    \\\\\n",
    "        &= -\\frac{1}{N} \\sum_{i=1}^I \n",
    "                            \\frac{ e^{L_i^*(\\mu,\\sigma)} \\sum_{s=1}^S w_s e^{ L_i( \\mu + \\sigma v_s ) - L_i^*(\\mu,\\sigma) } L_i^\\prime( \\mu + \\sigma v_s ) }\n",
    "                                { e^{L_i^*(\\mu,\\sigma)} \\sum_{s=1}^S  w_s e^{ L_i( \\mu + \\sigma v_s ) - L_i^*(\\mu,\\sigma) }  } \n",
    "                    \\\\\n",
    "        &= -\\frac{1}{N} \\sum_{i=1}^I \n",
    "                            \\frac{ \\sum_{s=1}^S w_s e^{ L_i( \\mu + \\sigma v_s ) - L_i^*(\\mu,\\sigma) } L_i^\\prime( \\mu + \\sigma v_s ) }\n",
    "                                { \\sum_{s=1}^S  w_s e^{ L_i( \\mu + \\sigma v_s ) - L_i^*(\\mu,\\sigma) }  } \n",
    "                    \\\\\n",
    "    D^\\sigma\n",
    "        &= -\\frac{1}{N} \\sum_{i=1}^I \n",
    "                            \\frac{ \\sum_{s=1}^S w_s e^{ L_i( \\mu + \\sigma v_s ) } L_i^\\prime( \\mu + \\sigma v_s ) v_s}\n",
    "                                { \\sum_{s=1}^S w_s  e^{ L_i( \\mu + \\sigma v_s ) } } \n",
    "                    \\\\\n",
    "        &= -\\frac{1}{N} \\sum_{i=1}^I \n",
    "                            \\frac{ e^{L_i^*(\\mu,\\sigma)} \\sum_{s=1}^S w_s e^{ L_i( \\mu + \\sigma v_s ) - L_i^*(\\mu,\\sigma) } L_i^\\prime( \\mu + \\sigma v_s ) v_s}\n",
    "                                { e^{L_i^*(\\mu,\\sigma)} \\sum_{s=1}^S w_s  e^{ L_i( \\mu + \\sigma v_s ) - L_i^*(\\mu,\\sigma) } } \n",
    "                    \\\\\n",
    "        &= -\\frac{1}{N} \\sum_{i=1}^I \n",
    "                            \\frac{ \\sum_{s=1}^S w_s e^{ L_i( \\mu + \\sigma v_s ) - L_i^*(\\mu,\\sigma) } L_i^\\prime( \\mu + \\sigma v_s ) v_s}\n",
    "                                { \\sum_{s=1}^S w_s  e^{ L_i( \\mu + \\sigma v_s ) - L_i^*(\\mu,\\sigma) } } \n",
    "                    \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RCLogit( FittableModel ) : \n",
    "    \n",
    "    type = \"Random Coefficient Logit\"\n",
    "    \n",
    "    def __init__( self , N , I , i , y ) : \n",
    "        \n",
    "        self.Nvars , self.N , self.I , self.i , self.y = 2 , N , I , i , -y\n",
    "        \n",
    "        # these matrices \"assign\" observations to individuals, facilitating \n",
    "        # sums over observations within individuals. \n",
    "        self.reducer = csr_matrix( (-np.ones(N),(i,np.arange(N))) , shape=(I,N) )\n",
    "        self.yeducer = csr_matrix( (y,(i,np.arange(N))) , shape=(I,N) ) \n",
    "        \n",
    "        # constraints: sigma is non-negative    \n",
    "        self.cons = LinearConstraint( np.array([[0.0,1.0]]) , np.array([0.0]) , np.array([np.inf]) )\n",
    "        \n",
    "        self.S = 0\n",
    "    \n",
    "    def allocate_workspace( self ) : \n",
    "        if self.S > 0 :\n",
    "            self.eU = np.zeros((self.N,self.S),dtype=np.float) # N x S space\n",
    "            self.ll = np.zeros((self.N,self.S),dtype=np.float) # N x S space\n",
    "            self.Li = np.zeros((self.I,self.S),dtype=np.float) # I x S space\n",
    "            self.PL = np.zeros((self.N,self.S),dtype=np.float) # I x S space\n",
    "            self.LP = np.zeros((self.I,self.S),dtype=np.float) # I x S space\n",
    "            self.E  = np.zeros((self.I,self.S),dtype=np.float) # I x S space\n",
    "            self.j  = np.zeros((self.I,3),dtype=np.float) # I x 3 space\n",
    "            self.LM = np.zeros((self.I,),dtype=np.float) # I space\n",
    "        return\n",
    "    \n",
    "    def basics( self , p ) : \n",
    "        theta = p[0] + p[1] * self.v \n",
    "        np.exp( np.outer( self.y , theta ) , out=self.eU )\n",
    "        np.log1p( self.eU , out=self.ll )\n",
    "        self.Li = self.reducer @ self.ll\n",
    "        \n",
    "        # stabilization\n",
    "        self.LM = np.max( self.Li , axis=1 ) # get largest Li[i,:] -> LM[i]\n",
    "        self.Li = self.Li - np.tile( self.LM.reshape((self.I,1)) , (1,self.S) ) # subtract max from each Li\n",
    "        np.exp( self.Li , self.E ) # E[i,:] <- exp{ Li[i,:] - LM[i] }\n",
    "        self.j[:,0] = self.E @ self.w # each term is at least as large as the minimum weight\n",
    "        \n",
    "    def obj( self , p ) :\n",
    "        self.basics( p ) # this updates things used in both obj and grad\n",
    "        np.log( self.j[:,0] , out=self.j[:,1] ) # log only the shifted terms\n",
    "        return - np.sum( self.LM + self.j[:,1] ) / self.N # have to add in the max terms\n",
    "\n",
    "    def grad( self , p ) :  \n",
    "        \n",
    "        self.basics( p ) # this updates things used in both obj and grad\n",
    "        \n",
    "        np.divide( self.eU , 1.0 + self.eU , out=self.PL )\n",
    "        self.LP = self.yeducer @ self.PL # checked, this is doing the intended sub-sums\n",
    "        np.multiply( self.E , self.LP , out=self.E )\n",
    "        self.j[:,1] = self.E @ self.w\n",
    "        \n",
    "        # if we would guess a loop over i is more efficient, otherwise, loop over s\n",
    "        if self.I <= self.S : \n",
    "            self.E = np.apply_along_axis( lambda e : e * self.v , 1 , self.E )\n",
    "        else : \n",
    "            for s in range(self.S) : self.E[:,s] = self.E[:,s] * self.v[s]\n",
    "        self.j[:,2] = self.E @ self.w\n",
    "        \n",
    "        np.divide( self.j[:,1] , self.j[:,0] , out=self.j[:,1] )\n",
    "        np.divide( self.j[:,2] , self.j[:,0] , out=self.j[:,2] )\n",
    "        \n",
    "        g = np.zeros(2)\n",
    "        g[0] = - np.sum( self.j[:,1] ) / self.N\n",
    "        g[1] = - np.sum( self.j[:,2] ) / self.N\n",
    "        \n",
    "        return g\n",
    "    \n",
    "    def draw_initial_condition( self ) : \n",
    "        p0 = randn( self.Nvars )\n",
    "        p0[1] = np.abs(p0[1])\n",
    "        return p0\n",
    "        \n",
    "    def solve( self , p0=None ) : \n",
    "        self.soln = minimize( self.obj , p0 , jac=self.grad , hess=BFGS() , \\\n",
    "                            method='trust-constr' , constraints=self.cons , \\\n",
    "                           options={ 'maxiter' : 100000 } )\n",
    "        return self.soln\n",
    "    \n",
    "    def printx( self ) : \n",
    "        if self.soln is None : return \"(no solution)\"\n",
    "        return \"%0.2f ( %0.2f )\" % ( self.soln['x'][0] , self.soln['x'][1] )\n",
    "    \n",
    "    def getx( self ) : \n",
    "        if self.soln is None : return None\n",
    "        else : return self.soln['x'][:2]\n",
    "    \n",
    "    def kld( self , PT ) : \n",
    "        theta = self.soln['x'][0] + self.soln['x'][1] * self.v # mu + sigma v\n",
    "        PL = np.zeros( self.S )\n",
    "        ind = np.where( theta > 0.0 )[0]\n",
    "        PL[ind] = 1.0 / ( 1.0 + np.exp(-theta[ind]) )\n",
    "        ind = np.where( theta <= 0.0 )[0]\n",
    "        PL[ind] = np.exp(theta[ind]) ; PL[ind] = PL[ind] / ( 1.0 + PL[ind] )\n",
    "        P = np.sum( PL * self.w )\n",
    "        return PT * np.log( PT/P ) + (1.0-PT) * np.log( (1.0-PT)/(1.0-P) )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Average Approximations\n",
    "\n",
    "The simplest integral approximation comes from just sample averaging: where we draw $S$ samples $v_s$ from a standard gaussian distribution and use weights $w_s = 1/S$ or maybe even $w_s = \\phi(v_s)$. This is easy, but is also probably the least efficient approximation we can use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class GRCLogitSAA( RCLogit ) : \n",
    "        \n",
    "    type = \"Gaussian RC Logit (SAA)\"\n",
    "    \n",
    "    def __init__( self , N , I , i , y , samples=1000 , weighted=False ) : \n",
    "        \n",
    "        # initialize the superclass\n",
    "        RCLogit.__init__( self , N , I , i , y )\n",
    "        \n",
    "        # if we change parameters from now on, redo\n",
    "        self.do_update_data = False\n",
    "        \n",
    "        # should we PDF-weight the samples? \n",
    "        self.weighted = True if weighted else False # \"truthy\" filter for this param\n",
    "        \n",
    "        # get samples\n",
    "        self.resample( samples=samples )\n",
    "        \n",
    "        # allocate workspace\n",
    "        self.allocate_workspace()\n",
    "        \n",
    "        # if we change parameters from now on, redo\n",
    "        self.do_update_data = True\n",
    "        \n",
    "    def resample( self , samples=1000 ) : \n",
    "        \n",
    "        # define basic sampling data: S random normal samples\n",
    "        self.S = samples\n",
    "        self.v = randn( self.S )\n",
    "        \n",
    "        # \"weight\" vector for summing/weighting samples\n",
    "        if self.weighted : \n",
    "            self.w = gaussian.pdf( self.v ) ; self.w = self.w / self.w.sum()\n",
    "        else : \n",
    "            self.w = np.ones( self.S ) / self.S\n",
    "            \n",
    "        if self.do_update_data :\n",
    "            self.allocate_workspace()\n",
    "            \n",
    "        return\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gauss-Legendre Quadrature \n",
    "\n",
    "Divide the real line into $Q+2$ intervals\n",
    "$$\n",
    "    (\\infty,p_1] \\; , \\; (p_1,p_2] \\; , \\; \\dotsc \\; , \\; (p_Q,p_{Q+1}] \\; , \\; (p_{Q+1},\\infty)\n",
    "$$\n",
    "using $Q+1$ points \n",
    "$$\n",
    "    p_1 \\; < \\; p_2 \\; < \\; \\dotsb \\; < \\; p_{Q+1}\n",
    "$$\n",
    "and take \n",
    "$$\n",
    "    \\int e^{ L_i( \\mu + \\sigma v ) }\\phi(v)dv\n",
    "        \\;\\; \\approx \\;\\; \\sum_{q=1}^Q \\int_{p_q}^{p_{q+1}} e^{ L_i( \\mu + \\sigma v ) }\\phi(v)dv\n",
    "$$\n",
    "presuming $p_{1},p_{Q+1}$ are sufficiently large in magnitude so that\n",
    "$$\n",
    "    \\int_{-\\infty}^{p_{1}} e^{ L_i( \\mu + \\sigma v ) }\\phi(v)dv\n",
    "        \\;\\; , \\;\\; \\int_{p_{Q+1}}^{\\infty} e^{ L_i( \\mu + \\sigma v ) }\\phi(v)dv\n",
    "        \\;\\; \\approx \\;\\; 0. \n",
    "$$\n",
    "This allows us to use standard [Gauss-Legendre](https://en.wikipedia.org/wiki/Gaussian_quadrature#Gauss%E2%80%93Legendre_quadrature) approximations\n",
    "$$\n",
    "    \\int_{p_q}^{p_{q+1}} e^{ L_i( \\mu + \\sigma v ) }\\phi(v)dv\n",
    "        \\;\\; \\approx \\;\\; \n",
    "            \\triangle_s \\sum_{k=1}^K \\alpha_k e^{ L_i( \\mu + \\sigma v_{q,k} ) } \\phi(v_{q,k})\n",
    "           \\quad\\quad\n",
    "           v_{q,k} = \\triangle_q \\xi_k + \\Gamma_q\n",
    "$$\n",
    "for some $K$-point integration rule with nodes $\\xi_k \\in [-1,1]$ and weights $\\alpha_k$, where\n",
    "$$\n",
    "    \\triangle_q = \\frac{p_{q+1}-p_{q}}{2}\n",
    "    \\quad\\quad\\text{and}\\quad\\quad\n",
    "    \\Gamma_q = \\frac{p_{q}+p_{q+1}}{2}\n",
    "$$\n",
    "Hence\n",
    "$$\n",
    "       \\int e^{ L_i( \\mu + \\sigma v ) }\\phi(v)dv\n",
    "           \\approx \\sum_{q=1}^Q \\triangle_q \\sum_{k=1}^K \\alpha_k e^{ L_i( \\mu + \\sigma v_{q,k} ) }\n",
    "                    \\phi(v_{q,k})\n",
    "            = \\sum_{s=1}^{QK} w_s e^{ L_i( \\mu + \\sigma v_s ) } \n",
    "$$\n",
    "letting $w_s \\sim \\triangle_q \\alpha_k \\phi(v_s)$ (for an appropriate $s \\to (q,k)$ index transformation). In this way we can absorb these quadrature-specific terms into the weights corresponding to specific samples. \n",
    "\n",
    "To compute, set\n",
    "$$\n",
    "    S = QK\n",
    "    \\quad\\quad\n",
    "    \\mathbf{V} = \\boldsymbol{\\triangle} \\boldsymbol{\\xi}^\\top + \\boldsymbol{\\Gamma}\\mathbf{1}^\\top\n",
    "    \\quad\\quad\n",
    "    \\mathbf{v} = \\mathrm{vec}( \\mathbf{V} )\n",
    "    \\quad\\quad\n",
    "    \\mathbf{w} = \\mathrm{vec}( \\; \\boldsymbol{\\triangle} \\boldsymbol{\\alpha}^\\top * \\phi(\\mathbf{V}) \\; )\n",
    "$$\n",
    "and apply the generic weighted sample formulation above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class GRCLogitGLQ( RCLogit ) : \n",
    "        \n",
    "    type = \"Gaussian RC Logit (GLQ)\"\n",
    "    \n",
    "    def __init__( self , N , I , i , y , quad_order=3 , partition=None ) : \n",
    "    \n",
    "        # initialize the superclass\n",
    "        RCLogit.__init__( self , N , I , i , y )\n",
    "        \n",
    "        # don't update in the routines below\n",
    "        self.do_update_data = False\n",
    "        \n",
    "        # define basic quadrature data\n",
    "        self.quadorder( K=quad_order )\n",
    "        \n",
    "        # define the basic partition data\n",
    "        if partition is None : \n",
    "            self.partition()\n",
    "        else : \n",
    "            if 'min' not in partition : partition['min'] = -3.0\n",
    "            if 'max' not in partition : partition['max'] =  4.0\n",
    "            if 'num' not in partition : partition['num'] =  10\n",
    "            self.partition( pmin=partition['min'] , pmax=partition['max'] , pnum=partition['num'] )\n",
    "        \n",
    "        # define the nodes and weights\n",
    "        self.define_nodes_and_weights()\n",
    "        \n",
    "        # allocate workspace\n",
    "        self.allocate_workspace()\n",
    "        \n",
    "        # if we change parameters from now on, redo\n",
    "        self.do_update_data = True\n",
    "        \n",
    "    def plot_quadrature( self ) :\n",
    "        plt.figure( )\n",
    "        for p in self.qp : plt.plot( [p,p] , [0,1] , '--b' )\n",
    "        plt.plot( self.v , 0.0 * np.ones( self.S ) , '.k' )\n",
    "        plt.plot( self.v , gaussian.pdf( self.v ) , '-k' )\n",
    "        return\n",
    "\n",
    "    def quadorder( self , K=3 ) : \n",
    "        \n",
    "        if K < 1 or K > 5 : \n",
    "            raise ArgumentError( \"only handling quadrature for K in {1,2,3,4,5}\" )\n",
    "            \n",
    "        self.K = K\n",
    "        if K == 1 : \n",
    "            self.xi = np.array( [ 0.0 ] )\n",
    "            self.qw = np.array( [ 2.0 ] )\n",
    "        elif K == 2 :\n",
    "            self.xi = np.array( [ -0.57735 , 0.57735 ] )\n",
    "            self.qw = np.array( [  1.00000 , 1.00000 ] )\n",
    "        elif K == 3 :\n",
    "            self.xi = np.array( [ -0.774597 , 0.000000 , 0.774597 ] )\n",
    "            self.qw = np.array( [  0.555556 , 0.888889 , 0.555556 ] )\n",
    "        elif K == 4 :\n",
    "            self.xi = np.array( [ -0.861136 , -0.339981 , 0.339981 , 0.861136 ] )\n",
    "            self.qw = np.array( [  0.347855 ,  0.652145 , 0.652145 , 0.347855 ] )\n",
    "        else :\n",
    "            self.xi = np.array( [ -0.906180 , -0.538469 , 0.000000 , 0.538469 , 0.906180 ] )\n",
    "            self.qw = np.array( [  0.236927 ,  0.478629 , 0.568889 , 0.478629 , 0.236927 ] )\n",
    "            \n",
    "        if self.do_update_data : \n",
    "            self.define_nodes_and_weights()\n",
    "            self.allocate_workspace()\n",
    "            \n",
    "        return\n",
    "        \n",
    "    def partition( self , pmin=-3.0 , pmax=4.0 , pnum=10 ) : \n",
    "        pdel = ( pmax - pmin ) / pnum\n",
    "        p = np.exp( 0.5 * np.arange( pmin , pmax + 0.5*pdel , pdel ) )\n",
    "        self.qp = np.concatenate( ( -p[::-1] , [0] , p )  )\n",
    "        self.Q = len( self.qp ) - 1\n",
    "        self.delta = ( self.qp[1:] - self.qp[:-1] ) / 2.0 # S-vector, approximation interval radii\n",
    "        self.gamma = ( self.qp[1:] + self.qp[:-1] ) / 2.0 # S-vector, approximation interval midpoints \n",
    "        if self.do_update_data : \n",
    "            self.define_nodes_and_weights()\n",
    "            self.allocate_workspace()\n",
    "        return\n",
    "    \n",
    "    def define_nodes_and_weights( self ) : \n",
    "        self.S = self.Q * self.K\n",
    "        V = np.outer( self.delta , self.xi ) + np.tile( self.gamma.reshape((self.Q,1)) , (1,self.K) )\n",
    "        self.v = V.copy().flatten()\n",
    "        self.w = ( np.outer( self.delta , self.qw ) * gaussian.pdf( V ) ).flatten()\n",
    "        return\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## idLogit\n",
    "\n",
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `idLogit` for this situation is\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\max &\\quad \\frac{1}{N} \\sum_{i=1}^I \\log( 1 + e^{-y_n(\\beta+\\delta_{i_n})} )\n",
    "                    + \\frac{\\Lambda_1}{N} || \\boldsymbol{\\delta} ||_1 \n",
    "                    + \\frac{\\Lambda_2}{2N} || \\boldsymbol{\\delta} ||_2^2 \\\\\n",
    "    \\text{w.r.t.} &\\quad \\beta , \\delta_1, \\dotsc , \\delta_I \\\\\n",
    "    \\text{s.to} &\\quad \\delta_1 + \\dotsb + \\delta_I = 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "or, in smooth NLP form, \n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\max &\\quad \\frac{1}{N} \\sum_{n=1}^N \\log( 1 + e^{-y_n(\\beta+\\delta_{i_n})} )\n",
    "                    + \\frac{\\Lambda_1}{N} \\sum_{i=1}^I s_i\n",
    "                    + \\frac{\\Lambda_2}{2N} || \\boldsymbol{\\delta} ||_2^2 \\\\\n",
    "    \\text{w.r.t.} &\\quad \\beta , \\boldsymbol{\\delta} , \\mathbf{s} \\\\\n",
    "    \\text{s.to} &\\quad \\delta_1 + \\dotsb + \\delta_I = 0 \\\\\n",
    "        &\\quad \\mathbf{s} - \\boldsymbol{\\delta} \\geq \\mathbf{0} \\;\\; , \\;\\; \n",
    "                \\mathbf{s} + \\boldsymbol{\\delta} \\geq \\mathbf{0}\n",
    "\\end{aligned}\n",
    "$$\n",
    "Writing the log likelihood part of the objective as\n",
    "$$\n",
    "    \\frac{1}{N} \\sum_{n=1}^N \\log( 1 + e^{\\mathbf{z}_n^\\top\\mathbf{x}} )\n",
    "    \\quad\\quad \\mathbf{x} = \\begin{pmatrix} \\beta \\\\ \\boldsymbol{\\delta} \\end{pmatrix}\n",
    "$$\n",
    "the gradient is\n",
    "$$\n",
    "    \\begin{pmatrix}\n",
    "        \\frac{1}{N} \\sum_{n=1}^N P_n(\\mathbf{x}) \\mathbf{z}_n\n",
    "        + \\begin{pmatrix} 0 \\\\ \\frac{\\Lambda_2}{N} \\boldsymbol{\\delta} \\end{pmatrix}\n",
    "        \\\\\n",
    "        \\frac{\\Lambda_1}{N} \\mathbf{1}\n",
    "        \\end{pmatrix}\n",
    "        \\quad\\text{where}\\quad\n",
    "        P_n(\\mathbf{x}) = \\frac{ e^{\\mathbf{z}_n^\\top\\mathbf{x}} }{ 1 + e^{\\mathbf{z}_n^\\top\\mathbf{x}} }\n",
    "$$\n",
    "Here the Hessian is even pretty straightforward: The first $1+I$ components are\n",
    "$$\n",
    "    \\frac{1}{N} \\sum_{n=1}^N P_n(\\mathbf{x})(1-P_n(\\mathbf{x})) \\mathbf{z}_n \\mathbf{z}_n^\\top\n",
    "        + \\frac{\\Lambda_2}{N} \\begin{pmatrix} 0 & \\mathbf{0} \\\\ \\mathbf{0} & \\mathbf{I} \\end{pmatrix}\n",
    "$$\n",
    "and there are no \"$\\mathbf{s}$\" components. Thus Hessian-vector products are\n",
    "$$\n",
    "    \\mathbf{H}\\begin{pmatrix}u\\\\\\mathbf{v}\\\\\\mathbf{w}\\end{pmatrix}\n",
    "        = \\begin{pmatrix} \n",
    "            \\frac{1}{N} \\sum_{n=1}^N \\left( \\mathbf{z}_n^\\top\n",
    "                \\begin{pmatrix} u \\\\ \\mathbf{v} \\end{pmatrix} \\right) P_n(\\mathbf{x})(1-P_n(\\mathbf{x})) \\mathbf{z}_n \n",
    "                + \\frac{\\Lambda_2}{N} \\begin{pmatrix} 0 \\\\ \\mathbf{v} \\end{pmatrix}\n",
    "              \\\\\n",
    "             \\mathbf{0}  \n",
    "        \\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class idLogit( FittableModel ) : \n",
    "        \n",
    "    type = \"idLogit\"\n",
    "    \n",
    "    def __init__( self , N , I , i , y , Lambda1=None , Lambda2=None ) : \n",
    "        \n",
    "        self.N , self.I , self.i , self.y = N , I , i , y\n",
    "        self.Nvars , self.Ncons = 1 + 2*I , 1 + 2*I\n",
    "        self.Ip1 = I + 1\n",
    "        \n",
    "        # map of coefficients to \"-y(b+d)\" terms\n",
    "        Znnzs = 2*N\n",
    "        Zdata = np.zeros( Znnzs )\n",
    "        Zrows , Zcols = np.zeros( Znnzs , dtype=np.int ) , np.zeros( Znnzs , dtype=np.int )\n",
    "        \n",
    "        Zdata[:N] , Zdata[N:] = -y , -y\n",
    "        Zrows[:N] , Zrows[N:] = np.arange(N) , np.arange(N) \n",
    "        Zcols[:N] , Zcols[N:] = 0 , 1 + i\n",
    "        \n",
    "        self.Z = csr_matrix( (Zdata,(Zrows,Zcols)) , shape=( N , 1+I ) )\n",
    "        \n",
    "        # spy( self.Z )\n",
    "        \n",
    "        # constraints\n",
    "        \n",
    "        lo , up = np.zeros( self.Ncons ) , np.inf * np.ones( self.Ncons ) ; up[0] = 0.0\n",
    "        \n",
    "        Cnnzs = 5*I\n",
    "        Cdata = np.ones( Cnnzs ) # almost all entries are ones\n",
    "        Crows , Ccols = np.zeros( Cnnzs , dtype=np.int ) , np.zeros( Cnnzs , dtype=np.int )\n",
    "        \n",
    "        Crows[0*I:1*I] , Ccols[0*I:1*I] = 0 , 1 + np.arange(I)\n",
    "        Crows[1*I:2*I] , Ccols[1*I:2*I] , Cdata[1*I:2*I] = 1 + np.arange(I) , 1 + np.arange(I) , -1.0\n",
    "        Crows[2*I:3*I] , Ccols[2*I:3*I] = 1 +     np.arange(I) , 1 + I + np.arange(I)\n",
    "        Crows[3*I:4*I] , Ccols[3*I:4*I] = 1 + I + np.arange(I) , 1 +     np.arange(I)\n",
    "        Crows[4*I:5*I] , Ccols[4*I:5*I] = 1 + I + np.arange(I) , 1 + I + np.arange(I)\n",
    "        \n",
    "        Cmtrx = csr_matrix( (Cdata,(Crows,Ccols)) , shape=( self.Ncons , self.Nvars ) )\n",
    "        \n",
    "        # spy( Cmtrx )\n",
    "            \n",
    "        self.cons = LinearConstraint( Cmtrx , lo , up )\n",
    "        \n",
    "        # initial regularization\n",
    "        L1 = Lambda1 if Lambda1 is not None else self.N\n",
    "        L2 = Lambda2 if Lambda2 is not None else self.N\n",
    "        self.regularize( Lambda1=L1 , Lambda2=L2 )\n",
    "        \n",
    "        self.zp = np.zeros((self.N,),dtype=np.float)\n",
    "        self.eU = np.zeros((self.N,),dtype=np.float)\n",
    "        self.ll = np.zeros((self.N,),dtype=np.float)\n",
    "        self.PL = np.zeros((self.N,),dtype=np.float)\n",
    "\n",
    "    def obj( self , p ) :\n",
    "        np.exp( self.Z @ p[:self.Ip1] , self.eU )\n",
    "        np.log1p( self.eU , self.ll )\n",
    "        return np.sum( self.ll ) / self.N \\\n",
    "                    + self.L1 * np.sum( p[self.Ip1:] ) \\\n",
    "                    + self.L2 * np.sum( p[1:self.Ip1] * p[1:self.Ip1] ) / 2.0\n",
    "        \n",
    "    def grad( self , p ) :  \n",
    "        g = np.zeros( self.Nvars )\n",
    "        np.exp( self.Z @ p[:self.Ip1] , out=self.eU )\n",
    "        np.divide( self.eU , 1.0 + self.eU , out=self.PL )\n",
    "        g[:self.Ip1] = self.Z.T @ self.PL / N\n",
    "        if( self.L2 > 0.0 ) : g[1:self.Ip1] += self.L2 * p[1:self.Ip1]\n",
    "        g[self.Ip1:] = self.L1\n",
    "        return g\n",
    "    \n",
    "    def hessp( self , p , v ) :  \n",
    "        h = np.zeros( self.Nvars )\n",
    "        zv = self.Z @ v[:self.Ip1]\n",
    "        np.exp( self.Z @ p[:self.Ip1] , out=self.eU )\n",
    "        np.divide( self.eU , 1.0 + self.eU , out=self.PL )\n",
    "        self.PL = self.PL * ( 1.0 - self.PL )\n",
    "        h[:self.Ip1] = self.Z.T @ ( zv * self.PL ) / N\n",
    "        if( self.L2 > 0.0 ) : h[1:self.Ip1] += self.L2 * v[1:self.Ip1]\n",
    "        return h\n",
    "    \n",
    "    def regularize( self , Lambda1=None , Lambda2=None ) : \n",
    "        if Lambda1 is not None : self.L1 = Lambda1 / self.N\n",
    "        if Lambda2 is not None : self.L2 = Lambda2 / self.N\n",
    "            \n",
    "    def draw_initial_condition( self ) : \n",
    "        p0 = randn( self.Nvars )\n",
    "        p0[1:] = p0[1:] - p0[1:].mean()\n",
    "        return p0\n",
    "    \n",
    "    def solve( self , p0=None ) : \n",
    "        self.soln = minimize( self.obj , p0 , jac=self.grad , hessp=self.hessp , \\\n",
    "                            method='trust-constr' , constraints=self.cons , \\\n",
    "                           options={ 'maxiter' : 100000 } )\n",
    "        return self.soln\n",
    "    \n",
    "    def trace( self , p0=None , Lambda1s=None , alpha=None ) : \n",
    "        p0 = self.draw_initial_condition() if p0 is None else p0.flatten()\n",
    "        if Lambda1s is None : Lambda1s = 10.0 ** np.arange(1,10,1)[::-1]\n",
    "        if alpha is None : alpha = 1.0\n",
    "        self.trace = [ None for l in range(Lambda1s.size) ]\n",
    "        for l in range(Lambda1s.size) : \n",
    "            L = Lambda1s[l]\n",
    "            self.regularize( Lambda1=L/self.N , Lambda2=alpha*L/self.N )\n",
    "            self.trace[l] = self.fit( p0=p0 )\n",
    "            p0 = trace[l].x\n",
    "        return self\n",
    "    \n",
    "    def printx( self ) : \n",
    "        if self.soln is None : return \"(no solution)\"\n",
    "        k2 , p = normaltest( self.soln['x'][1:I+1] )\n",
    "        return \"%0.2f ( %0.2f , %0.4f )\" % ( self.soln['x'][0] , self.soln['x'][1:I+1].std() , p )\n",
    "    \n",
    "    def getx( self ) : \n",
    "        if self.soln is None : return None\n",
    "        else : \n",
    "            k2 , p = normaltest( self.soln['x'][1:I+1] )\n",
    "            return np.array( [ self.soln['x'][0] , self.soln['x'][1:I+1].std() , p ] )\n",
    "    \n",
    "    def kld( self , PT ) : \n",
    "        _ , Ni = np.unique( i , return_counts=True )\n",
    "        Wi = Ni / self.N # individual observation weights\n",
    "        B = self.soln['x'][1:I+1] + self.soln['x'][0]\n",
    "        PL = np.zeros( self.I )\n",
    "        ind = np.where( B >  0.0 )[0] ; PL[ind] = 1.0 / ( 1.0 + np.exp(-B[ind]) )\n",
    "        ind = np.where( B <= 0.0 )[0] ; PL[ind] = np.exp(B[ind]) ; PL[ind] = PL[ind] / ( 1.0 + PL[ind] )\n",
    "        P = np.sum( PL * Wi )\n",
    "        return PT * np.log( PT/P ) + (1.0-PT) * np.log( (1.0-PT)/(1.0-P) )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generating Processes\n",
    "\n",
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define some simple data generating processes that map $(N,I,\\mathbf{i})$ (along with possibly other parameters) to draws of $\\mathbf{y}$. Below we define functions that will randomly draw from a Logit, a Latent Class Logit, and a (Gaussian) Random Coefficients Logit model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# multinomial Logit data generating process\n",
    "def mnl_dgp( N , I , i ) : \n",
    "    \n",
    "    pT = 2.0 * rand() - 1.0\n",
    "    \n",
    "    print( pT )\n",
    "    \n",
    "    if pT > 0 : PT = 1.0 / ( 1.0 + np.exp( -pT ) )\n",
    "    else : PT = np.exp( pT ) ; PT = PT / ( 1.0 + PT )\n",
    "    y = 2.0 * ( rand(N) <= PT ) - 1.0\n",
    "    \n",
    "    return y , PT\n",
    "\n",
    "# Latent Class Logit data generating process\n",
    "def lcl_dgp( N , I , i , C ) : \n",
    "    \n",
    "    w = rand(C) ; w = w / w.sum()\n",
    "    pT = 2.0 * rand(C) - 1.0\n",
    "    ic = randc(C,I,w)\n",
    "    eU = np.exp( pT[ ic[i] ] )\n",
    "    PL = eU / ( 1.0 + eU )\n",
    "    y = 2.0 * ( rand(N) <= PL ) - 1.0\n",
    "    \n",
    "    PL = np.exp( pT ) ; PL = PL / ( 1.0 + PL )\n",
    "    PT = np.sum( PL * w )\n",
    "    \n",
    "    return y , PT\n",
    "\n",
    "# Gaussian Random Coefficient Logit data generating process\n",
    "def grc_dgp( N , I , i ) : \n",
    "    \n",
    "    pT = rand(2) ; pT[0] = 2.0 * pT[0] - 1.0\n",
    "    \n",
    "    print( pT )\n",
    "    \n",
    "    v  = pT[0] + pT[1] * randn(I)\n",
    "    eU = np.exp( v[i] )\n",
    "    PL = eU / ( 1.0 + eU )\n",
    "    y  = 2.0 * ( rand(N) <= PL ) - 1.0\n",
    "    \n",
    "    v   = pT[0] + pT[1] * randn( int(1e6) )\n",
    "    PL  = np.zeros( int(1e6) )\n",
    "    ind = np.where( v >  0.0 )[0] ; PL[ind] = 1.0 / ( 1.0 + np.exp( -v[ind] ) )\n",
    "    ind = np.where( v <= 0.0 )[0] ; PL[ind] = np.exp( v[ind] ) ; PL[ind] = PL[ind] / ( 1.0 + PL[ind] )\n",
    "    PT  = PL.mean()\n",
    "    \n",
    "    return y , PT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation Experiments\n",
    "\n",
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To draw a simulation experiment, we need to choose or draw $(N,I)$, then $i_n \\in \\{1,\\dotsc,I\\}$ for all $n$, and then choice dummies $y_n$ following some data generating process. Below we wrap the second two parts in a single function, `draw_experiment`. Then we create model object instances from the resultant data for each of our model types. \n",
    "\n",
    "Just to make sure everything works, we run a test that just (a) checks the gradient for each model and (b) fits the model on the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.24907225351595952\n",
      "Logit :: Checking gradient...\n",
      "0.1000000000000000 , 0.0124346836740238\n",
      "0.0100000000000000 , 0.0012460218368490\n",
      "0.0010000000000000 , 0.0001246231591542\n",
      "0.0001000000000000 , 0.0000124625235973\n",
      "0.0000100000000000 , 0.0000012462567930\n",
      "0.0000010000000000 , 0.0000001246428802\n",
      "0.0000001000000000 , 0.0000000132875108\n",
      "0.0000000100000000 , 0.0000000011453885\n",
      "0.0000000010000000 , 0.0000001010654607\n",
      "0.0000000001000000 , 0.0000016752913787\n",
      "Logit :: Fitting model...\n",
      "Logit :: Solver ran in 0.012133 seconds\n",
      "Logit :: Solver message: `gtol` termination condition is satisfied.\n",
      "Logit :: Formatted solution: -0.28\n",
      "Logit :: Kullback-Leibler Divergence at solution : 0.000101\n",
      "Logit :: Relative Average Likelihood at solution : 1.000\n",
      " \n",
      "Latent Class Logit :: Checking gradient...\n",
      "0.1000000000000000 , 0.0074115349231856\n",
      "0.0100000000000000 , 0.0006851778380661\n",
      "0.0010000000000000 , 0.0000678640654999\n",
      "0.0001000000000000 , 0.0000067797812024\n",
      "0.0000100000000000 , 0.0000006779099719\n",
      "0.0000010000000000 , 0.0000000678979311\n",
      "0.0000001000000000 , 0.0000000079458877\n",
      "0.0000000100000000 , 0.0000000199033259\n",
      "0.0000000010000000 , 0.0000002531304927\n",
      "0.0000000001000000 , 0.0000013987066845\n",
      "Latent Class Logit :: Fitting model...\n",
      "Latent Class Logit :: Solver ran in 0.070185 seconds\n",
      "Latent Class Logit :: Solver message: `gtol` termination condition is satisfied.\n",
      "Latent Class Logit :: Formatted solution: ( -0.28 , 0.65 ) , ( -0.28 , 0.28 ) , ( -0.28 , 0.08 )\n",
      "Latent Class Logit :: Kullback-Leibler Divergence at solution : 0.000101\n",
      "Latent Class Logit :: Relative Average Likelihood at solution : 1.000\n",
      " \n",
      "Class Class Logit :: Checking gradient...\n",
      "0.1000000000000000 , 0.0017925218517886\n",
      "0.0100000000000000 , 0.0002112392191352\n",
      "0.0010000000000000 , 0.0000214353027624\n",
      "0.0001000000000000 , 0.0000021466341001\n",
      "0.0000100000000000 , 0.0000002147061492\n",
      "0.0000010000000000 , 0.0000000216716719\n",
      "0.0000001000000000 , 0.0000000033054166\n",
      "0.0000000100000000 , 0.0000000168304123\n",
      "0.0000000010000000 , 0.0000001700829006\n",
      "0.0000000001000000 , 0.0000015811805141\n",
      "Class Class Logit :: Fitting model...\n",
      "Class Class Logit :: Solver ran in 1.672211 seconds\n",
      "Class Class Logit :: Solver message: `gtol` termination condition is satisfied.\n",
      "Class Class Logit :: Formatted solution: ( -0.93 , 0.39 ) , ( 0.58 , 0.24 ) , ( -0.16 , 0.37 )\n",
      "Class Class Logit :: Kullback-Leibler Divergence at solution : 0.000031\n",
      "Class Class Logit :: Relative Average Likelihood at solution : 1.000\n",
      " \n",
      "Gaussian RC Logit (SAA) :: Checking gradient...\n",
      "0.1000000000000000 , 0.0093779975164274\n",
      "0.0100000000000000 , 0.0009418083385613\n",
      "0.0010000000000000 , 0.0000942193356110\n",
      "0.0001000000000000 , 0.0000094223166489\n",
      "0.0000100000000000 , 0.0000009422366773\n",
      "0.0000010000000000 , 0.0000000942372289\n",
      "0.0000001000000000 , 0.0000000084169891\n",
      "0.0000000100000000 , 0.0000000145624761\n",
      "0.0000000010000000 , 0.0000001038961692\n",
      "0.0000000001000000 , 0.0000020055275776\n",
      "Gaussian RC Logit (SAA) :: Fitting model...\n",
      "Gaussian RC Logit (SAA) :: Solver ran in 0.603679 seconds\n",
      "Gaussian RC Logit (SAA) :: Solver message: `gtol` termination condition is satisfied.\n",
      "Gaussian RC Logit (SAA) :: Formatted solution: -0.28 ( 0.00 )\n",
      "Gaussian RC Logit (SAA) :: Kullback-Leibler Divergence at solution : 0.000101\n",
      "Gaussian RC Logit (SAA) :: Relative Average Likelihood at solution : 1.000\n",
      " \n",
      "Gaussian RC Logit (GLQ) :: Checking gradient...\n",
      "0.1000000000000000 , 0.0081469751688260\n",
      "0.0100000000000000 , 0.0008149020992046\n",
      "0.0010000000000000 , 0.0000814913357064\n",
      "0.0001000000000000 , 0.0000081491454965\n",
      "0.0000100000000000 , 0.0000008149211575\n",
      "0.0000010000000000 , 0.0000000817742810\n",
      "0.0000001000000000 , 0.0000000092767175\n",
      "0.0000000100000000 , 0.0000000340338584\n",
      "0.0000000010000000 , 0.0000002338740028\n",
      "0.0000000001000000 , 0.0000004559186077\n",
      "Gaussian RC Logit (GLQ) :: Fitting model...\n",
      "Gaussian RC Logit (GLQ) :: Solver ran in 0.100177 seconds\n",
      "Gaussian RC Logit (GLQ) :: Solver message: `gtol` termination condition is satisfied.\n",
      "Gaussian RC Logit (GLQ) :: Formatted solution: -0.28 ( 0.00 )\n",
      "Gaussian RC Logit (GLQ) :: Kullback-Leibler Divergence at solution : 0.000101\n",
      "Gaussian RC Logit (GLQ) :: Relative Average Likelihood at solution : 1.000\n",
      " \n",
      "idLogit :: Checking gradient...\n",
      "0.1000000000000000 , 0.0501641601432264\n",
      "0.0100000000000000 , 0.0050165887763295\n",
      "0.0010000000000000 , 0.0005016605690297\n",
      "0.0001000000000000 , 0.0000501661680526\n",
      "0.0000100000000000 , 0.0000050175720797\n",
      "0.0000010000000000 , 0.0000005156231411\n",
      "0.0000001000000000 , 0.0000002131830983\n",
      "0.0000000100000000 , 0.0000015742179952\n",
      "0.0000000010000000 , 0.0000178463087650\n",
      "0.0000000001000000 , 0.0001599548559170\n",
      "idLogit :: Fitting model...\n",
      "idLogit :: Solver ran in 0.194901 seconds\n",
      "idLogit :: Solver message: `gtol` termination condition is satisfied.\n",
      "idLogit :: Formatted solution: -0.28 ( 0.00 , 0.0000 )\n",
      "idLogit :: Kullback-Leibler Divergence at solution : 0.000101\n",
      "idLogit :: Relative Average Likelihood at solution : 1.000\n",
      " \n"
     ]
    }
   ],
   "source": [
    "def draw_experiment( N , I , dgp ) : \n",
    "    i = randi( I , N ) ; y , PT = dgp( N , I , i )\n",
    "    return i , y , PT\n",
    "\n",
    "N , I = 1000 , 100\n",
    "i , y , PT = draw_experiment( N , I , mnl_dgp )\n",
    "\n",
    "# define simple Logit\n",
    "mnl = Logit( N , y )\n",
    "\n",
    "# define latent class Logit instances, for a specific number of modeled classes\n",
    "M = 3\n",
    "lcl = LatentClassLogit( N , I , i , y , M )\n",
    "ccl = ClassClassLogit( N , I , i , y , M )\n",
    "\n",
    "# define sample average and gauss-legendre random coefficient Logit instances\n",
    "saa = GRCLogitSAA( N , I , i , y )\n",
    "glq = GRCLogitGLQ( N , I , i , y )\n",
    "\n",
    "# finally, define an idLogit instance\n",
    "idl = idLogit( N , I , i , y )\n",
    "\n",
    "def basic_test( prob ) : \n",
    "    print( \"%s :: Checking gradient...\" % ( prob.type ) )\n",
    "    prob.grad_check(  )\n",
    "    print( \"%s :: Fitting model...\" % ( prob.type ) )\n",
    "    soln = prob.fit().soln\n",
    "    print( \"%s :: Solver ran in %0.6f seconds\" % ( prob.type , soln.solvertime ) )\n",
    "    print( \"%s :: Solver message: %s\" % ( prob.type , soln.message ) )\n",
    "    print( \"%s :: Formatted solution: %s\" % ( prob.type , prob.printx() ) )\n",
    "    kld = prob.kld( PT )\n",
    "    print( \"%s :: Kullback-Leibler Divergence at solution : %0.6f\" % ( prob.type , kld ) )\n",
    "    print( \"%s :: Relative Average Likelihood at solution : %0.3f\" % ( prob.type , np.exp( - kld ) ) )\n",
    "    \n",
    "basic_test( mnl ) ; print(\" \")\n",
    "basic_test( lcl ) ; print(\" \")\n",
    "basic_test( ccl ) ; print(\" \")\n",
    "basic_test( saa ) ; print(\" \")\n",
    "basic_test( glq ) ; print(\" \")\n",
    "basic_test( idl ) ; print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now that we have some confidence that everything actually works, we can run some real tests. \n",
    "\n",
    "The following code helps us abstract away from the mechanics behind doing the tests themselves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_model_object( code , N , I , i , y , data={} ) : \n",
    "    \"\"\"\n",
    "    This is a convenience function for instantiating model objects from a \n",
    "    code, basic data shared by all models, and optional additional data that\n",
    "    a class might require or use. \n",
    "    \"\"\"\n",
    "    if data is None : data = {} # generic empty object for use \n",
    "    if   code == \"mnl\" : return Logit( N , y[:N] )\n",
    "    elif code == \"lcl\" : return LatentClassLogit( N , I , i[:N] , y[:N] , **data )\n",
    "    elif code == \"ccl\" : return ClassClassLogit( N , I , i[:N] , y[:N] , **data )\n",
    "    elif code == \"saa\" : return GRCLogitSAA( N , I , i[:N] , y[:N] )\n",
    "    elif code == \"glq\" : return GRCLogitGLQ( N , I , i[:N] , y[:N] )\n",
    "    elif code == \"idl\" : return idLogit( N , I , i[:N] , y[:N] , **data )\n",
    "    else : return None\n",
    "    \n",
    "def fit_model( prob , p0=None , PT=None , timeout=None , verbose=True ) : \n",
    "    \n",
    "    res = { 'N' : prob.N , 'p' : None , 'x' : None , 'k' : None , \\\n",
    "               't' : None , 'e' : True , 'm' : None , 's' : None }\n",
    "    \n",
    "    try : \n",
    "        prob.fit( maxtime=timeout , p0=p0 )\n",
    "        if prob.soln is None    : raise Exception( \"indeterminate solver error\" )\n",
    "        if prob.soln['timeout'] : raise Exception( \"solver timeout\" )\n",
    "        if prob.soln['error']   : raise prob.soln['message']\n",
    "        res['s'] = prob.soln['status']\n",
    "        res['p'] = prob.printx()\n",
    "        res['x'] = prob.getx()\n",
    "        res['t'] = prob.soln['solvertime']\n",
    "        res['e'] = prob.soln['error']\n",
    "        res['m'] = prob.soln['message']\n",
    "        if ( PT is not None ) and ( not prob.soln['error'] ) : res['k'] = prob.kld( PT )\n",
    "        if verbose : \n",
    "            print( \"%s :: Solve attempt finished (%i) for N = %i\" \\\n",
    "                      % ( prob.type , prob.soln['status'] , prob.N ) )\n",
    "            \n",
    "    except Exception as e : \n",
    "        if verbose : print( \"%s :: solve exception occurred: %s\" % ( prob.type , e ) )\n",
    "        try : solvertime = prob.soln['solvertime']\n",
    "        except AttributeError as ae : solvertime = None\n",
    "        res['t'] , res['e'] , res['m'] = solvertime , True , e\n",
    "        \n",
    "    return res\n",
    "    \n",
    "def fit_sequence( Ns , I , i , y , code , data , PT=None , timeout=None , chain=False , verbose=True ) :\n",
    "    \"\"\"\n",
    "    This is a convenience function for running a sequence of fits over \n",
    "    increasing numbers of observations with structured initial conditions. \n",
    "    We can either \"chain\" the solves by passing the solution for one problem\n",
    "    to the next, or not and keep the same inital\n",
    "    \"\"\"\n",
    "    results , prob_type , p0 = [] , None , None\n",
    "    for N in Ns : \n",
    "        prob , res = None , None\n",
    "        try :\n",
    "            prob = create_model_object( code , N , I , i , y , data )\n",
    "            if prob_type is None : prob_type = prob.type\n",
    "        except Exception as e : \n",
    "            print( \"Error creating model object: \" , e )\n",
    "        if prob is not None : \n",
    "            res = fit_model( prob , p0=p0 , PT=PT , timeout=timeout , verbose=verbose )\n",
    "            p0 = prob.soln['x'] if chain else prob.soln['x0']\n",
    "        results.append( res )\n",
    "    return results, prob_type\n",
    "\n",
    "class FitSequenceTrial( object ) :\n",
    "    \"\"\"\n",
    "    This is a wrapper class to enable parallelization with pool.map by \n",
    "    making a callable object instance storing the desired model data. \n",
    "    \"\"\"\n",
    "    def __init__( self , Ns , I , i , y , PT=None , timeout=None , verbose=None ) : \n",
    "        self.Ns , self.I , self.i , self.y = Ns , I , i , y\n",
    "        self.timeout , self.verbose = timeout , verbose\n",
    "    def __call__( self , params ) : # expects params = { code , data , trial }  \n",
    "        return fit_sequence( self.Ns , self.I , self.i , self.y , params['code'] , params['data'] , \\\n",
    "                                PT=PT , timeout=self.timeout , verbose=self.verbose )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define a class `ModelFitExp` to facilitate running experiments over the sample size ($N$) -- with the same observsational data -- for a variety of models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "class ModelFitExp( object ) : \n",
    "    \n",
    "    def __init__( self , I=0 , dgp=None , obsvnums=None , timeout=None , \\\n",
    "                     models=None , trials=None , processes=None , verbose=True ) : \n",
    "        self.I = I \n",
    "        self.dgp = dgp\n",
    "        self.timeout = timeout\n",
    "        self.models = {}\n",
    "        self.verbose = verbose\n",
    "        self.T = trials if trials is not None else 1 # one trial by default\n",
    "        if models is not None : \n",
    "            for m in models : \n",
    "                self.add_model( m['code'] , data=( m['data'] if 'data' in m else None ) )\n",
    "        self.results = {}\n",
    "        self.Ns = []\n",
    "        if obsvnums is None : \n",
    "            self.Ns = [ int(N) for N in 10.0 ** np.arange( 3 , 6.1 , 0.1 ) ]\n",
    "        else : \n",
    "            self.Ns = obsvnums\n",
    "        self.P = 1\n",
    "        self.parallelize( processes )\n",
    "        self.iy_saved = False\n",
    "    \n",
    "    def be_verbose( self ) : \n",
    "        self.verbose = True\n",
    "        \n",
    "    def be_quiet( self ) : \n",
    "        self.verbose = False\n",
    "    \n",
    "    def set_obsnums( self , Nmin , Nmax , Nnum ) : \n",
    "        try : \n",
    "            Ndel = ( Nmax - Nmin ) / Nnum\n",
    "            self.Ns = [ int(N) for N in 10.0 ** np.arange( Nmin , Nmax + 0.5*Ndel , Ndel ) ]\n",
    "        except Exception as e : \n",
    "            self.Ns = None\n",
    "            raise e\n",
    "            \n",
    "    def set_dgp( self , dgp ) : \n",
    "        self.dgp = dgp\n",
    "    \n",
    "    def set_timeout( self , timeout ) : \n",
    "        self.timeout = timeout\n",
    "        \n",
    "    def add_model( self , code , data=None ) : \n",
    "        mid = random_string( 16 )\n",
    "        self.models[mid] = { 'code' : code , 'data' : data }\n",
    "        return mid\n",
    "        \n",
    "    def del_model( self , mid ) : \n",
    "        if mid in self.models : \n",
    "            del self.models[mid]\n",
    "            \n",
    "    def clear_models( self ) : \n",
    "        del self.models\n",
    "        self.models = {}\n",
    "        \n",
    "    def reset_results( self ) : \n",
    "        del self.results\n",
    "        self.results = {}\n",
    "        \n",
    "    def parallelize( self , procs ) : \n",
    "        try : self.P = int( procs )\n",
    "        except Exception as e : pass\n",
    "        if self.P < 1 : self.P = 1\n",
    "        \n",
    "    def print_models( self ) : \n",
    "        for k in self.models : \n",
    "            print( \"(%s) %s %s\" % ( k , self.models[k]['code'] , \\\n",
    "                                   str(self.models[k]['data']) if self.models[k]['data'] is not None else \"\" ) )\n",
    "        \n",
    "    def print_results( self ) : \n",
    "        for k in self.results : \n",
    "            print( \"%s%s\" % ( self.models[k]['type'] , \"\" if self.models[k]['data'] is None \\\n",
    "                                 else \", %s\" % str(self.models[k]['data']) ) )\n",
    "            t = 1\n",
    "            for trial in self.results[k] : \n",
    "                for i in trial : \n",
    "                    if i['e'] : \n",
    "                        print( \"%i , %i (%s) , %s\" \\\n",
    "                                  % ( t , i['N'] , \"-\" if i['s'] is None else \"%i\" % i['s'] , i['m'] ) )\n",
    "                    else :\n",
    "                        print( \"%i , %i (%s) , %0.2fs , %0.3f , %s \" \\\n",
    "                                  % ( t , i['N'] , \"-\" if i['s'] is None else \"%i\" % i['s'] , \\\n",
    "                                       i['t'] , np.exp( - i['k'] ) , i['p'] ) )\n",
    "                print( \" \" )\n",
    "                t += 1\n",
    "    \n",
    "    def run( self , trials=None , save=False , resample=False ) : \n",
    "        \n",
    "        if trials is not None : self.T = trials\n",
    "        if self.iy_saved and ( not resample ) : \n",
    "            i , y = self.i , self.y\n",
    "        else : \n",
    "            i , y , self.PT = draw_experiment( self.Ns[-1] , self.I , self.dgp )\n",
    "            if save : \n",
    "                self.i , self.y , self.iy_saved = i , y , True\n",
    "                \n",
    "        # parallelize? May not be worthwhile. \n",
    "        if self.P > 1 : \n",
    "            map_params = []\n",
    "            for k in self.models : \n",
    "                if k not in self.results : self.results[k] = [ [] for t in range(self.T) ]\n",
    "                for t in range( self.T ) : \n",
    "                    map_params.append( { \n",
    "                        'code'  : self.models[k]['code'] ,\n",
    "                        'data'  : self.models[k]['data'] ,\n",
    "                        'trial' : t\n",
    "                    } )\n",
    "            with ThreadPool( processes=self.P ) as pool : \n",
    "                FST = FitSequenceTrial( self.Ns , self.I , i , y , \\\n",
    "                                           PT=self.PT , timeout=self.timeout , verbose=self.verbose )\n",
    "                results = pool.map( FST , map_params )\n",
    "            j = 0\n",
    "            for k in self.models : \n",
    "                for t in range( self.T ) : \n",
    "                    self.results[k][t] , self.models[k]['type'] = results[j][0] , results[j][1]\n",
    "                    j += 1\n",
    "                    \n",
    "        # serial trials; seems to be the best\n",
    "        else : \n",
    "            for k in self.models : \n",
    "                if k not in self.results : self.results[k] = []\n",
    "                for t in range( self.T ) : \n",
    "                    while t >= len( self.results[k] ) : self.results[k].append( [] )\n",
    "                    self.results[k][t] , self.models[k]['type'] \\\n",
    "                        = fit_sequence( self.Ns , self.I , i , y , \\\n",
    "                                        self.models[k]['code'] , self.models[k]['data'] , \\\n",
    "                                        PT=self.PT , timeout=self.timeout , verbose=self.verbose )\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logit data\n",
    "\n",
    "Let's take the simplest case in the data, the Logit DGP, but try out each model with more and more data. This will help us _start_ to gauge comparative efficiency. To start, we will apply a one minute (60 second) timeout on the fit attempts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4884605886768756\n",
      "Logit\n",
      "1 , 1000 (1) , 0.03s , 0.998 , 0.37 \n",
      "1 , 1258 (1) , 0.04s , 0.999 , 0.38 \n",
      "1 , 1584 (1) , 0.04s , 0.999 , 0.40 \n",
      "1 , 1995 (1) , 0.03s , 1.000 , 0.45 \n",
      "1 , 2511 (1) , 0.03s , 1.000 , 0.44 \n",
      "1 , 3162 (1) , 0.03s , 1.000 , 0.46 \n",
      "1 , 3981 (1) , 0.03s , 1.000 , 0.50 \n",
      "1 , 5011 (1) , 0.03s , 1.000 , 0.51 \n",
      "1 , 6309 (1) , 0.03s , 1.000 , 0.49 \n",
      "1 , 7943 (1) , 0.03s , 1.000 , 0.50 \n",
      "1 , 10000 (1) , 0.03s , 1.000 , 0.50 \n",
      " \n",
      "2 , 1000 (1) , 0.03s , 0.998 , 0.37 \n",
      "2 , 1258 (1) , 0.03s , 0.999 , 0.38 \n",
      "2 , 1584 (1) , 0.03s , 0.999 , 0.40 \n",
      "2 , 1995 (1) , 0.03s , 1.000 , 0.45 \n",
      "2 , 2511 (1) , 0.03s , 1.000 , 0.44 \n",
      "2 , 3162 (1) , 0.03s , 1.000 , 0.46 \n",
      "2 , 3981 (1) , 0.03s , 1.000 , 0.50 \n",
      "2 , 5011 (1) , 0.03s , 1.000 , 0.51 \n",
      "2 , 6309 (1) , 0.03s , 1.000 , 0.49 \n",
      "2 , 7943 (1) , 0.03s , 1.000 , 0.50 \n",
      "2 , 10000 (1) , 0.03s , 1.000 , 0.50 \n",
      " \n",
      "Gaussian RC Logit (GLQ)\n",
      "1 , 1000 (1) , 0.09s , 0.998 , 0.37 ( 0.18 ) \n",
      "1 , 1258 (1) , 0.10s , 0.999 , 0.38 ( 0.09 ) \n",
      "1 , 1584 (1) , 0.11s , 0.999 , 0.40 ( 0.10 ) \n",
      "1 , 1995 (1) , 0.15s , 1.000 , 0.45 ( 0.00 ) \n",
      "1 , 2511 (1) , 0.23s , 1.000 , 0.44 ( 0.22 ) \n",
      "1 , 3162 (1) , 0.20s , 1.000 , 0.47 ( 0.17 ) \n",
      "1 , 3981 (1) , 0.29s , 1.000 , 0.50 ( 0.03 ) \n",
      "1 , 5011 (1) , 0.24s , 1.000 , 0.52 ( 0.12 ) \n",
      "1 , 6309 (1) , 0.27s , 1.000 , 0.49 ( 0.09 ) \n",
      "1 , 7943 (1) , 0.53s , 1.000 , 0.50 ( 0.08 ) \n",
      "1 , 10000 (1) , 0.81s , 1.000 , 0.50 ( 0.10 ) \n",
      " \n",
      "2 , 1000 (1) , 0.09s , 0.998 , 0.37 ( 0.18 ) \n",
      "2 , 1258 (1) , 0.11s , 0.999 , 0.38 ( 0.09 ) \n",
      "2 , 1584 (1) , 0.11s , 0.999 , 0.40 ( 0.10 ) \n",
      "2 , 1995 (1) , 0.15s , 1.000 , 0.45 ( 0.00 ) \n",
      "2 , 2511 (1) , 0.11s , 1.000 , 0.44 ( 0.22 ) \n",
      "2 , 3162 (1) , 0.12s , 1.000 , 0.47 ( 0.17 ) \n",
      "2 , 3981 (1) , 0.20s , 1.000 , 0.50 ( 0.03 ) \n",
      "2 , 5011 (1) , 0.19s , 1.000 , 0.52 ( 0.12 ) \n",
      "2 , 6309 (1) , 0.26s , 1.000 , 0.49 ( 0.09 ) \n",
      "2 , 7943 (1) , 0.26s , 1.000 , 0.50 ( 0.08 ) \n",
      "2 , 10000 (1) , 0.39s , 1.000 , 0.50 ( 0.10 ) \n",
      " \n",
      "idLogit, {'Lambda1': 10.0, 'Lambda2': 1.0}\n",
      "1 , 1000 (1) , 0.21s , 0.329 , 0.37 ( 0.00 , 0.0317 ) \n",
      "1 , 1258 (1) , 0.26s , 0.288 , 0.38 ( 0.00 , 0.4441 ) \n",
      "1 , 1584 (1) , 0.27s , 0.252 , 0.40 ( 0.00 , 0.0002 ) \n",
      "1 , 1995 (1) , 0.29s , 0.221 , 0.45 ( 0.00 , 0.0000 ) \n",
      "1 , 2511 (2) , 0.97s , 0.192 , 0.44 ( 0.08 , 0.0000 ) \n",
      "1 , 3162 (2) , 0.85s , 0.168 , 0.47 ( 0.13 , 0.0000 ) \n",
      "1 , 3981 (2) , 1.04s , 0.147 , 0.50 ( 0.14 , 0.0003 ) \n",
      "1 , 5011 (1) , 1.10s , 0.128 , 0.52 ( 0.19 , 0.0372 ) \n",
      "1 , 6309 (2) , 1.29s , 0.110 , 0.49 ( 0.20 , 0.0176 ) \n",
      "1 , 7943 (2) , 1.17s , 0.096 , 0.50 ( 0.19 , 0.0329 ) \n",
      "1 , 10000 (2) , 1.59s , 0.083 , 0.50 ( 0.19 , 0.2150 ) \n",
      " \n",
      "2 , 1000 (1) , 0.25s , 0.329 , 0.37 ( 0.00 , 0.0984 ) \n",
      "2 , 1258 (1) , 0.24s , 0.288 , 0.38 ( 0.00 , 0.0856 ) \n",
      "2 , 1584 (1) , 0.24s , 0.252 , 0.40 ( 0.00 , 0.0000 ) \n",
      "2 , 1995 (1) , 0.37s , 0.221 , 0.45 ( 0.00 , 0.0000 ) \n",
      "2 , 2511 (2) , 0.79s , 0.192 , 0.44 ( 0.08 , 0.0000 ) \n",
      "2 , 3162 (2) , 0.92s , 0.168 , 0.47 ( 0.13 , 0.0000 ) \n",
      "2 , 3981 (2) , 1.32s , 0.147 , 0.50 ( 0.15 , 0.0003 ) \n",
      "2 , 5011 (2) , 1.31s , 0.128 , 0.52 ( 0.19 , 0.0370 ) \n",
      "2 , 6309 (2) , 0.94s , 0.110 , 0.49 ( 0.19 , 0.0032 ) \n",
      "2 , 7943 (2) , 1.23s , 0.096 , 0.50 ( 0.19 , 0.0429 ) \n",
      "2 , 10000 (2) , 1.18s , 0.083 , 0.50 ( 0.19 , 0.1483 ) \n",
      " \n",
      "idLogit, {'Lambda1': 1.0, 'Lambda2': 1.0}\n",
      "1 , 1000 (1) , 0.47s , 0.330 , 0.38 ( 0.25 , 0.0426 ) \n",
      "1 , 1258 (2) , 1.00s , 0.287 , 0.39 ( 0.31 , 0.0691 ) \n",
      "1 , 1584 (2) , 0.87s , 0.251 , 0.40 ( 0.35 , 0.0358 ) \n",
      "1 , 1995 (2) , 1.26s , 0.221 , 0.45 ( 0.35 , 0.1257 ) \n",
      "1 , 2511 (2) , 1.05s , 0.192 , 0.45 ( 0.37 , 0.7516 ) \n",
      "1 , 3162 (2) , 1.12s , 0.168 , 0.48 ( 0.35 , 0.0478 ) \n",
      "1 , 3981 (2) , 1.34s , 0.147 , 0.51 ( 0.31 , 0.9684 ) \n",
      "1 , 5011 (2) , 1.42s , 0.128 , 0.53 ( 0.31 , 0.5388 ) \n",
      "1 , 6309 (2) , 1.08s , 0.110 , 0.49 ( 0.27 , 0.5129 ) \n",
      "1 , 7943 (2) , 1.25s , 0.096 , 0.50 ( 0.24 , 0.1510 ) \n",
      "1 , 10000 (2) , 1.08s , 0.083 , 0.50 ( 0.22 , 0.0684 ) \n",
      " \n",
      "2 , 1000 (1) , 0.59s , 0.330 , 0.38 ( 0.25 , 0.0424 ) \n",
      "2 , 1258 (2) , 0.89s , 0.287 , 0.39 ( 0.31 , 0.0695 ) \n",
      "2 , 1584 (2) , 1.09s , 0.251 , 0.40 ( 0.35 , 0.0355 ) \n",
      "2 , 1995 (2) , 1.08s , 0.221 , 0.45 ( 0.35 , 0.1258 ) \n",
      "2 , 2511 (2) , 1.01s , 0.192 , 0.45 ( 0.36 , 0.7730 ) \n",
      "2 , 3162 (2) , 1.15s , 0.168 , 0.48 ( 0.37 , 0.3115 ) \n",
      "2 , 3981 (2) , 1.06s , 0.147 , 0.51 ( 0.31 , 0.9825 ) \n",
      "2 , 5011 (1) , 1.85s , 0.128 , 0.53 ( 0.31 , 0.5368 ) \n",
      "2 , 6309 (2) , 1.50s , 0.110 , 0.49 ( 0.27 , 0.6575 ) \n",
      "2 , 7943 (2) , 1.19s , 0.096 , 0.50 ( 0.24 , 0.1509 ) \n",
      "2 , 10000 (1) , 0.83s , 0.083 , 0.50 ( 0.22 , 0.0499 ) \n",
      " \n",
      "idLogit, {'Lambda1': 0.1, 'Lambda2': 1.0}\n",
      "1 , 1000 (1) , 0.53s , 0.330 , 0.39 ( 0.45 , 0.3667 ) \n",
      "1 , 1258 (2) , 1.00s , 0.287 , 0.39 ( 0.47 , 0.9628 ) \n",
      "1 , 1584 (2) , 0.98s , 0.251 , 0.41 ( 0.43 , 0.1388 ) \n",
      "1 , 1995 (1) , 0.70s , 0.221 , 0.46 ( 0.43 , 0.3807 ) \n",
      "1 , 2511 (2) , 1.34s , 0.192 , 0.45 ( 0.44 , 0.5349 ) \n",
      "1 , 3162 (2) , 2.82s , 0.168 , 0.48 ( 0.40 , 0.4078 ) \n",
      "1 , 3981 (1) , 1.02s , 0.147 , 0.52 ( 0.33 , 0.7484 ) \n",
      "1 , 5011 (2) , 1.72s , 0.128 , 0.53 ( 0.33 , 0.4903 ) \n",
      "1 , 6309 (2) , 1.60s , 0.110 , 0.49 ( 0.28 , 0.7297 ) \n",
      "1 , 7943 (2) , 2.51s , 0.096 , 0.50 ( 0.24 , 0.1468 ) \n",
      "1 , 10000 (2) , 1.31s , 0.083 , 0.50 ( 0.23 , 0.0392 ) \n",
      " \n",
      "2 , 1000 (1) , 0.69s , 0.330 , 0.39 ( 0.45 , 0.3673 ) \n",
      "2 , 1258 (2) , 1.35s , 0.287 , 0.39 ( 0.47 , 0.9607 ) \n",
      "2 , 1584 (2) , 1.05s , 0.251 , 0.41 ( 0.45 , 0.5586 ) \n",
      "2 , 1995 (1) , 1.06s , 0.221 , 0.46 ( 0.43 , 0.3806 ) \n",
      "2 , 2511 (2) , 1.30s , 0.192 , 0.45 ( 0.44 , 0.5348 ) \n",
      "2 , 3162 (2) , 1.85s , 0.168 , 0.48 ( 0.40 , 0.4078 ) \n",
      "2 , 3981 (2) , 0.97s , 0.147 , 0.52 ( 0.33 , 0.7873 ) \n",
      "2 , 5011 (1) , 0.95s , 0.128 , 0.53 ( 0.33 , 0.4893 ) \n",
      "2 , 6309 (2) , 2.17s , 0.110 , 0.49 ( 0.28 , 0.7309 ) \n",
      "2 , 7943 (2) , 1.81s , 0.096 , 0.50 ( 0.24 , 0.1497 ) \n",
      "2 , 10000 (2) , 1.40s , 0.083 , 0.50 ( 0.23 , 0.0383 ) \n",
      " \n"
     ]
    }
   ],
   "source": [
    "\n",
    "mdls = [\n",
    "    { 'code' : 'mnl' } , \n",
    "    # { 'code' : 'lcl' , 'data' : { 'C' : 3 } } , \n",
    "    # { 'code' : 'ccl' , 'data' : { 'C' : 3 } } , \n",
    "    # { 'code' : 'saa' } , \n",
    "    { 'code' : 'glq' } , \n",
    "    { 'code' : 'idl' , 'data' : { 'Lambda1' : 10.0 , 'Lambda2' : 1.0 } } , \n",
    "    { 'code' : 'idl' , 'data' : { 'Lambda1' :  1.0 , 'Lambda2' : 1.0 } } , \n",
    "    { 'code' : 'idl' , 'data' : { 'Lambda1' :  0.1 , 'Lambda2' : 1.0 } } , \n",
    "]\n",
    "fitr = ModelFitExp( I=100 , dgp=mnl_dgp , timeout=60 , models=mdls , trials=2 , verbose=False )\n",
    "# fitr.set_obsnums( 3 , 6 , 25 )\n",
    "# fitr.set_obsnums( 3 , 5 , 20 )\n",
    "fitr.set_obsnums( 3 , 4 , 10 )\n",
    "\n",
    "fitr.run()\n",
    "\n",
    "fitr.print_results()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random (Gaussian) Coefficient Logit data\n",
    "\n",
    "Let's look at the Gaussian coefficient case now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.33182725 0.38897958]\n",
      "Logit\n",
      "1 , 1000 (1) , 0.04s , 0.999 , 0.39 \n",
      "1 , 1359 (1) , 0.03s , 1.000 , 0.34 \n",
      "1 , 1847 (1) , 0.03s , 1.000 , 0.34 \n",
      "1 , 2511 (1) , 0.03s , 1.000 , 0.35 \n",
      "1 , 3414 (1) , 0.03s , 1.000 , 0.35 \n",
      "1 , 4641 (1) , 0.03s , 1.000 , 0.37 \n",
      "1 , 6309 (1) , 0.03s , 1.000 , 0.36 \n",
      "1 , 8576 (1) , 0.03s , 1.000 , 0.38 \n",
      "1 , 11659 (1) , 0.03s , 1.000 , 0.37 \n",
      "1 , 15848 (1) , 0.04s , 1.000 , 0.37 \n",
      "1 , 21544 (1) , 0.03s , 1.000 , 0.36 \n",
      "1 , 29286 (1) , 0.03s , 1.000 , 0.36 \n",
      "1 , 39810 (1) , 0.03s , 1.000 , 0.35 \n",
      "1 , 54116 (1) , 0.03s , 1.000 , 0.35 \n",
      "1 , 73564 (1) , 0.04s , 1.000 , 0.35 \n",
      "1 , 100000 (1) , 0.04s , 1.000 , 0.35 \n",
      " \n",
      "Gaussian RC Logit (SAA)\n",
      "1 , 1000 (1) , 0.45s , 0.999 , 0.43 ( 0.55 ) \n",
      "1 , 1359 (1) , 0.70s , 1.000 , 0.34 ( 0.44 ) \n",
      "1 , 1847 (1) , 0.85s , 1.000 , 0.35 ( 0.37 ) \n",
      "1 , 2511 (1) , 1.11s , 1.000 , 0.35 ( 0.33 ) \n",
      "1 , 3414 (1) , 1.55s , 1.000 , 0.35 ( 0.35 ) \n",
      "1 , 4641 (1) , 2.71s , 1.000 , 0.37 ( 0.36 ) \n",
      "1 , 6309 (1) , 3.74s , 1.000 , 0.37 ( 0.40 ) \n",
      "1 , 8576 (1) , 25.82s , 1.000 , 0.39 ( 0.39 ) \n",
      "1 , 11659 (1) , 13.58s , 1.000 , 0.38 ( 0.39 ) \n",
      "1 , 15848 (1) , 10.19s , 1.000 , 0.40 ( 0.40 ) \n",
      "1 , 21544 (1) , 11.20s , 1.000 , 0.34 ( 0.42 ) \n",
      "1 , 29286 (1) , 36.85s , 1.000 , 0.38 ( 0.38 ) \n",
      "1 , 39810 (-) , solver timeout\n",
      "1 , 54116 (1) , 28.84s , 1.000 , 0.38 ( 0.39 ) \n",
      "1 , 73564 (-) , solver timeout\n",
      "1 , 100000 (-) , solver timeout\n",
      " \n",
      "Gaussian RC Logit (GLQ)\n",
      "1 , 1000 (1) , 0.10s , 0.999 , 0.42 ( 0.54 ) \n",
      "1 , 1359 (1) , 0.11s , 1.000 , 0.36 ( 0.44 ) \n",
      "1 , 1847 (1) , 0.12s , 1.000 , 0.35 ( 0.38 ) \n",
      "1 , 2511 (1) , 0.12s , 1.000 , 0.36 ( 0.32 ) \n",
      "1 , 3414 (1) , 0.13s , 1.000 , 0.36 ( 0.35 ) \n",
      "1 , 4641 (1) , 0.19s , 1.000 , 0.38 ( 0.37 ) \n",
      "1 , 6309 (1) , 0.28s , 1.000 , 0.37 ( 0.39 ) \n",
      "1 , 8576 (1) , 0.37s , 1.000 , 0.39 ( 0.39 ) \n",
      "1 , 11659 (1) , 0.66s , 1.000 , 0.38 ( 0.39 ) \n",
      "1 , 15848 (1) , 1.11s , 1.000 , 0.38 ( 0.41 ) \n",
      "1 , 21544 (1) , 3.14s , 1.000 , 0.36 ( 0.39 ) \n",
      "1 , 29286 (1) , 0.87s , 1.000 , 0.36 ( 0.39 ) \n",
      "1 , 39810 (1) , 1.50s , 1.000 , 0.36 ( 0.39 ) \n",
      "1 , 54116 (1) , 28.92s , 1.000 , 0.36 ( 0.39 ) \n",
      "1 , 73564 (1) , 11.48s , 1.000 , 0.36 ( 0.38 ) \n",
      "1 , 100000 (1) , 4.72s , 1.000 , 0.36 ( 0.38 ) \n",
      " \n",
      "idLogit, {'Lambda1': 10.0, 'Lambda2': 0.0}\n",
      "1 , 1000 (1) , 0.26s , 0.376 , 0.39 ( 0.00 , 0.1077 ) \n",
      "1 , 1359 (1) , 0.24s , 0.313 , 0.34 ( 0.00 , 0.0017 ) \n",
      "1 , 1847 (2) , 0.69s , 0.263 , 0.34 ( 0.01 , 0.0000 ) \n",
      "1 , 2511 (2) , 0.91s , 0.222 , 0.35 ( 0.11 , 0.0000 ) \n",
      "1 , 3414 (2) , 1.05s , 0.186 , 0.35 ( 0.23 , 0.0175 ) \n",
      "1 , 4641 (2) , 1.34s , 0.157 , 0.37 ( 0.31 , 0.4491 ) \n",
      "1 , 6309 (2) , 1.48s , 0.131 , 0.37 ( 0.39 , 0.6080 ) \n",
      "1 , 8576 (2) , 1.14s , 0.110 , 0.39 ( 0.41 , 0.9873 ) \n",
      "1 , 11659 (2) , 1.94s , 0.092 , 0.38 ( 0.41 , 0.6444 ) \n",
      "1 , 15848 (2) , 1.23s , 0.077 , 0.38 ( 0.42 , 0.8949 ) \n",
      "1 , 21544 (2) , 2.05s , 0.064 , 0.36 ( 0.41 , 0.8546 ) \n",
      "1 , 29286 (2) , 3.08s , 0.054 , 0.37 ( 0.41 , 0.9812 ) \n",
      "1 , 39810 (2) , 2.67s , 0.045 , 0.36 ( 0.40 , 0.9085 ) \n",
      "1 , 54116 (2) , 4.28s , 0.038 , 0.36 ( 0.40 , 0.9531 ) \n",
      "1 , 73564 (2) , 3.75s , 0.032 , 0.36 ( 0.39 , 0.9108 ) \n",
      "1 , 100000 (2) , 5.45s , 0.026 , 0.36 ( 0.39 , 0.9478 ) \n",
      " \n",
      "idLogit, {'Lambda1': 1.0, 'Lambda2': 0.0}\n",
      "1 , 1000 (1) , 0.54s , 0.375 , 0.42 ( 0.57 , 0.0367 ) \n",
      "1 , 1359 (2) , 0.85s , 0.313 , 0.37 ( 0.59 , 0.0048 ) \n",
      "1 , 1847 (2) , 1.05s , 0.263 , 0.36 ( 0.54 , 0.7371 ) \n",
      "1 , 2511 (2) , 1.23s , 0.222 , 0.37 ( 0.48 , 0.0453 ) \n",
      "1 , 3414 (1) , 1.26s , 0.186 , 0.36 ( 0.48 , 0.5234 ) \n",
      "1 , 4641 (2) , 1.11s , 0.156 , 0.38 ( 0.47 , 0.5264 ) \n",
      "1 , 6309 (2) , 1.45s , 0.131 , 0.38 ( 0.47 , 0.7025 ) \n",
      "1 , 8576 (2) , 2.48s , 0.110 , 0.39 ( 0.45 , 0.7228 ) \n",
      "1 , 11659 (2) , 1.46s , 0.092 , 0.38 ( 0.43 , 0.5958 ) \n",
      "1 , 15848 (1) , 1.48s , 0.077 , 0.38 ( 0.44 , 0.7085 ) \n",
      "1 , 21544 (2) , 2.36s , 0.064 , 0.36 ( 0.42 , 0.7896 ) \n",
      "1 , 29286 (1) , 1.74s , 0.054 , 0.37 ( 0.41 , 0.9672 ) \n",
      "1 , 39810 (2) , 1.90s , 0.045 , 0.36 ( 0.41 , 0.9622 ) \n",
      "1 , 54116 (2) , 1.74s , 0.038 , 0.36 ( 0.40 , 0.9557 ) \n",
      "1 , 73564 (2) , 4.93s , 0.032 , 0.36 ( 0.39 , 0.9149 ) \n",
      "1 , 100000 (2) , 3.08s , 0.026 , 0.36 ( 0.39 , 0.9491 ) \n",
      " \n",
      "idLogit, {'Lambda1': 0.1, 'Lambda2': 0.0}\n",
      "1 , 1000 (1) , 0.82s , 0.376 , 0.50 ( 1.09 , 0.0001 ) \n",
      "1 , 1359 (1) , 0.57s , 0.313 , 0.40 ( 0.89 , 0.0000 ) \n",
      "1 , 1847 (2) , 2.01s , 0.263 , 0.37 ( 0.65 , 0.9193 ) \n",
      "1 , 2511 (2) , 1.37s , 0.222 , 0.38 ( 0.57 , 0.1675 ) \n",
      "1 , 3414 (1) , 1.51s , 0.186 , 0.36 ( 0.51 , 0.3156 ) \n",
      "1 , 4641 (1) , 0.72s , 0.156 , 0.38 ( 0.49 , 0.4318 ) \n",
      "1 , 6309 (1) , 1.59s , 0.131 , 0.38 ( 0.48 , 0.6662 ) \n",
      "1 , 8576 (1) , 1.04s , 0.110 , 0.39 ( 0.45 , 0.6813 ) \n",
      "1 , 11659 (1) , 1.01s , 0.092 , 0.38 ( 0.44 , 0.5868 ) \n",
      "1 , 15848 (1) , 2.62s , 0.077 , 0.38 ( 0.44 , 0.6985 ) \n",
      "1 , 21544 (1) , 1.04s , 0.064 , 0.36 ( 0.42 , 0.7826 ) \n",
      "1 , 29286 (2) , 1.13s , 0.054 , 0.37 ( 0.41 , 0.9658 ) \n",
      "1 , 39810 (2) , 1.60s , 0.045 , 0.36 ( 0.41 , 0.9615 ) \n",
      "1 , 54116 (1) , 2.30s , 0.038 , 0.36 ( 0.40 , 0.9569 ) \n",
      "1 , 73564 (2) , 2.12s , 0.032 , 0.36 ( 0.39 , 0.9151 ) \n",
      "1 , 100000 (1) , 2.08s , 0.026 , 0.36 ( 0.39 , 0.9495 ) \n",
      " \n"
     ]
    }
   ],
   "source": [
    "\n",
    "mdls = [\n",
    "    { 'code' : 'mnl' } , \n",
    "    { 'code' : 'saa' } , \n",
    "    { 'code' : 'glq' } , \n",
    "    { 'code' : 'idl' , 'data' : { 'Lambda1' : 10.0 , 'Lambda2' : 0.0 } } , \n",
    "    { 'code' : 'idl' , 'data' : { 'Lambda1' :  1.0 , 'Lambda2' : 0.0 } } , \n",
    "    { 'code' : 'idl' , 'data' : { 'Lambda1' :  0.1 , 'Lambda2' : 0.0 } } , \n",
    "]\n",
    "fitr = ModelFitExp( I=100 , dgp=grc_dgp , timeout=60 , models=mdls , trials=1 , verbose=False )\n",
    "# fitr.set_obsnums( 3 , 6 , 25 )\n",
    "fitr.set_obsnums( 3 , 5 , 15 )\n",
    "\n",
    "fitr.run()\n",
    "fitr.print_results()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
