{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Several Discrete Choice Models\n",
    "\n",
    "- - -\n",
    "\n",
    "W. Ross Morrow ([wrossmorrow@stanford.edu](mailto:wrossmorrow@stanford.edu), [wrossmorrow.com](https://wrossmorrow.com))\n",
    "\n",
    "Research Analytics Consultant, Stanford GSB\n",
    "\n",
    "February 5th, 2019\n",
    "\n",
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "\n",
    "- - - - \n",
    "\n",
    "In this notebook we'll compare several Discrete Choice Models in a simple situation: single-feature binary choice data. Specifically, the data will be composed of a number of observations, $N$, a number of individuals, $I$, an individual index per observations $i_n$, and a \"yes/no\" dummy $y_n \\in \\{\\pm 1\\}$ standing in for random (but structured) choices. We can draw those choices with a variety of data generating processes, as discussed below. But the basic setup is that we see some number of repeated trials for different individuals in which they answer \"yes\" or \"no\" to some question, and our broad goal is to model the frequency with which respondents say \"yes\" or \"no\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, lets do our basic imports and function definitions. We'll discuss the models for data $(N,I,\\mathbf{i},\\mathbf{y})$ below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import time\n",
    "from multiprocessing import Pool , Queue , Process\n",
    "\n",
    "import string\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import rand , randn , randint , choice\n",
    "\n",
    "import cvxpy as cp\n",
    "import ecos\n",
    "\n",
    "from scipy.sparse import csc_matrix , csr_matrix , coo_matrix , issparse\n",
    "from scipy.optimize import minimize , LinearConstraint , BFGS\n",
    "from scipy.stats import norm as gaussian\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Colormap\n",
    "%matplotlib notebook\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are some wrappers around random number generation that are useful here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def randi(A,N) : return randint(0,high=A,size=N) \n",
    "\n",
    "def rands(N) : return np.sign( 2.0*rand(N) - 1.0 )\n",
    "\n",
    "def randc(C,N,p) : return choice( C , size=N , p=p )\n",
    "\n",
    "def randp(A,N) : \n",
    "    L , R = randi(A,N) , randi(A-1,N)\n",
    "    R[ np.where( R >= L )[0] ] += 1\n",
    "    return L , R\n",
    "\n",
    "def random_string( l ) : \n",
    "    return ''.join( random.choice(string.ascii_letters) for m in range(l) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a nice simple way to plot sparsity patterns for not-too-large sparse matrices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def spy( A , b=None ) : \n",
    "    plt.figure( figsize=(10,3) )\n",
    "    plt.xticks([]) ; plt.yticks([])\n",
    "    if issparse(A) : \n",
    "        A = A.toarray()\n",
    "    if b is None : \n",
    "        plt.imshow( A != 0 , cmap='Greys' , interpolation='nearest', aspect='auto' )\n",
    "    else : \n",
    "        X = np.zeros( ( A.shape[0] , A.shape[1]+4 ) )\n",
    "        X[:,:A.shape[1]] = A\n",
    "        X[:,A.shape[1]+3] = b\n",
    "        plt.imshow( X != 0 , cmap='Greys' , interpolation='nearest', aspect='auto' )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "- - -\n",
    "\n",
    "Below we review Logit, Latent Class Logit, Random Coefficient Logit, and `idLogit` models and provide code for this specific (and rather simple) situation. \n",
    "\n",
    "First, however, we define a general class `FittableModel` each model-specific class will be derived from. This class has a timeout-enabled fitting method as well as derivative checking routines for gradients and Hessian-vector products. The timeout-enabled fitting method is optional, but is helpful given that the models we try to fit can have dramatically different fit times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FittableModel( object ) : \n",
    "    \n",
    "    def __init__( self ) : \n",
    "        pass\n",
    "    \n",
    "    def fit( self , p0=None , maxtime=None ) : \n",
    "        \n",
    "        if p0 is None : \n",
    "            try : \n",
    "                p0 = self.draw_initial_condition(  )\n",
    "            except AttributeError as e : \n",
    "                p0 = randn( self.Nvars )\n",
    "        else : \n",
    "            p0 = p0.flatten()\n",
    "            \n",
    "        start = time.time()\n",
    "        \n",
    "        if ( maxtime is None ) or ( maxtime <= 0.0 ) : \n",
    "            \n",
    "            self.soln = self.wrapped_solve( p0=p0 )\n",
    "            self.soln['timeout'] = False\n",
    "            \n",
    "        else : \n",
    "            \n",
    "            q = Queue()\n",
    "            p = Process( target=self.wrapped_solve , kwargs={ 'p0' : p0 , 'queue' : q } )\n",
    "            \n",
    "            p.start()\n",
    "            while time.time() - start < maxtime :\n",
    "                p.join( timeout=1 )\n",
    "                if not p.is_alive() : break\n",
    "            if p.is_alive():\n",
    "                p.terminate()\n",
    "                self.soln = { 'timeout' : True }\n",
    "            else : \n",
    "                self.soln = q.get()\n",
    "                self.soln['timeout'] = False\n",
    "                \n",
    "        self.soln['solvertime'] = time.time() - start\n",
    "        self.soln['x0'] = p0\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def wrapped_solve( self , p0=None , queue=None ) :\n",
    "        try : \n",
    "            self.soln = self.solve( p0=p0 )\n",
    "            self.soln['error'] = False\n",
    "        except Exception as e : \n",
    "            print( \"caught solve exception: \" , e )\n",
    "            self.soln = { 'error' : True , 'message' : e }\n",
    "        if queue is not None : queue.put( self.soln )\n",
    "        return self.soln\n",
    "    \n",
    "    def grad_check( self , p=None , verbose=True ) : \n",
    "\n",
    "        \"\"\"\n",
    "        gradient check of an object that has certain attributes (Nvars, obj, and grad)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if p is None : p = rand( self.Nvars )\n",
    "\n",
    "        f0 , g0 = self.obj( p ) , self.grad( p ) # original objective and gradient\n",
    "\n",
    "        Hs = 10.0**np.arange( -10 , 0 , 1 )[::-1] # perturbation sizes\n",
    "        pP = p.copy() # copy for perturbed betas\n",
    "\n",
    "        df = np.zeros( self.Nvars ) # space for dinite differences\n",
    "        ds = np.zeros( Hs.size ) # differences between gradient and finite differences\n",
    "        \n",
    "        # iterations\n",
    "        for h in range( Hs.size ) : \n",
    "            H = Hs[h] # actual perturbation\n",
    "            for k in range( self.Nvars ) : \n",
    "                pP[k] += H # perturb on coordinate k\n",
    "                fk = self.obj( pP ) # evalute objective at perturbed argument\n",
    "                df[k] = ( fk - f0 ) / H # compute finite difference\n",
    "                pP[k] -= H # perturb on coordinate k\n",
    "            ds[h] = np.max( np.abs( g0 - df ) ) # compute difference\n",
    "\n",
    "        if( verbose ) : \n",
    "            for h in range( Hs.size ) : \n",
    "                print( \"%0.16f , %0.16f\" % ( Hs[h] , ds[h] ) )\n",
    "\n",
    "        return Hs , ds\n",
    "\n",
    "    def hessp_check( self , p=None , v=None , verbose=True ) :  \n",
    "\n",
    "        \"\"\"\n",
    "        hessian product check of an object that has certain attributes (Nvars, grad, and hessp)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if p is None : p = rand( self.Nvars )\n",
    "        if v is None : v = rand( self.Nvars )\n",
    "\n",
    "        g0 , h0 = self.grad( p ) , self.hessp( p , v ) # original gradient\n",
    "\n",
    "        Hs = 10.0**np.arange( -10 , 1 , 1 )[::-1] # perturbation sizes\n",
    "        pP = p.copy() # copy for perturbed betas\n",
    "\n",
    "        ds = np.zeros( Hs.size ) # differences between gradient and finite differences\n",
    "\n",
    "        # iterations\n",
    "        for h in range( Hs.size ) : \n",
    "            H = Hs[h] # actual perturbation\n",
    "            pP = p + H * v # perturb by H in direction v\n",
    "            gP = self.grad( pP ) # evalute objective gradient at perturbed argument\n",
    "            df = ( gP - g0 ) / H\n",
    "            ds[h] = np.max( np.abs( df - h0 ) ) # compute difference\n",
    "\n",
    "        if( verbose ) : \n",
    "            for h in range( Hs.size ) : \n",
    "                print( \"%0.16f , %0.16f\" % ( Hs[h] , ds[h] ) )\n",
    "\n",
    "        return Hs , ds\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have a specific way of testing derivative accuracy: Compare finite differences (for gradients or for hessian-vector products) for a _decreasing sequence_ of perturbation sizes. Regardless of the function evaluation accuracy, we should see a \"V\" shape in the associated relative error magnitude between computed derivatives and finite differences. The finite difference approximation should be innaccurate for \"large\" perturbations but get more accurate as the perturbation decreases, that is until floating point errors start to accumulate and reduce the approximation accuracy. \n",
    "\n",
    "This is obviously more computationally intensive that comparing derivatives at a single perturbation size and for large enough problems can be quite burdensome. However single-point approximations give no indication of what a \"reasonable\" error should be, and thus are somewhat useless when we don't know how much error exists in our function and derivative evaluations to begin with. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logit\n",
    "\n",
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Logit is the simplest model, with \"yes\" probability\n",
    "$$\n",
    "    \\frac{e^\\beta}{1+e^\\beta}\n",
    "$$\n",
    "and MLE problem\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\min &\\quad \\frac{1}{N} \\sum_{n=1}^N \\log( 1 + e^{-y_n\\beta} ) \\\\\n",
    "    \\text{w.r.t.} &\\quad \\beta \\in \\mathbb{R}\n",
    "\\end{aligned}\n",
    "$$\n",
    "The derivative of the objective is easily derived as\n",
    "$$\n",
    "    D^\\beta = \\frac{1}{N} \\sum_{n=1}^N (-y_n)\\left( \\frac{ e^{-y_n\\beta} }{ 1 + e^{-y_n\\beta} } \\right)\n",
    "$$\n",
    "Moreover the Hessian is even easily derived as\n",
    "$$\n",
    "    \\frac{1}{N} \\sum_{n=1}^N \\left( \\frac{ e^{-y_n\\beta} }{ 1 + e^{-y_n\\beta} } \\right)\\left( 1 - \\frac{ e^{-y_n\\beta} }{ 1 + e^{-y_n\\beta} } \\right)\n",
    "$$\n",
    "The following class, derived from `FittableModel`, implements the Logit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Logit( FittableModel ) : \n",
    "    \n",
    "    type = \"Logit\"\n",
    "    \n",
    "    def __init__( self , N , y ) : \n",
    "        self.Nvars , self.N , self.y , self.ny = 1 , N , y , - y\n",
    "\n",
    "    def obj( self , p ) :\n",
    "        return np.sum( np.log1p( np.exp( self.ny * p ) ) ) / self.N\n",
    "\n",
    "    def grad( self , p ) :  \n",
    "        eU = np.exp( self.ny * p ) ; PL = np.divide( eU , 1.0 + eU )\n",
    "        return np.array( [ np.sum( PL * self.ny ) / self.N ] )\n",
    "    \n",
    "    def hess( self , p ) :  \n",
    "        eU = np.exp( self.ny * p ) ; PL = eU / ( 1.0 + eU )\n",
    "        return np.array( [ np.sum( PL * ( 1.0 - PL ) ) / self.N ] )\n",
    "    \n",
    "    def solve( self , p0=None ) : \n",
    "        self.soln = minimize( self.obj , p0 , jac=self.grad , \\\n",
    "                              hess=self.hess , method='trust-constr' )\n",
    "        return self.soln\n",
    "    \n",
    "    def printx( self ) : \n",
    "        if self.soln is None : return \"(no solution)\"\n",
    "        return \"%0.2f\" % self.soln['x'][0]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Class Logit\n",
    "\n",
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we presume that each individual is \"drawn\" from one of $C$ classes each with its own coefficient. Our job is to estimate the class coefficients and the mass function for the classes. The simplest version of this problem is: \n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\\\\n",
    "    \\min &\\quad - \\frac{1}{N} \\sum_{i=1}^I \\log \\left( \\sum_{c=1}^C \\rho_c e^{L_i(\\beta_c)} \\right)    \n",
    "        \\quad\\quad\\text{where}\\quad \n",
    "        L_i(\\theta) = - \\sum_{ n \\in \\mathcal{O}_i } \\log \\Big( 1 + e^{-y_n\\theta} \\Big) \n",
    "        \\\\\n",
    "    \\text{w.r.t.} &\\quad 0 \\leq \\rho_c \\leq 1 \\;\\; , \\;\\; \\beta_c \\in \\mathbb{R} \n",
    "                        \\;\\; \\text{for all} \\;\\; c = 1,\\dotsc,C \\\\\n",
    "    \\text{s.to} &\\quad \\sum_{c=1}^C \\rho_c = 1 \\\\\n",
    "   \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "The derivatives are\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    D_c^\\rho \n",
    "        &= - \\frac{1}{N} \\sum_{i=1}^I \\frac{ e^{L_i(\\beta_c)} }{ \\sum_{d=1}^C \\rho_d e^{L_i(\\beta_d)} }\n",
    "    \\\\\n",
    "    D_c^\\beta \n",
    "        &= - \\frac{1}{N} \\sum_{i=1}^I \\frac{ \\rho_c e^{L_i(\\beta_c)} L_i^\\prime(\\beta_c) }{ \\sum_{d=1}^C \\rho_d e^{L_i(\\beta_d)} }\n",
    "    \\quad\\quad\\text{where}\\quad \n",
    "    L_i^\\prime(\\theta) \n",
    "        = - \\sum_{ n \\in \\mathcal{O}_i } (-y_n)\\frac{ e^{-y_n\\theta} }{ 1 + e^{-y_n\\theta} }\n",
    "        = \\sum_{ n \\in \\mathcal{O}_i } y_n\\frac{ e^{-y_n\\theta} }{ 1 + e^{-y_n\\theta} }\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stabilization\n",
    "\n",
    "We have to consider an important stabilization technique. If $\\max_c L_i( \\beta_c ) \\ll 0$, then $e^{ L_i( \\beta_c ) } \\approx 0$ and \n",
    "$$\n",
    "    \\log \\left( \\sum_{c=1}^C \\rho_c e^{ L_i( \\beta_c ) } \\right )\n",
    "$$ \n",
    "will not be computable. Specifically, if \n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\\\\n",
    "    \\max_c L_i( \\beta_c ) \\leq -745.15\n",
    "    \\quad\\quad\\text{then}\\quad\\quad\n",
    "    \\texttt{float}( e^{ L_i( \\beta_c ) } ) = 0\n",
    "    \\\\\n",
    "    \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "where $\\texttt{float}$ is the floating point representation of the exponential (in double precision). This has to be handled carefully if we want to allow for sufficient exploration of $\\beta_1,\\dotsc,\\beta_C$ for arbitrary data. \n",
    "\n",
    "In particular, note that \n",
    "$$\n",
    "    L_i(\\theta) = - |\\{n:y_n=+1\\}| \\log\\left( 1 + e^{-\\theta} \\right) - |\\{n:y_n=-1\\}| \\log\\left( 1 + e^{\\theta} \\right)\n",
    "$$\n",
    "This is, in fact, a formula we could use to dramatically improve the scalability of the fitting methods we explore, but at the cost of _complete_ loss of generality. What is worth noting here is that as the number of observations increase, the size of both sets goes to infinity; moreover the $\\log$ terms are both positive, having arguments larger than one. Thus, for any finite $\\theta$, there is _some_ number of observations such that $L_i$ is too negative to effectively compute with. It is probably even possible to analyze this situation further, but knowing exactly _when_ we get into trouble isn't helpful. \n",
    "\n",
    "A simple trick sidesteps this potential problem: Let \n",
    "$$\n",
    "L_i^*(\\boldsymbol{\\beta}) = \\max_c L_i( \\beta_c )\n",
    "$$\n",
    "and then\n",
    "$$\n",
    "   -\\frac{1}{N} \\sum_{i=1}^I \\log \\left( \\sum_{c=1}^C \\rho_c e^{ L_i( \\beta_c ) } \\right )\n",
    "        = -\\frac{1}{N} \\sum_{i=1}^I L_i^*(\\boldsymbol{\\beta}) -\\frac{1}{N} \\sum_{i=1}^I  \\log \\left( \\sum_{c=1}^C \\rho_c e^{ L_i( \\beta_c ) - L_i^*(\\boldsymbol{\\beta}) } \\right )\n",
    "$$ \n",
    "Note also that the derivatives can be evaluated with $L_i( \\beta_c ) - L_i^*(\\boldsymbol{\\beta})$: \n",
    "$$\n",
    "\\begin{aligned}\n",
    "    D_c^\\rho \n",
    "        &= - \\frac{1}{N} \\sum_{i=1}^I \\frac{ e^{L_i(\\beta_c)} }{ \\sum_{d=1}^C \\rho_d e^{L_i(\\beta_d)} }\n",
    "        = - \\frac{1}{N} \\sum_{i=1}^I \\frac{ e^{L_i^*(\\boldsymbol{\\beta})} e^{L_i(\\beta_c)-L_i^*(\\boldsymbol{\\beta})} }{ e^{L_i^*(\\boldsymbol{\\beta})} \\sum_{d=1}^C \\rho_d e^{L_i(\\beta_d)-L_i^*(\\boldsymbol{\\beta})} }\n",
    "        = - \\frac{1}{N} \\sum_{i=1}^I \\frac{ e^{L_i(\\beta_c)-L_i^*(\\boldsymbol{\\beta})} }{\\sum_{d=1}^C \\rho_d e^{L_i(\\beta_d)-L_i^*(\\boldsymbol{\\beta})} }\n",
    "    \\\\\n",
    "    D_c^\\beta \n",
    "        &= - \\frac{1}{N} \\sum_{i=1}^I \\frac{ \\rho_c e^{L_i(\\beta_c)} L_i^\\prime(\\beta_c) }{ \\sum_{d=1}^C \\rho_d e^{L_i(\\beta_d)} }\n",
    "        = - \\frac{1}{N} \\sum_{i=1}^I \\frac{ e^{L_i^*(\\boldsymbol{\\beta})} \\rho_c e^{L_i(\\beta_c)-L_i^*(\\boldsymbol{\\beta})} L_i^\\prime(\\beta_c) }{ e^{L_i^*(\\boldsymbol{\\beta})} \\sum_{d=1}^C \\rho_d e^{L_i(\\beta_d)-L_i^*(\\boldsymbol{\\beta})} }\n",
    "        = - \\frac{1}{N} \\sum_{i=1}^I \\frac{ \\rho_c e^{L_i(\\beta_c)-L_i^*(\\boldsymbol{\\beta})} L_i^\\prime(\\beta_c) }{ \\sum_{d=1}^C \\rho_d e^{L_i(\\beta_d)-L_i^*(\\boldsymbol{\\beta})} }\n",
    "    \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LatentClassLogit( FittableModel ) : \n",
    "    \n",
    "    type = \"Latent Class Logit\"\n",
    "    \n",
    "    def __init__( self , N , I , i , y , C , ordered=True ) : \n",
    "        \n",
    "        if C <= 1 : \n",
    "            raise ArgumentError( \"Trivial number of classes: should be at least two.\" )\n",
    "        \n",
    "        self.N , self.I , self.i , self.y , self.C = N , I , i , -y , C\n",
    "        self.Nvars , self.Ncons = 2*C , 2*C if ordered else C+1\n",
    "        \n",
    "        # this matrix \"assigns\" observations to individuals, facilitating \n",
    "        # sums over observations within individuals\n",
    "        self.reducer = csr_matrix( (-np.ones(N),(i,np.arange(N))) , shape=(I,N) )\n",
    "        self.yeducer = csr_matrix( (y,(i,np.arange(N))) , shape=(I,N) )\n",
    "        \n",
    "        # constraints: \n",
    "        #  \n",
    "        #     p[0] , p[1] , ... , p[C] >= 0.0\n",
    "        #     p[0] + p[1] + ... + p[C] = 1.0\n",
    "        #     p[0] >= p[1] >= ... >= p[C] if ordered\n",
    "        # \n",
    "        \n",
    "        lo = np.zeros( self.Ncons )\n",
    "        up = np.inf * np.ones( self.Ncons )\n",
    "        lo[C] , up[C] = 1.0 , 1.0\n",
    "        Cmtrx = np.zeros( ( self.Ncons , 2*C ) )\n",
    "        for c in range(self.C) : \n",
    "            Cmtrx[c,c] = 1.0 # p[c] >= 0\n",
    "            Cmtrx[C,c] = 1.0 # sum_c p[c] = 1\n",
    "        if ordered : \n",
    "            for c in range(1,self.C) : \n",
    "                Cmtrx[C+c,c-1] = 1.0\n",
    "                Cmtrx[C+c,c] = - 1.0\n",
    "            \n",
    "        self.cons = LinearConstraint( Cmtrx , lo , up )\n",
    "        \n",
    "        # workspace\n",
    "        self.yp = np.zeros((self.N,self.C),dtype=np.float)\n",
    "        self.eU = np.zeros((self.N,self.C),dtype=np.float)\n",
    "        self.ll = np.zeros((self.N,self.C),dtype=np.float)\n",
    "        self.Li = np.zeros((self.I,self.C),dtype=np.float)\n",
    "        self.EL = np.zeros((self.I,self.C),dtype=np.float)\n",
    "        self.PL = np.zeros((self.N,self.C),dtype=np.float)\n",
    "        self.LP = np.zeros((self.I,self.C),dtype=np.float)\n",
    "        self.j  = np.zeros((self.I,),dtype=np.float)\n",
    "        self.LM = np.zeros((self.I,),dtype=np.float)\n",
    "        \n",
    "    def basics( self , p ) : \n",
    "        np.outer( self.y , p[self.C:] , out=self.yp )\n",
    "        np.exp( self.yp , out=self.eU )\n",
    "        np.log1p( self.eU , out=self.ll )\n",
    "        self.Li = self.reducer @ self.ll\n",
    "        np.max( self.Li , axis=1 , out=self.LM )\n",
    "        self.Li = self.Li - np.tile( self.LM.reshape((self.I,1)) , (1,self.C) )\n",
    "        np.exp( self.Li , out=self.EL )\n",
    "        self.j = self.EL @ p[:self.C]\n",
    "        \n",
    "    def obj( self , p ) :\n",
    "        self.basics( p )\n",
    "        np.log( self.j , out=self.j )\n",
    "        return - np.sum( self.LM + self.j ) / self.N\n",
    "\n",
    "    def grad( self , p ) :  \n",
    "        self.basics( p ) \n",
    "        np.divide( self.eU , 1.0 + self.eU , out=self.PL )\n",
    "        self.LP = self.yeducer @ self.PL\n",
    "        self.j  = 1.0 / self.j\n",
    "        \n",
    "        # gradient terms\n",
    "        g = np.zeros( self.Nvars )\n",
    "        g[:self.C] = - self.EL.T @ self.j / self.N\n",
    "        g[self.C:] = - p[:self.C] * ( ( self.EL * self.LP ).T @ self.j ) / self.N\n",
    "        \n",
    "        return g\n",
    "    \n",
    "    def draw_initial_condition( self ) : \n",
    "        p0 = randn( self.Nvars ) \n",
    "        p0[:self.C] = np.abs( p0[:self.C] )\n",
    "        p0[:self.C] = p0[:self.C] / p0[:self.C].sum()\n",
    "        return p0\n",
    "    \n",
    "    def solve( self , p0=None ) : \n",
    "        self.soln = minimize( self.obj , p0 , jac=self.grad , hess=BFGS() , \\\n",
    "                            method='trust-constr' , constraints=self.cons , \\\n",
    "                           options={ 'maxiter' : 100000 } )\n",
    "        return self.soln\n",
    "    \n",
    "    def printx( self ) : \n",
    "        if self.soln is None : return \"(no solution)\"\n",
    "        s = \" ) , ( \".join( [ \"%0.2f , %0.2f\" % ( self.soln['x'][c+self.C] , self.soln['x'][c] ) \\\n",
    "                                    for c in range(self.C) ] )\n",
    "        return \"( %s )\" % s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Classification\" Latent Class Logit\n",
    "\n",
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we \"classify\" individuals to classes, instead of presume each individual was drawn from a class at random. For this we introduce individual-specific class membership probabilities and solve\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\min &\\quad - \\frac{1}{N} \\sum_{i=1}^I \\log \\left( \\sum_{c=1}^C \\rho_{i,c} e^{L_i(\\beta_c)} \\right) \\\\\n",
    "    \\text{w.r.t.} &\\quad 0 \\leq \\rho_{i,c} \\leq 1 \\;\\; , \\;\\; \\beta_c \\in \\mathbb{R} \n",
    "                        \\;\\; \\text{for all} \\;\\; c = 1,\\dotsc,C \\\\\n",
    "    \\text{s.to} &\\quad \\sum_{c=1}^C \\rho_{i,c} = 1 \\text{ for all } i \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "The derivatives here are simpler, if more numerous: \n",
    "$$\n",
    "\\begin{aligned}\n",
    "    D_{i,c}^\\rho \n",
    "        &= - \\frac{1}{N} \\frac{ e^{L_i(\\beta_c)} }{ \\sum_{d=1}^C \\rho_{i,d} e^{L_i(\\beta_d)} }\n",
    "        \\\\\n",
    "    D_{c}^\\beta \n",
    "        &= - \\frac{1}{N} \\sum_{i=1}^I \\frac{ \\rho_{i,c} e^{L_i(\\beta_c)} }{ \\sum_{d=1}^C \\rho_{i,d} e^{L_i(\\beta_d)} }\n",
    "            L_i^\\prime(\\beta_c)\n",
    "        \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "This problem is also not always well-determined, as the large number of variables ($(I+1)C$) can exceed the number of observations ($N$). \n",
    "\n",
    "We stabilize in the same way as described above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ClassClassLogit( FittableModel ) : \n",
    "    \n",
    "    type = \"Class Class Logit\"\n",
    "    \n",
    "    def __init__( self , N , I , i , y , C , ordered=True ) : \n",
    "        \n",
    "        if C <= 1 : \n",
    "            raise ArgumentError( \"Trivial number of classes: should be at least two.\" )\n",
    "        \n",
    "        self.N , self.I , self.i , self.y , self.C = N , I , i , -y , C\n",
    "        self.Nvars , self.IC , self.Ncons = (I+1)*C , I*C , I*(C+1)\n",
    "        \n",
    "        if self.N - self.Nvars <= 0 : \n",
    "            raise ArgumentError( \"Not enough observations for the number of individuals and classes.\" )\n",
    "        \n",
    "        # this matrix \"assigns\" observations to individuals, facilitating \n",
    "        # sums over observations within individuals\n",
    "        self.reducer = csr_matrix( (-np.ones(N),(i,np.arange(N))) , shape=(I,N) )\n",
    "        self.yeducer = csr_matrix( (y,(i,np.arange(N))) , shape=(I,N) )\n",
    "        \n",
    "        # constraints: \n",
    "        #  \n",
    "        #     p[0] , p[1] , ... , p[IC-1] >= 0.0                          IC equations\n",
    "        #     p[i,0] + p[i,1] + ... + p[i,C-1] = 1.0 for all I            I equations\n",
    "        #     p[i,0] - p[i,1] , ... p[i,C] - p[i,C-1] >= 0 if ordered ?   I(C-1) equations\n",
    "        # \n",
    "        \n",
    "        lo , up = np.zeros( self.Ncons ) , np.inf * np.ones( self.Ncons )\n",
    "        lo[I*C:I*(C+1)] , up[I*C:I*(C+1)] = 1.0 , 1.0\n",
    "        \n",
    "        Cnnzs = 2*I*C\n",
    "        Crows , Ccols = np.zeros( Cnnzs , dtype=np.int ) , np.zeros( Cnnzs , dtype=np.int )\n",
    "        Cdata = np.ones( Cnnzs )\n",
    "        \n",
    "        Crows[:I*C] , Ccols[:I*C] = np.arange(I*C) , np.arange(I*C)\n",
    "        Crows[I*C:] , Ccols[I*C:] = I*C + np.repeat( np.arange(I) , C ) , np.arange(I*C)\n",
    "        \n",
    "        Cmtrx = csr_matrix( (Cdata,(Crows,Ccols)) , shape=( self.Ncons , self.Nvars ) )\n",
    "            \n",
    "        self.cons = LinearConstraint( Cmtrx , lo , up )\n",
    "        \n",
    "        # workspace\n",
    "        self.yp = np.zeros((self.N,self.C),dtype=np.float)\n",
    "        self.eU = np.zeros((self.N,self.C),dtype=np.float)\n",
    "        self.ll = np.zeros((self.N,self.C),dtype=np.float)\n",
    "        self.Li = np.zeros((self.I,self.C),dtype=np.float)\n",
    "        self.EL = np.zeros((self.I,self.C),dtype=np.float)\n",
    "        self.TL = np.zeros((self.I,self.C),dtype=np.float)\n",
    "        self.PL = np.zeros((self.N,self.C),dtype=np.float)\n",
    "        self.LP = np.zeros((self.I,self.C),dtype=np.float)\n",
    "        self.j  = np.zeros((self.I,),dtype=np.float)\n",
    "        self.LM = np.zeros((self.I,),dtype=np.float)\n",
    "\n",
    "    def basics( self , p ) : \n",
    "        np.outer( self.y , p[self.IC:] , out=self.yp )\n",
    "        np.exp( self.yp , self.eU )\n",
    "        np.log1p( self.eU , out=self.ll )\n",
    "        self.Li = self.reducer @ self.ll\n",
    "        np.max( self.Li , axis=1 , out=self.LM )\n",
    "        self.Li = self.Li - np.tile( self.LM.reshape((self.I,1)) , (1,self.C) )\n",
    "        np.exp( self.Li , out=self.EL )\n",
    "        np.multiply( self.EL , p[:self.IC].reshape((self.I,self.C)) , out=self.TL )\n",
    "        self.j = np.sum( self.TL , axis=1 )\n",
    "        \n",
    "    def obj( self , p ) :\n",
    "        self.basics( p )\n",
    "        np.log( self.j , out=self.j )\n",
    "        return - np.sum( self.LM + self.j ) / self.N\n",
    "\n",
    "    def grad( self , p ) :  \n",
    "        \n",
    "        self.basics( p )\n",
    "        \n",
    "        np.divide( self.eU , 1.0 + self.eU , out=self.PL )\n",
    "        self.LP = self.yeducer @ self.PL\n",
    "        self.j = 1.0 / self.j\n",
    "        \n",
    "        # gradient terms\n",
    "        g = np.zeros( self.Nvars )\n",
    "        g[:self.IC] = - self.EL.flatten() * np.repeat( self.j , self.C ) / self.N\n",
    "        g[self.IC:] = - ( ( self.TL * self.LP ).T @ self.j ) / self.N\n",
    "        \n",
    "        return g\n",
    "    \n",
    "    def draw_initial_condition( self ) : \n",
    "        p0 = randn(self.Nvars)\n",
    "        p0[:self.IC] = np.abs( p0[:self.IC] )\n",
    "        for i in range( self.I ) : \n",
    "            p0[i*self.C:i*self.C+1] = p0[i*self.C:i*self.C+1] / p0[i*self.C:i*self.C+1].sum()\n",
    "        return p0\n",
    "        \n",
    "    def solve( self , p0=None ) : \n",
    "        self.soln = minimize( self.obj , p0 , jac=self.grad , hess=BFGS() , \\\n",
    "                            method='trust-constr' , constraints=self.cons , \\\n",
    "                           options={ 'maxiter' : 100000 } )\n",
    "        return self.soln\n",
    "    \n",
    "    def printx( self ) : \n",
    "        if self.soln is None : return \"(no solution)\"\n",
    "        rho = self.soln['x'][:self.IC].reshape((self.I,self.C)).mean( axis=0 )\n",
    "        s = \" ) , ( \".join( [ \"%0.2f , %0.2f\" % ( self.soln['x'][self.IC+c] , rho[c] ) for c in range(self.C) ] )\n",
    "        return \"( %s )\" % s\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Gaussian) Random Coefficients\n",
    "\n",
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random (gaussian) coefficient Logit for this situation is\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\min &\\quad -\\frac{1}{N} \\sum_{i=1}^I \\log \\left( \n",
    "                        \\int e^{ L_i( \\mu + \\sigma v ) } \\phi(v) dv\n",
    "                    \\right) \\\\\n",
    "    \\text{w.r.t.} &\\quad \\mu \\in \\mathbb{R} \\; , \\; \\sigma \\geq 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "where $L_i$ has the same definition as above. Broadly speaking, the derivatives are\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    D^\\mu \n",
    "        &= -\\frac{1}{N} \\sum_{i=1}^I \n",
    "                            \\frac{ \\int e^{ L_i( \\mu + \\sigma v ) } L_i^\\prime( \\mu + \\sigma v ) \\phi(v) dv }\n",
    "                                { \\int e^{ L_i( \\mu + \\sigma v ) } \\phi(v) dv } \n",
    "                    \\\\\n",
    "    D^\\sigma\n",
    "        &= -\\frac{1}{N} \\sum_{i=1}^I \n",
    "                            \\frac{ \\int e^{ L_i( \\mu + \\sigma v ) } L_i^\\prime( \\mu + \\sigma v ) v \\phi(v) dv }\n",
    "                                { \\int e^{ L_i( \\mu + \\sigma v ) } \\phi(v) dv }\n",
    "                    \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "we can approximate these directly, or approximate them by differentiating our approximation to the actual integral. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we approximate the integrals with a sequence of $S$ \"standardized\" samples $v_s$ and associated weights $w_s$ that we hold fix over all individuals: \n",
    "$$\n",
    "    \\int e^{ L_i( \\mu + \\sigma v ) } \\phi(v) dv\n",
    "        \\approx \\sum_{s=1}^S w_s e^{ L_i( \\mu + \\sigma v_s ) }. \n",
    "$$\n",
    "This makes the negative log likelihood approximation \n",
    "$$\n",
    "    -\\frac{1}{N} \\sum_{i=1}^I \\log\\left( \\sum_{s=1}^S w_s e^{ L_i( \\mu + \\sigma v_s ) } \\right) .\n",
    "$$\n",
    "The associated (identically sampled and weighted) derivative approximations are\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    D^\\mu \n",
    "        &= -\\frac{1}{N} \\sum_{i=1}^I \n",
    "                            \\frac{ \\sum_{s=1}^S w_s e^{ L_i( \\mu + \\sigma v_s ) } L_i^\\prime( \\mu + \\sigma v_s ) }\n",
    "                                { \\sum_{s=1}^S  w_s e^{ L_i( \\mu + \\sigma v_s ) }  } \n",
    "                    \\\\\n",
    "    D^\\sigma\n",
    "        &= -\\frac{1}{N} \\sum_{i=1}^I \n",
    "                            \\frac{ \\sum_{s=1}^S w_s e^{ L_i( \\mu + \\sigma v_s ) } L_i^\\prime( \\mu + \\sigma v_s ) v_s}\n",
    "                                { \\sum_{s=1}^S w_s  e^{ L_i( \\mu + \\sigma v_s ) } } \n",
    "                    \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "It is important that we either hold such approximations fixed over the course of an estimation attempt or ensure that the approximation is so good that changing approximation error does not interfere with optimizer progress. \n",
    "\n",
    "Given $\\mathbf{v}$ and $\\mathbf{w}$, we can compute the negative log likelihood $f$ and gradient $\\mathbf{g}$ as\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    & \\\\\n",
    "    &\\quad \\boldsymbol{\\theta} \\leftarrow \\mu + \\sigma \\mathbf{v} \\in \\mathbb{R}^{S} \\\\\n",
    "    &\\quad \\boldsymbol{\\eta} \\leftarrow \\exp\\{ - \\mathbf{y} \\boldsymbol{\\theta}^\\top \\} \\in \\mathbb{R}^{N \\times S} \\\\\n",
    "    &\\quad \\boldsymbol{\\ell} \\leftarrow \\log( 1 + \\boldsymbol{\\eta} ) \\in \\mathbb{R}^{N \\times S} \\\\\n",
    "    &\\quad \\mathbf{L} \\leftarrow - \\mathbf{R}\\boldsymbol{\\ell} \\in \\mathbb{R}^{I \\times S} \\\\\n",
    "    &\\quad \\mathbf{P} \\leftarrow \\boldsymbol{\\eta} \\; / \\; (1+\\boldsymbol{\\eta}) \\in \\mathbb{R}^{N \\times S} \\\\\n",
    "    &\\quad \\mathbf{L}^\\prime \\leftarrow \\mathbf{R}(\\mathrm{diag}(\\mathbf{y})\\mathbf{P}) \\in \\mathbb{I \\times S} \\\\\n",
    "    &\\quad \\mathbf{E}_1 \\leftarrow \\exp\\{ \\mathbf{L} \\} \\in \\mathbb{R}^{I \\times S} \\\\\n",
    "    &\\quad \\mathbf{E}_2 \\leftarrow \\mathbf{E}_1 * \\mathbf{L}^\\prime \\in \\mathbb{R}^{I \\times S} \\\\\n",
    "    &\\quad\\begin{aligned}\n",
    "        &\\texttt{for } i=1,\\dotsc,I \\\\\n",
    "        &\\quad\\quad \\mathbf{E}_3[i,:] \\leftarrow \\mathbf{E}_2[i,:] * \\mathbf{v}[:] \\\\\n",
    "     \\end{aligned} \\quad\\quad\\text{or}\\quad\\quad\n",
    "     \\begin{aligned}\n",
    "        &\\texttt{for } s=1,\\dotsc,S \\\\\n",
    "        &\\quad\\quad \\mathbf{E}_3[:,s] \\leftarrow \\mathbf{v}[s]\\;\\mathbf{E}_2[:,s] \\\\\n",
    "     \\end{aligned}\n",
    "    \\\\\n",
    "    &\\quad\\begin{aligned}\n",
    "        &\\texttt{for } k=1,2,3 \\\\\n",
    "        &\\quad\\quad \\mathbf{j}_k \\leftarrow \\mathbf{E}_k \\mathbf{w} \\in \\mathbb{R}^{I} \\\\\n",
    "     \\end{aligned}\n",
    "     \\\\\n",
    "    &\\quad f \\leftarrow - \\; \\texttt{sum}( \\; \\log( \\; \\mathbf{j}_1 \\; ) \\; ) \\; / \\; N \\\\\n",
    "    &\\quad \\mathbf{g} \\leftarrow \n",
    "        - \\frac{1}{N} \\begin{pmatrix} \n",
    "            \\texttt{sum}( \\; \\mathbf{j}_2 \\; / \\; \\mathbf{j}_1 \\; ) \\\\\n",
    "            \\texttt{sum}( \\; \\mathbf{j}_3 \\; / \\; \\mathbf{j}_1 \\; ) \\\\\n",
    "        \\end{pmatrix}\n",
    "     \\\\\n",
    "     & \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "After defining a class for this approach we review two ways of computing sample points and weights: sample average approximations and Gauss-Legendre quadrature. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stabilization\n",
    "\n",
    "We have to again consider our stabilization. If $\\max_s L_i( \\mu + \\sigma v_s ) \\ll 0$, then $e^{ L_i( \\mu + \\sigma v_s ) } \\approx 0$; if the latter holds, the resulting $\\log$ will fail. Specifically, if \n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\\\\n",
    "    \\max_s L_i( \\mu + \\sigma v_s ) \\leq -745.15\n",
    "    \\quad\\quad\\text{then}\\quad\\quad\n",
    "    \\texttt{float}( e^{ L_i( \\mu + \\sigma v_s ) } ) = 0\n",
    "    \\\\\n",
    "    \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "where $\\texttt{float}$ is the floating point representation of the exponential (in double precision). This has to be handled carefully if we want to allow for sufficient exploration of $\\mu,\\sigma$ for arbitrary data. \n",
    "\n",
    "As above, let \n",
    "$$\n",
    "L_i^*(\\mu,\\sigma) = \\max_s L_i( \\mu + \\sigma v_s )\n",
    "$$\n",
    "and note that\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    -\\frac{1}{N} \\sum_{i=1}^I \\log\\left( \\sum_{s=1}^S w_s e^{ L_i( \\mu + \\sigma v_s ) } \\right) \n",
    "        &= -\\frac{1}{N} \\sum_{i=1}^I \\log\\left( e^{L_i^*(\\mu,\\sigma)} \\sum_{s=1}^S w_s e^{ L_i( \\mu + \\sigma v_s ) - L_i^*(\\mu,\\sigma) } \\right) \\\\\n",
    "        &= -\\frac{1}{N} \\sum_{i=1}^I L_i^*(\\mu,\\sigma) - \\frac{1}{N} \\sum_{i=1}^I \\log\\left( \\sum_{s=1}^S w_s e^{ L_i( \\mu + \\sigma v_s ) - L_i^*(\\mu,\\sigma) } \\right) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "Here at least one of the exponents, for any $i$, is zero by definition and thus its corresponding exponentiated value is one. Thus the sum is at least as large as the smallest weight, which is nonzero if we carefully construct the weights. \n",
    "\n",
    "If we want, we can absorb the weights too: \n",
    "$$\n",
    "\\begin{aligned}\n",
    "    -\\frac{1}{N} \\sum_{i=1}^I \\log\\left( \\sum_{s=1}^S w_s e^{ L_i( \\mu + \\sigma v_s ) } \\right) \n",
    "        &= -\\frac{1}{N} \\sum_{i=1}^I \\log\\left( \\sum_{s=1}^S e^{ W_{i,s}( \\mu + \\sigma v_s ) } \\right) \n",
    "            &&\\quad W_{i,s}(\\theta) = L_i(\\theta) + \\log w_s\\\\\n",
    "        &= -\\frac{1}{N} \\sum_{i=1}^I \\log\\left( e^{W_i^*(\\mu,\\sigma)} \\sum_{s=1}^S e^{ W_{i,s}( \\mu + \\sigma v_s ) - W_i^*(\\mu,\\sigma) } \\right) \n",
    "            &&\\quad W_i^*(\\mu,\\sigma) = \\max_s W_{i,s}(\\mu+\\sigma v_s) \\\\\n",
    "        &= -\\frac{1}{N} \\sum_{i=1}^I W_i^*(\\mu,\\sigma) - \\frac{1}{N} \\sum_{i=1}^I \\log\\left( \\sum_{s=1}^S e^{ W_{i,s}( \\mu + \\sigma v_s ) - W_i^*(\\mu,\\sigma) } \\right) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "This form would ensure that the $\\log$ is of a term that is always greater than one. \n",
    "\n",
    "Presuming the first form, the associated derivative approximations can be evaluated the same way with shifted values for $L_i(\\mu+\\sigma v_s)$: \n",
    "$$\n",
    "\\begin{aligned}\n",
    "    D^\\mu \n",
    "        &= -\\frac{1}{N} \\sum_{i=1}^I \n",
    "                            \\frac{ \\sum_{s=1}^S w_s e^{ L_i( \\mu + \\sigma v_s ) } L_i^\\prime( \\mu + \\sigma v_s ) }\n",
    "                                { \\sum_{s=1}^S  w_s e^{ L_i( \\mu + \\sigma v_s ) }  } \n",
    "                    \\\\\n",
    "        &= -\\frac{1}{N} \\sum_{i=1}^I \n",
    "                            \\frac{ e^{L_i^*(\\mu,\\sigma)} \\sum_{s=1}^S w_s e^{ L_i( \\mu + \\sigma v_s ) - L_i^*(\\mu,\\sigma) } L_i^\\prime( \\mu + \\sigma v_s ) }\n",
    "                                { e^{L_i^*(\\mu,\\sigma)} \\sum_{s=1}^S  w_s e^{ L_i( \\mu + \\sigma v_s ) - L_i^*(\\mu,\\sigma) }  } \n",
    "                    \\\\\n",
    "        &= -\\frac{1}{N} \\sum_{i=1}^I \n",
    "                            \\frac{ \\sum_{s=1}^S w_s e^{ L_i( \\mu + \\sigma v_s ) - L_i^*(\\mu,\\sigma) } L_i^\\prime( \\mu + \\sigma v_s ) }\n",
    "                                { \\sum_{s=1}^S  w_s e^{ L_i( \\mu + \\sigma v_s ) - L_i^*(\\mu,\\sigma) }  } \n",
    "                    \\\\\n",
    "    D^\\sigma\n",
    "        &= -\\frac{1}{N} \\sum_{i=1}^I \n",
    "                            \\frac{ \\sum_{s=1}^S w_s e^{ L_i( \\mu + \\sigma v_s ) } L_i^\\prime( \\mu + \\sigma v_s ) v_s}\n",
    "                                { \\sum_{s=1}^S w_s  e^{ L_i( \\mu + \\sigma v_s ) } } \n",
    "                    \\\\\n",
    "        &= -\\frac{1}{N} \\sum_{i=1}^I \n",
    "                            \\frac{ e^{L_i^*(\\mu,\\sigma)} \\sum_{s=1}^S w_s e^{ L_i( \\mu + \\sigma v_s ) - L_i^*(\\mu,\\sigma) } L_i^\\prime( \\mu + \\sigma v_s ) v_s}\n",
    "                                { e^{L_i^*(\\mu,\\sigma)} \\sum_{s=1}^S w_s  e^{ L_i( \\mu + \\sigma v_s ) - L_i^*(\\mu,\\sigma) } } \n",
    "                    \\\\\n",
    "        &= -\\frac{1}{N} \\sum_{i=1}^I \n",
    "                            \\frac{ \\sum_{s=1}^S w_s e^{ L_i( \\mu + \\sigma v_s ) - L_i^*(\\mu,\\sigma) } L_i^\\prime( \\mu + \\sigma v_s ) v_s}\n",
    "                                { \\sum_{s=1}^S w_s  e^{ L_i( \\mu + \\sigma v_s ) - L_i^*(\\mu,\\sigma) } } \n",
    "                    \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RCLogit( FittableModel ) : \n",
    "    \n",
    "    type = \"Random Coefficient Logit\"\n",
    "    \n",
    "    def __init__( self , N , I , i , y ) : \n",
    "        \n",
    "        self.Nvars , self.N , self.I , self.i , self.y = 2 , N , I , i , -y\n",
    "        \n",
    "        # these matrices \"assign\" observations to individuals, facilitating \n",
    "        # sums over observations within individuals. \n",
    "        self.reducer = csr_matrix( (-np.ones(N),(i,np.arange(N))) , shape=(I,N) )\n",
    "        self.yeducer = csr_matrix( (y,(i,np.arange(N))) , shape=(I,N) ) \n",
    "        \n",
    "        # constraints: sigma is non-negative    \n",
    "        self.cons = LinearConstraint( np.array([[0.0,1.0]]) , np.array([0.0]) , np.array([np.inf]) )\n",
    "        \n",
    "        self.S = 0\n",
    "    \n",
    "    def allocate_workspace( self ) : \n",
    "        if self.S > 0 :\n",
    "            self.eU = np.zeros((self.N,self.S),dtype=np.float) # N x S space\n",
    "            self.ll = np.zeros((self.N,self.S),dtype=np.float) # N x S space\n",
    "            self.Li = np.zeros((self.I,self.S),dtype=np.float) # I x S space\n",
    "            self.PL = np.zeros((self.N,self.S),dtype=np.float) # I x S space\n",
    "            self.LP = np.zeros((self.I,self.S),dtype=np.float) # I x S space\n",
    "            self.E  = np.zeros((self.I,self.S),dtype=np.float) # I x S space\n",
    "            self.j  = np.zeros((self.I,3),dtype=np.float) # I x 3 space\n",
    "            self.LM = np.zeros((self.I,),dtype=np.float) # I space\n",
    "        return\n",
    "    \n",
    "    def basics( self , p ) : \n",
    "        theta = p[0] + p[1] * self.v \n",
    "        np.exp( np.outer( self.y , theta ) , out=self.eU )\n",
    "        np.log1p( self.eU , out=self.ll )\n",
    "        self.Li = self.reducer @ self.ll\n",
    "        \n",
    "        # stabilization\n",
    "        self.LM = np.max( self.Li , axis=1 ) # get largest Li[i,:] -> LM[i]\n",
    "        self.Li = self.Li - np.tile( self.LM.reshape((self.I,1)) , (1,self.S) ) # subtract max from each Li\n",
    "        np.exp( self.Li , self.E ) # E[i,:] <- exp{ Li[i,:] - LM[i] }\n",
    "        self.j[:,0] = self.E @ self.w # each term is at least as large as the minimum weight\n",
    "        \n",
    "    def obj( self , p ) :\n",
    "        self.basics( p ) # this updates things used in both obj and grad\n",
    "        np.log( self.j[:,0] , out=self.j[:,1] ) # log only the shifted terms\n",
    "        return - np.sum( self.LM + self.j[:,1] ) / self.N # have to add in the max terms\n",
    "\n",
    "    def grad( self , p ) :  \n",
    "        \n",
    "        self.basics( p ) # this updates things used in both obj and grad\n",
    "        \n",
    "        np.divide( self.eU , 1.0 + self.eU , out=self.PL )\n",
    "        self.LP = self.yeducer @ self.PL # checked, this is doing the intended sub-sums\n",
    "        np.multiply( self.E , self.LP , out=self.E )\n",
    "        self.j[:,1] = self.E @ self.w\n",
    "        \n",
    "        # if we would guess a loop over i is more efficient, otherwise, loop over s\n",
    "        if self.I <= self.S : \n",
    "            self.E = np.apply_along_axis( lambda e : e * self.v , 1 , self.E )\n",
    "        else : \n",
    "            for s in range(self.S) : self.E[:,s] = self.E[:,s] * self.v[s]\n",
    "        self.j[:,2] = self.E @ self.w\n",
    "        \n",
    "        np.divide( self.j[:,1] , self.j[:,0] , out=self.j[:,1] )\n",
    "        np.divide( self.j[:,2] , self.j[:,0] , out=self.j[:,2] )\n",
    "        \n",
    "        g = np.zeros(2)\n",
    "        g[0] = - np.sum( self.j[:,1] ) / self.N\n",
    "        g[1] = - np.sum( self.j[:,2] ) / self.N\n",
    "        \n",
    "        return g\n",
    "    \n",
    "    def draw_initial_condition( self ) : \n",
    "        p0 = randn( self.Nvars )\n",
    "        p0[1] = np.abs(p0[1])\n",
    "        return p0\n",
    "        \n",
    "    def solve( self , p0=None ) : \n",
    "        self.soln = minimize( self.obj , p0 , jac=self.grad , hess=BFGS() , \\\n",
    "                            method='trust-constr' , constraints=self.cons , \\\n",
    "                           options={ 'maxiter' : 100000 } )\n",
    "        return self.soln\n",
    "    \n",
    "    def printx( self ) : \n",
    "        if self.soln is None : return \"(no solution)\"\n",
    "        return \"%0.2f ( %0.2f )\" % ( self.soln['x'][0] , self.soln['x'][1] )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Average Approximations\n",
    "\n",
    "The simplest integral approximation comes from just sample averaging: where we draw $S$ samples $v_s$ from a standard gaussian distribution and use weights $w_s = 1/S$ or maybe even $w_s = \\phi(v_s)$. This is easy, but is also probably the least efficient approximation we can use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class GRCLogitSAA( RCLogit ) : \n",
    "        \n",
    "    type = \"Gaussian RC Logit (SAA)\"\n",
    "    \n",
    "    def __init__( self , N , I , i , y , samples=1000 , weighted=False ) : \n",
    "        \n",
    "        # initialize the superclass\n",
    "        RCLogit.__init__( self , N , I , i , y )\n",
    "        \n",
    "        # if we change parameters from now on, redo\n",
    "        self.do_update_data = False\n",
    "        \n",
    "        # should we PDF-weight the samples? \n",
    "        self.weighted = True if weighted else False # \"truthy\" filter for this param\n",
    "        \n",
    "        # get samples\n",
    "        self.resample( samples=samples )\n",
    "        \n",
    "        # allocate workspace\n",
    "        self.allocate_workspace()\n",
    "        \n",
    "        # if we change parameters from now on, redo\n",
    "        self.do_update_data = True\n",
    "        \n",
    "    def resample( self , samples=1000 ) : \n",
    "        \n",
    "        # define basic sampling data: S random normal samples\n",
    "        self.S = samples\n",
    "        self.v = randn( self.S )\n",
    "        \n",
    "        # \"weight\" vector for summing/weighting samples\n",
    "        if self.weighted : \n",
    "            self.w = gaussian.pdf( self.v ) ; self.w = self.w / self.w.sum()\n",
    "        else : \n",
    "            self.w = np.ones( self.S ) / self.S\n",
    "            \n",
    "        if self.do_update_data :\n",
    "            self.allocate_workspace()\n",
    "            \n",
    "        return\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gauss-Legendre Quadrature \n",
    "\n",
    "Divide the real line into $Q+2$ intervals\n",
    "$$\n",
    "    (\\infty,p_1] \\; , \\; (p_1,p_2] \\; , \\; \\dotsc \\; , \\; (p_Q,p_{Q+1}] \\; , \\; (p_{Q+1},\\infty)\n",
    "$$\n",
    "using $Q+1$ points \n",
    "$$\n",
    "    p_1 \\; < \\; p_2 \\; < \\; \\dotsb \\; < \\; p_{Q+1}\n",
    "$$\n",
    "and take \n",
    "$$\n",
    "    \\int e^{ L_i( \\mu + \\sigma v ) }\\phi(v)dv\n",
    "        \\;\\; \\approx \\;\\; \\sum_{q=1}^Q \\int_{p_q}^{p_{q+1}} e^{ L_i( \\mu + \\sigma v ) }\\phi(v)dv\n",
    "$$\n",
    "presuming $p_{1},p_{Q+1}$ are sufficiently large in magnitude so that\n",
    "$$\n",
    "    \\int_{-\\infty}^{p_{1}} e^{ L_i( \\mu + \\sigma v ) }\\phi(v)dv\n",
    "        \\;\\; , \\;\\; \\int_{p_{Q+1}}^{\\infty} e^{ L_i( \\mu + \\sigma v ) }\\phi(v)dv\n",
    "        \\;\\; \\approx \\;\\; 0. \n",
    "$$\n",
    "This allows us to use standard [Gauss-Legendre](https://en.wikipedia.org/wiki/Gaussian_quadrature#Gauss%E2%80%93Legendre_quadrature) approximations\n",
    "$$\n",
    "    \\int_{p_q}^{p_{q+1}} e^{ L_i( \\mu + \\sigma v ) }\\phi(v)dv\n",
    "        \\;\\; \\approx \\;\\; \n",
    "            \\triangle_s \\sum_{k=1}^K \\alpha_k e^{ L_i( \\mu + \\sigma v_{q,k} ) } \\phi(v_{q,k})\n",
    "           \\quad\\quad\n",
    "           v_{q,k} = \\triangle_q \\xi_k + \\Gamma_q\n",
    "$$\n",
    "for some $K$-point integration rule with nodes $\\xi_k \\in [-1,1]$ and weights $\\alpha_k$, where\n",
    "$$\n",
    "    \\triangle_q = \\frac{p_{q+1}-p_{q}}{2}\n",
    "    \\quad\\quad\\text{and}\\quad\\quad\n",
    "    \\Gamma_q = \\frac{p_{q}+p_{q+1}}{2}\n",
    "$$\n",
    "Hence\n",
    "$$\n",
    "       \\int e^{ L_i( \\mu + \\sigma v ) }\\phi(v)dv\n",
    "           \\approx \\sum_{q=1}^Q \\triangle_q \\sum_{k=1}^K \\alpha_k e^{ L_i( \\mu + \\sigma v_{q,k} ) }\n",
    "                    \\phi(v_{q,k})\n",
    "            = \\sum_{s=1}^{QK} w_s e^{ L_i( \\mu + \\sigma v_s ) } \n",
    "$$\n",
    "letting $w_s \\sim \\triangle_q \\alpha_k \\phi(v_s)$ (for an appropriate $s \\to (q,k)$ index transformation). In this way we can absorb these quadrature-specific terms into the weights corresponding to specific samples. \n",
    "\n",
    "To compute, set\n",
    "$$\n",
    "    S = QK\n",
    "    \\quad\\quad\n",
    "    \\mathbf{V} = \\boldsymbol{\\triangle} \\boldsymbol{\\xi}^\\top + \\boldsymbol{\\Gamma}\\mathbf{1}^\\top\n",
    "    \\quad\\quad\n",
    "    \\mathbf{v} = \\mathrm{vec}( \\mathbf{V} )\n",
    "    \\quad\\quad\n",
    "    \\mathbf{w} = \\mathrm{vec}( \\; \\boldsymbol{\\triangle} \\boldsymbol{\\alpha}^\\top * \\phi(\\mathbf{V}) \\; )\n",
    "$$\n",
    "and apply the generic weighted sample formulation above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class GRCLogitGLQ( RCLogit ) : \n",
    "        \n",
    "    type = \"Gaussian RC Logit (GLQ)\"\n",
    "    \n",
    "    def __init__( self , N , I , i , y , quad_order=3 , partition=None ) : \n",
    "    \n",
    "        # initialize the superclass\n",
    "        RCLogit.__init__( self , N , I , i , y )\n",
    "        \n",
    "        # don't update in the routines below\n",
    "        self.do_update_data = False\n",
    "        \n",
    "        # define basic quadrature data\n",
    "        self.quadorder( K=quad_order )\n",
    "        \n",
    "        # define the basic partition data\n",
    "        if partition is None : \n",
    "            self.partition()\n",
    "        else : \n",
    "            if 'min' not in partition : partition['min'] = -3.0\n",
    "            if 'max' not in partition : partition['max'] =  4.0\n",
    "            if 'num' not in partition : partition['num'] =  10\n",
    "            self.partition( pmin=partition['min'] , pmax=partition['max'] , pnum=partition['num'] )\n",
    "        \n",
    "        # define the nodes and weights\n",
    "        self.define_nodes_and_weights()\n",
    "        \n",
    "        # allocate workspace\n",
    "        self.allocate_workspace()\n",
    "        \n",
    "        # if we change parameters from now on, redo\n",
    "        self.do_update_data = True\n",
    "        \n",
    "    def plot_quadrature( self ) :\n",
    "        plt.figure( )\n",
    "        for p in self.qp : plt.plot( [p,p] , [0,1] , '--b' )\n",
    "        plt.plot( self.v , 0.0 * np.ones( self.S ) , '.k' )\n",
    "        plt.plot( self.v , gaussian.pdf( self.v ) , '-k' )\n",
    "        return\n",
    "\n",
    "    def quadorder( self , K=3 ) : \n",
    "        \n",
    "        if K < 1 or K > 5 : \n",
    "            raise ArgumentError( \"only handling quadrature for K in {1,2,3,4,5}\" )\n",
    "            \n",
    "        self.K = K\n",
    "        if K == 1 : \n",
    "            self.xi = np.array( [ 0.0 ] )\n",
    "            self.qw = np.array( [ 2.0 ] )\n",
    "        elif K == 2 :\n",
    "            self.xi = np.array( [ -0.57735 , 0.57735 ] )\n",
    "            self.qw = np.array( [  1.00000 , 1.00000 ] )\n",
    "        elif K == 3 :\n",
    "            self.xi = np.array( [ -0.774597 , 0.000000 , 0.774597 ] )\n",
    "            self.qw = np.array( [  0.555556 , 0.888889 , 0.555556 ] )\n",
    "        elif K == 4 :\n",
    "            self.xi = np.array( [ -0.861136 , -0.339981 , 0.339981 , 0.861136 ] )\n",
    "            self.qw = np.array( [  0.347855 ,  0.652145 , 0.652145 , 0.347855 ] )\n",
    "        else :\n",
    "            self.xi = np.array( [ -0.906180 , -0.538469 , 0.000000 , 0.538469 , 0.906180 ] )\n",
    "            self.qw = np.array( [  0.236927 ,  0.478629 , 0.568889 , 0.478629 , 0.236927 ] )\n",
    "            \n",
    "        if self.do_update_data : \n",
    "            self.define_nodes_and_weights()\n",
    "            self.allocate_workspace()\n",
    "            \n",
    "        return\n",
    "        \n",
    "    def partition( self , pmin=-3.0 , pmax=4.0 , pnum=10 ) : \n",
    "        pdel = ( pmax - pmin ) / pnum\n",
    "        p = np.exp( 0.5 * np.arange( pmin , pmax + 0.5*pdel , pdel ) )\n",
    "        self.qp = np.concatenate( ( -p[::-1] , [0] , p )  )\n",
    "        self.Q = len( self.qp ) - 1\n",
    "        self.delta = ( self.qp[1:] - self.qp[:-1] ) / 2.0 # S-vector, approximation interval radii\n",
    "        self.gamma = ( self.qp[1:] + self.qp[:-1] ) / 2.0 # S-vector, approximation interval midpoints \n",
    "        if self.do_update_data : \n",
    "            self.define_nodes_and_weights()\n",
    "            self.allocate_workspace()\n",
    "        return\n",
    "    \n",
    "    def define_nodes_and_weights( self ) : \n",
    "        self.S = self.Q * self.K\n",
    "        V = np.outer( self.delta , self.xi ) + np.tile( self.gamma.reshape((self.Q,1)) , (1,self.K) )\n",
    "        self.v = V.copy().flatten()\n",
    "        self.w = ( np.outer( self.delta , self.qw ) * gaussian.pdf( V ) ).flatten()\n",
    "        return\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## idLogit\n",
    "\n",
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `idLogit` for this situation is\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\max &\\quad \\frac{1}{N} \\sum_{i=1}^I \\log( 1 + e^{-y_n(\\beta+\\delta_{i_n})} )\n",
    "                    + \\frac{\\Lambda_1}{N} || \\boldsymbol{\\delta} ||_1 \n",
    "                    + \\frac{\\Lambda_2}{2N} || \\boldsymbol{\\delta} ||_2^2 \\\\\n",
    "    \\text{w.r.t.} &\\quad \\beta , \\delta_1, \\dotsc , \\delta_I \\\\\n",
    "    \\text{s.to} &\\quad \\delta_1 + \\dotsb + \\delta_I = 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "or, in smooth NLP form, \n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\max &\\quad \\frac{1}{N} \\sum_{n=1}^N \\log( 1 + e^{-y_n(\\beta+\\delta_{i_n})} )\n",
    "                    + \\frac{\\Lambda_1}{N} \\sum_{i=1}^I s_i\n",
    "                    + \\frac{\\Lambda_2}{2N} || \\boldsymbol{\\delta} ||_2^2 \\\\\n",
    "    \\text{w.r.t.} &\\quad \\beta , \\boldsymbol{\\delta} , \\mathbf{s} \\\\\n",
    "    \\text{s.to} &\\quad \\delta_1 + \\dotsb + \\delta_I = 0 \\\\\n",
    "        &\\quad \\mathbf{s} - \\boldsymbol{\\delta} \\geq \\mathbf{0} \\;\\; , \\;\\; \n",
    "                \\mathbf{s} + \\boldsymbol{\\delta} \\geq \\mathbf{0}\n",
    "\\end{aligned}\n",
    "$$\n",
    "Writing the log likelihood part of the objective as\n",
    "$$\n",
    "    \\frac{1}{N} \\sum_{n=1}^N \\log( 1 + e^{\\mathbf{z}_n^\\top\\mathbf{x}} )\n",
    "    \\quad\\quad \\mathbf{x} = \\begin{pmatrix} \\beta \\\\ \\boldsymbol{\\delta} \\end{pmatrix}\n",
    "$$\n",
    "the gradient is\n",
    "$$\n",
    "    \\begin{pmatrix}\n",
    "        \\frac{1}{N} \\sum_{n=1}^N P_n(\\mathbf{x}) \\mathbf{z}_n\n",
    "        + \\begin{pmatrix} 0 \\\\ \\frac{\\Lambda_2}{N} \\boldsymbol{\\delta} \\end{pmatrix}\n",
    "        \\\\\n",
    "        \\frac{\\Lambda_1}{N} \\mathbf{1}\n",
    "        \\end{pmatrix}\n",
    "        \\quad\\text{where}\\quad\n",
    "        P_n(\\mathbf{x}) = \\frac{ e^{\\mathbf{z}_n^\\top\\mathbf{x}} }{ 1 + e^{\\mathbf{z}_n^\\top\\mathbf{x}} }\n",
    "$$\n",
    "Here the Hessian is even pretty straightforward: The first $1+I$ components are\n",
    "$$\n",
    "    \\frac{1}{N} \\sum_{n=1}^N P_n(\\mathbf{x})(1-P_n(\\mathbf{x})) \\mathbf{z}_n \\mathbf{z}_n^\\top\n",
    "        + \\frac{\\Lambda_2}{N} \\begin{pmatrix} 0 & \\mathbf{0} \\\\ \\mathbf{0} & \\mathbf{I} \\end{pmatrix}\n",
    "$$\n",
    "and there are no \"$\\mathbf{s}$\" components. Thus Hessian-vector products are\n",
    "$$\n",
    "    \\mathbf{H}\\begin{pmatrix}u\\\\\\mathbf{v}\\\\\\mathbf{w}\\end{pmatrix}\n",
    "        = \\begin{pmatrix} \n",
    "            \\frac{1}{N} \\sum_{n=1}^N \\left( \\mathbf{z}_n^\\top\n",
    "                \\begin{pmatrix} u \\\\ \\mathbf{v} \\end{pmatrix} \\right) P_n(\\mathbf{x})(1-P_n(\\mathbf{x})) \\mathbf{z}_n \n",
    "                + \\frac{\\Lambda_2}{N} \\begin{pmatrix} 0 \\\\ \\mathbf{v} \\end{pmatrix}\n",
    "              \\\\\n",
    "             \\mathbf{0}  \n",
    "        \\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class idLogit( FittableModel ) : \n",
    "        \n",
    "    type = \"idLogit\"\n",
    "    \n",
    "    def __init__( self , N , I , i , y , Lambda1=None , Lambda2=None ) : \n",
    "        \n",
    "        self.N , self.I , self.i , self.y = N , I , i , y\n",
    "        self.Nvars , self.Ncons = 1 + 2*I , 1 + 2*I\n",
    "        self.Ip1 = I + 1\n",
    "        \n",
    "        # map of coefficients to \"-y(b+d)\" terms\n",
    "        Znnzs = 2*N\n",
    "        Zdata = np.zeros( Znnzs )\n",
    "        Zrows , Zcols = np.zeros( Znnzs , dtype=np.int ) , np.zeros( Znnzs , dtype=np.int )\n",
    "        \n",
    "        Zdata[:N] , Zdata[N:] = -y , -y\n",
    "        Zrows[:N] , Zrows[N:] = np.arange(N) , np.arange(N) \n",
    "        Zcols[:N] , Zcols[N:] = 0 , 1 + i\n",
    "        \n",
    "        self.Z = csr_matrix( (Zdata,(Zrows,Zcols)) , shape=( N , 1+I ) )\n",
    "        \n",
    "        # spy( self.Z )\n",
    "        \n",
    "        # constraints\n",
    "        \n",
    "        lo , up = np.zeros( self.Ncons ) , np.inf * np.ones( self.Ncons ) ; up[0] = 0.0\n",
    "        \n",
    "        Cnnzs = 5*I\n",
    "        Cdata = np.ones( Cnnzs ) # almost all entries are ones\n",
    "        Crows , Ccols = np.zeros( Cnnzs , dtype=np.int ) , np.zeros( Cnnzs , dtype=np.int )\n",
    "        \n",
    "        Crows[0*I:1*I] , Ccols[0*I:1*I] = 0 , 1 + np.arange(I)\n",
    "        Crows[1*I:2*I] , Ccols[1*I:2*I] , Cdata[1*I:2*I] = 1 + np.arange(I) , 1 + np.arange(I) , -1.0\n",
    "        Crows[2*I:3*I] , Ccols[2*I:3*I] = 1 +     np.arange(I) , 1 + I + np.arange(I)\n",
    "        Crows[3*I:4*I] , Ccols[3*I:4*I] = 1 + I + np.arange(I) , 1 +     np.arange(I)\n",
    "        Crows[4*I:5*I] , Ccols[4*I:5*I] = 1 + I + np.arange(I) , 1 + I + np.arange(I)\n",
    "        \n",
    "        Cmtrx = csr_matrix( (Cdata,(Crows,Ccols)) , shape=( self.Ncons , self.Nvars ) )\n",
    "        \n",
    "        # spy( Cmtrx )\n",
    "            \n",
    "        self.cons = LinearConstraint( Cmtrx , lo , up )\n",
    "        \n",
    "        # initial regularization\n",
    "        L1 = Lambda1 if Lambda1 is not None else self.N\n",
    "        L2 = Lambda2 if Lambda2 is not None else self.N\n",
    "        self.regularize( Lambda1=L1 , Lambda2=L2 )\n",
    "        \n",
    "        self.zp = np.zeros((self.N,),dtype=np.float)\n",
    "        self.eU = np.zeros((self.N,),dtype=np.float)\n",
    "        self.ll = np.zeros((self.N,),dtype=np.float)\n",
    "        self.PL = np.zeros((self.N,),dtype=np.float)\n",
    "\n",
    "    def obj( self , p ) :\n",
    "        np.exp( self.Z @ p[:self.Ip1] , self.eU )\n",
    "        np.log1p( self.eU , self.ll )\n",
    "        return np.sum( self.ll ) / self.N \\\n",
    "                    + self.L1 * np.sum( p[self.Ip1:] ) \\\n",
    "                    + self.L2 * np.sum( p[1:self.Ip1] * p[1:self.Ip1] ) / 2.0\n",
    "        \n",
    "    def grad( self , p ) :  \n",
    "        g = np.zeros( self.Nvars )\n",
    "        np.exp( self.Z @ p[:self.Ip1] , out=self.eU )\n",
    "        np.divide( self.eU , 1.0 + self.eU , out=self.PL )\n",
    "        g[:self.Ip1] = self.Z.T @ self.PL / N\n",
    "        if( self.L2 > 0.0 ) : g[1:self.Ip1] += self.L2 * p[1:self.Ip1]\n",
    "        g[self.Ip1:] = self.L1\n",
    "        return g\n",
    "    \n",
    "    def hessp( self , p , v ) :  \n",
    "        h = np.zeros( self.Nvars )\n",
    "        zv = self.Z @ v[:self.Ip1]\n",
    "        np.exp( self.Z @ p[:self.Ip1] , out=self.eU )\n",
    "        np.divide( self.eU , 1.0 + self.eU , out=self.PL )\n",
    "        self.PL = self.PL * ( 1.0 - self.PL )\n",
    "        h[:self.Ip1] = self.Z.T @ ( zv * self.PL ) / N\n",
    "        if( self.L2 > 0.0 ) : h[1:self.Ip1] += self.L2 * v[1:self.Ip1]\n",
    "        return h\n",
    "    \n",
    "    def regularize( self , Lambda1=None , Lambda2=None ) : \n",
    "        if Lambda1 is not None : self.L1 = Lambda1 / self.N\n",
    "        if Lambda2 is not None : self.L2 = Lambda2 / self.N\n",
    "            \n",
    "    def draw_initial_condition( self ) : \n",
    "        p0 = randn( self.Nvars )\n",
    "        p0[1:] = p0[1:] - p0[1:].mean()\n",
    "        return p0\n",
    "    \n",
    "    def solve( self , p0=None ) : \n",
    "        self.soln = minimize( self.obj , p0 , jac=self.grad , hessp=self.hessp , \\\n",
    "                            method='trust-constr' , constraints=self.cons , \\\n",
    "                           options={ 'maxiter' : 100000 } )\n",
    "        return self.soln\n",
    "    \n",
    "    def printx( self ) : \n",
    "        if self.soln is None : return \"(no solution)\"\n",
    "        return \"%0.2f ( %0.2f )\" % ( self.soln['x'][0] , self.soln['x'][1:].std() )\n",
    "    \n",
    "    def trace( self , p0=None , Lambda1s=None , alpha=None ) : \n",
    "        p0 = self.draw_initial_condition() if p0 is None else p0.flatten()\n",
    "        if Lambda1s is None : Lambda1s = 10.0 ** np.arange(1,10,1)[::-1]\n",
    "        if alpha is None : alpha = 1.0\n",
    "        self.trace = [ None for l in range(Lambda1s.size) ]\n",
    "        for l in range(Lambda1s.size) : \n",
    "            L = Lambda1s[l]\n",
    "            self.regularize( Lambda1=L/self.N , Lambda2=alpha*L/self.N )\n",
    "            self.trace[l] = self.fit( p0=p0 )\n",
    "            p0 = trace[l].x\n",
    "        return self\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generating Processes\n",
    "\n",
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define some simple data generating processes that map $(N,I,\\mathbf{i})$ (along with possibly other parameters) to draws of $\\mathbf{y}$. Below we define functions that will randomly draw from a Logit, a Latent Class Logit, and a (Gaussian) Random Coefficients Logit model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# multinomial Logit data generating process\n",
    "def mnl_dgp( N , I , i ) : \n",
    "    pT = 2.0 * rand() - 1.0\n",
    "    eU = np.exp( pT )\n",
    "    PT = eU / ( 1.0 + eU )\n",
    "    y = 2.0 * ( rand(N) <= PT ) - 1.0\n",
    "    return y\n",
    "\n",
    "# Latent Class Logit data generating process\n",
    "def lcl_dgp( N , I , i , C ) : \n",
    "    w = rand(C) ; w = w / w.sum()\n",
    "    pT = 2.0 * rand(C) - 1.0\n",
    "    ic = randc(C,I,w)\n",
    "    eU = np.exp( pT[ ic[i] ] )\n",
    "    PT = eU / ( 1.0 + eU )\n",
    "    y = 2.0 * ( rand(N) <= PT ) - 1.0\n",
    "    return y\n",
    "\n",
    "# Gaussian Random Coefficient Logit data generating process\n",
    "def grc_dgp( N , I , i ) : \n",
    "    pT = rand(2) ; pT[0] = 2.0 * pT[0] - 1.0\n",
    "    v = pT[0] + pT[1] * randn(I)\n",
    "    eU = np.exp( v[i] )\n",
    "    PT = eU / ( 1.0 + eU )\n",
    "    y = 2.0 * ( rand(N) <= PT ) - 1.0\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation Experiments\n",
    "\n",
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To draw a simulation experiment, we need to choose or draw $(N,I)$, then $i_n \\in \\{1,\\dotsc,I\\}$ for all $n$, and then choice dummies $y_n$ following some data generating process. Below we wrap the second two parts in a single function, `draw_experiment`. \n",
    "\n",
    "Then we create model object instances from the resultant data for each of our model types. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def draw_experiment( N , I , dgp ) : \n",
    "    i = randi( I , N ) ; y = dgp( N , I , i ) ; return i , y\n",
    "\n",
    "N , I = 1000 , 100\n",
    "i , y = draw_experiment( N , I , mnl_dgp )\n",
    "\n",
    "# define simple Logit\n",
    "mnl = Logit( N , y )\n",
    "\n",
    "# define latent class Logit instances, for a specific number of modeled classes\n",
    "M = 3\n",
    "lcl = LatentClassLogit( N , I , i , y , M )\n",
    "ccl = ClassClassLogit( N , I , i , y , M )\n",
    "\n",
    "# define sample average and gauss-legendre random coefficient Logit instances\n",
    "saa = GRCLogitSAA( N , I , i , y )\n",
    "glq = GRCLogitGLQ( N , I , i , y )\n",
    "\n",
    "# finally, define an idLogit instance\n",
    "idl = idLogit( N , I , i , y )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to make sure everything works, we could run a test that just (a) checks the gradient for each model and (b) fits the model on the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logit :: Checking gradient...\n",
      "0.1000000000000000 , 0.0123856314493208\n",
      "0.0100000000000000 , 0.0012419849296684\n",
      "0.0010000000000000 , 0.0001242282188018\n",
      "0.0001000000000000 , 0.0000124231111852\n",
      "0.0000100000000000 , 0.0000012423226680\n",
      "0.0000010000000000 , 0.0000001242170599\n",
      "0.0000001000000000 , 0.0000000109743113\n",
      "0.0000000100000000 , 0.0000000245528254\n",
      "0.0000000010000000 , 0.0000000198560955\n",
      "0.0000000001000000 , 0.0000028667237685\n",
      "Logit :: Fitting model...\n",
      "Logit :: Solver ran in 0.018808 seconds\n",
      "Logit :: Solver message: `gtol` termination condition is satisfied.\n",
      "-0.09\n",
      " \n",
      "Latent Class Logit :: Checking gradient...\n",
      "0.1000000000000000 , 0.0052477196374758\n",
      "0.0100000000000000 , 0.0005597695079021\n",
      "0.0010000000000000 , 0.0000563570940140\n",
      "0.0001000000000000 , 0.0000056395450756\n",
      "0.0000100000000000 , 0.0000005640006464\n",
      "0.0000010000000000 , 0.0000000564510885\n",
      "0.0000001000000000 , 0.0000000061579855\n",
      "0.0000000100000000 , 0.0000000292824487\n",
      "0.0000000010000000 , 0.0000002242530218\n",
      "0.0000000001000000 , 0.0000004580187565\n",
      "Latent Class Logit :: Fitting model...\n",
      "Latent Class Logit :: Solver ran in 0.144738 seconds\n",
      "Latent Class Logit :: Solver message: `gtol` termination condition is satisfied.\n",
      "( -0.02 , 0.85 ) , ( -0.84 , 0.09 ) , ( -0.02 , 0.05 )\n",
      " \n",
      "Class Class Logit :: Checking gradient...\n",
      "0.1000000000000000 , 0.0024475115106288\n",
      "0.0100000000000000 , 0.0002678571682753\n",
      "0.0010000000000000 , 0.0000270005342380\n",
      "0.0001000000000000 , 0.0000027021845723\n",
      "0.0000100000000000 , 0.0000002702476982\n",
      "0.0000010000000000 , 0.0000000270089357\n",
      "0.0000001000000000 , 0.0000000019310792\n",
      "0.0000000100000000 , 0.0000000161831162\n",
      "0.0000000010000000 , 0.0000002594896371\n",
      "0.0000000001000000 , 0.0000015252986984\n",
      "Class Class Logit :: Fitting model...\n",
      "Class Class Logit :: Solver ran in 2.342274 seconds\n",
      "Class Class Logit :: Solver message: `gtol` termination condition is satisfied.\n",
      "( -1.25 , 0.17 ) , ( -0.29 , 0.44 ) , ( 0.64 , 0.39 )\n",
      " \n",
      "Gaussian RC Logit (SAA) :: Checking gradient...\n",
      "0.1000000000000000 , 0.0137590497623035\n",
      "0.0100000000000000 , 0.0017274847612019\n",
      "0.0010000000000000 , 0.0001759934073891\n",
      "0.0001000000000000 , 0.0000176313850047\n",
      "0.0000100000000000 , 0.0000017634691345\n",
      "0.0000010000000000 , 0.0000001764386275\n",
      "0.0000001000000000 , 0.0000000180098019\n",
      "0.0000000100000000 , 0.0000000206141752\n",
      "0.0000000010000000 , 0.0000001871476289\n",
      "0.0000000001000000 , 0.0000020745267708\n",
      "Gaussian RC Logit (SAA) :: Fitting model...\n",
      "Gaussian RC Logit (SAA) :: Solver ran in 0.266794 seconds\n",
      "Gaussian RC Logit (SAA) :: Solver message: `gtol` termination condition is satisfied.\n",
      "-0.10 ( 0.22 )\n",
      " \n",
      "Gaussian RC Logit (GLQ) :: Checking gradient...\n",
      "0.1000000000000000 , 0.0097282565128567\n",
      "0.0100000000000000 , 0.0009745761193782\n",
      "0.0010000000000000 , 0.0000974733405825\n",
      "0.0001000000000000 , 0.0000097474893326\n",
      "0.0000100000000000 , 0.0000009747612331\n",
      "0.0000010000000000 , 0.0000000973741812\n",
      "0.0000001000000000 , 0.0000000104437183\n",
      "0.0000000100000000 , 0.0000000082854227\n",
      "0.0000000010000000 , 0.0000000859388840\n",
      "0.0000000001000000 , 0.0000004190057914\n",
      "Gaussian RC Logit (GLQ) :: Fitting model...\n",
      "Gaussian RC Logit (GLQ) :: Solver ran in 0.056875 seconds\n",
      "Gaussian RC Logit (GLQ) :: Solver message: `gtol` termination condition is satisfied.\n",
      "-0.09 ( 0.22 )\n",
      " \n",
      "idLogit :: Checking gradient...\n",
      "0.1000000000000000 , 0.0502009446337464\n",
      "0.0100000000000000 , 0.0050202333920761\n",
      "0.0010000000000000 , 0.0005020246729142\n",
      "0.0001000000000000 , 0.0000502025696274\n",
      "0.0000100000000000 , 0.0000050205781459\n",
      "0.0000010000000000 , 0.0000005135554159\n",
      "0.0000001000000000 , 0.0000001652652215\n",
      "0.0000000100000000 , 0.0000011322720010\n",
      "0.0000000010000000 , 0.0000129117597934\n",
      "0.0000000001000000 , 0.0001192457686223\n",
      "idLogit :: Fitting model...\n",
      "idLogit :: Solver ran in 0.235876 seconds\n",
      "idLogit :: Solver message: `gtol` termination condition is satisfied.\n",
      "-0.09 ( 0.00 )\n",
      " \n"
     ]
    }
   ],
   "source": [
    "def basic_test( prob ) : \n",
    "    print( \"%s :: Checking gradient...\" % ( prob.type ) )\n",
    "    prob.grad_check(  )\n",
    "    print( \"%s :: Fitting model...\" % ( prob.type ) )\n",
    "    soln = prob.fit().soln\n",
    "    print( \"%s :: Solver ran in %0.6f seconds\" % ( prob.type , soln.solvertime ) )\n",
    "    print( \"%s :: Solver message: %s\" % ( prob.type , soln.message ) )\n",
    "    print( prob.printx() )\n",
    "    \n",
    "basic_test( mnl ) ; print(\" \")\n",
    "basic_test( lcl ) ; print(\" \")\n",
    "basic_test( ccl ) ; print(\" \")\n",
    "basic_test( saa ) ; print(\" \")\n",
    "basic_test( glq ) ; print(\" \")\n",
    "basic_test( idl ) ; print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now that we have some confidence that everything actually works, we can run some real tests. \n",
    "\n",
    "Below we define a class `ModelFitExp` to facilitate running experiments over the sample size ($N$). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_model_object( code , N , I , i , y , data={} ) : \n",
    "    if data is None : data = {} # generic empty object for use \n",
    "    if   code == \"mnl\" : return Logit( N , y[:N] )\n",
    "    elif code == \"lcl\" : return LatentClassLogit( N , I , i[:N] , y[:N] , **data )\n",
    "    elif code == \"ccl\" : return ClassClassLogit( N , I , i[:N] , y[:N] , **data )\n",
    "    elif code == \"saa\" : return GRCLogitSAA( N , I , i[:N] , y[:N] )\n",
    "    elif code == \"glq\" : return GRCLogitGLQ( N , I , i[:N] , y[:N] )\n",
    "    elif code == \"idl\" : return idLogit( N , I , i[:N] , y[:N] , **data )\n",
    "    else : return None\n",
    "    \n",
    "class ModelFitExp( object ) : \n",
    "    \n",
    "    def __init__( self , I=0 , dgp=None , obsvnums=None , timeout=None , models=None , verbose=True ) : \n",
    "        self.I = I \n",
    "        self.dgp = dgp\n",
    "        self.timeout = timeout\n",
    "        self.models = {}\n",
    "        self.verbose = verbose\n",
    "        if models is not None : \n",
    "            for m in models : \n",
    "                self.add_model( m['code'] , data=( m['data'] if 'data' in m else None ) )\n",
    "        self.results = {}\n",
    "        self.Ns = []\n",
    "        if obsvnums is None : \n",
    "            self.Ns = [ int(N) for N in 10.0 ** np.arange( 3 , 6.1 , 0.1 ) ]\n",
    "        else : \n",
    "            self.Ns = obsvnums\n",
    "    \n",
    "    def be_verbose( self ) : \n",
    "        self.verbose = True\n",
    "        \n",
    "    def be_quiet( self ) : \n",
    "        self.verbose = False\n",
    "    \n",
    "    def set_obsnums( self , Nmin , Nmax , Nnum ) : \n",
    "        try : \n",
    "            self.Ns = [ int(N) for N in 10.0 ** np.arange( Nmin , Nmax , (Nmax-Nmin)/Nnum ) ]\n",
    "        except Exception as e : \n",
    "            self.Ns = None\n",
    "            raise e\n",
    "            \n",
    "    def set_dgp( self , dgp ) : \n",
    "        self.dgp = dgp\n",
    "    \n",
    "    def set_timeout( self , timeout ) : \n",
    "        self.timeout = timeout\n",
    "        \n",
    "    def add_model( self , code , data=None ) : \n",
    "        mid = random_string( 16 )\n",
    "        self.models[mid] = { 'code' : code , 'data' : data }\n",
    "        return mid\n",
    "        \n",
    "    def del_model( self , mid ) : \n",
    "        if mid in self.models : \n",
    "            del self.models[mid]\n",
    "            \n",
    "    def clear_models( self ) : \n",
    "        del self.models\n",
    "        self.models = {}\n",
    "        \n",
    "    def reset_results( self ) : \n",
    "        del self.results\n",
    "        self.results = {}\n",
    "\n",
    "    def fit_model( self , rkey , prob , p0=None ) : \n",
    "        \n",
    "        if rkey not in self.results : self.results[ rkey ] = []\n",
    "            \n",
    "        try : \n",
    "            \n",
    "            prob.fit( maxtime=self.timeout , p0=p0 )\n",
    "            if prob.soln is None    : raise Exception( \"indeterminate solver error\" )\n",
    "            if prob.soln['timeout'] : raise Exception( \"solver timeout\" )\n",
    "            if prob.soln['error']   : raise prob.soln['message']\n",
    "            res = { \n",
    "                'N' : prob.N , \n",
    "                'x' : prob.printx() , \n",
    "                't' : prob.soln['solvertime'] , \n",
    "                'e' : prob.soln['error'] , \n",
    "                'm' : prob.soln['message'] , \n",
    "            }\n",
    "            if self.verbose : \n",
    "                print( \"%s :: Solve attempt succeded for N = %i\" % ( prob.type , prob.N ) )\n",
    "            \n",
    "        except Exception as e : \n",
    "            \n",
    "            if self.verbose : \n",
    "                print( \"%s :: solve exception occurred: %s\" % ( prob.type , e ) )\n",
    "            try : \n",
    "                solvertime = prob.soln['solvertime']\n",
    "            except AttributeError as e : \n",
    "                solvertime = None\n",
    "            res = { \n",
    "                'N' : prob.N , \n",
    "                'x' : None , \n",
    "                't' : solvertime , \n",
    "                'e' : True , \n",
    "                'm' : e , \n",
    "            }\n",
    "            \n",
    "        self.results[ rkey ].append( res )\n",
    "        return prob.soln['x0']\n",
    "    \n",
    "    def run( self ) : \n",
    "        i , y = draw_experiment( self.Ns[-1] , self.I , self.dgp )\n",
    "        for rkey in self.models : \n",
    "            p0 = None\n",
    "            for N in self.Ns : \n",
    "                \n",
    "                try :\n",
    "                    obj = create_model_object( self.models[rkey]['code'] , \\\n",
    "                                                  N , self.I , i , y , \\\n",
    "                                                  data=self.models[rkey]['data'] )\n",
    "                except Exception as e : \n",
    "                    print( \"Error creating model object: \" , e )\n",
    "                    obj = None\n",
    "                    \n",
    "                if obj is not None : \n",
    "                    p0 = self.fit_model( rkey , obj , p0=p0 )\n",
    "        \n",
    "    def print_models( self ) : \n",
    "        for mid in self.models : \n",
    "            print( \"(%s) %s %s\" % ( mid , self.models[mid]['code'] , \\\n",
    "                                   str(self.models[mid]['data']) if self.models[mid]['data'] is not None else \"\" ) )\n",
    "        \n",
    "    def print_results( self ) : \n",
    "        for k in self.results : \n",
    "            print( self.models[k]['code'] )\n",
    "            for i in self.results[k] : \n",
    "                if i['e'] : \n",
    "                    print( \"%i , %s\" % ( i['N'] , i['m'] ) )\n",
    "                else :\n",
    "                    print( \"%i , %0.2fs , %s \" % ( i['N'] , i['t'] , i['x'] ) )\n",
    "            print( \" \" )\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logit data\n",
    "\n",
    "Let's take the simplest case in the data, the Logit DGP, but try out each model with more and more data. This will help us _start_ to gauge comparative efficiency. To start, we will apply a one minute (60 second) timeout on the fit attempts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(LrPTwMJnGRdcKUmg) mnl \n",
      "(YcieLBpsILrxtKgf) lcl {'C': 3}\n",
      "(PlcMiGGHRnHYzzQi) glq \n",
      "(EnUNqrzvVbDFbKYW) idl \n",
      "(goZjCILwFOYvVPpY) idl {'Lambda1': 10.0, 'Lambda2': 0.0}\n",
      "(rfOGmBcbMBkEFKyj) idl {'Lambda1': 1.0, 'Lambda2': 0.0}\n",
      "(ShoMYLFLjpYRuHQr) idl {'Lambda1': 0.1, 'Lambda2': 0.0}\n",
      "Logit :: Solve attempt succeded for N = 1000\n",
      "Logit :: Solve attempt succeded for N = 1584\n",
      "Logit :: Solve attempt succeded for N = 2511\n",
      "Logit :: Solve attempt succeded for N = 3981\n",
      "Logit :: Solve attempt succeded for N = 6309\n",
      "Latent Class Logit :: Solve attempt succeded for N = 1000\n",
      "Latent Class Logit :: Solve attempt succeded for N = 1584\n",
      "Latent Class Logit :: Solve attempt succeded for N = 2511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/morrowwr/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:62: RuntimeWarning: invalid value encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent Class Logit :: solve exception occurred: solver timeout\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/morrowwr/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:62: RuntimeWarning: invalid value encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent Class Logit :: Solve attempt succeded for N = 6309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/morrowwr/anaconda/lib/python3.6/site-packages/scipy/optimize/_hessian_update_strategy.py:187: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  'approximations.', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian RC Logit (GLQ) :: Solve attempt succeded for N = 1000\n",
      "Gaussian RC Logit (GLQ) :: Solve attempt succeded for N = 1584\n",
      "Gaussian RC Logit (GLQ) :: Solve attempt succeded for N = 2511\n",
      "Gaussian RC Logit (GLQ) :: Solve attempt succeded for N = 3981\n",
      "Gaussian RC Logit (GLQ) :: Solve attempt succeded for N = 6309\n",
      "idLogit :: Solve attempt succeded for N = 1000\n",
      "idLogit :: Solve attempt succeded for N = 1584\n",
      "idLogit :: Solve attempt succeded for N = 2511\n",
      "idLogit :: Solve attempt succeded for N = 3981\n",
      "idLogit :: Solve attempt succeded for N = 6309\n",
      "idLogit :: Solve attempt succeded for N = 1000\n",
      "idLogit :: Solve attempt succeded for N = 1584\n",
      "idLogit :: Solve attempt succeded for N = 2511\n",
      "idLogit :: Solve attempt succeded for N = 3981\n",
      "idLogit :: Solve attempt succeded for N = 6309\n",
      "idLogit :: Solve attempt succeded for N = 1000\n",
      "idLogit :: Solve attempt succeded for N = 1584\n",
      "idLogit :: Solve attempt succeded for N = 2511\n",
      "idLogit :: Solve attempt succeded for N = 3981\n",
      "idLogit :: Solve attempt succeded for N = 6309\n",
      "idLogit :: Solve attempt succeded for N = 1000\n",
      "idLogit :: Solve attempt succeded for N = 1584\n",
      "idLogit :: Solve attempt succeded for N = 2511\n",
      "idLogit :: Solve attempt succeded for N = 3981\n",
      "idLogit :: Solve attempt succeded for N = 6309\n",
      "mnl\n",
      "1000 , 0.04s , 0.34 \n",
      "1584 , 0.04s , 0.36 \n",
      "2511 , 0.03s , 0.29 \n",
      "3981 , 0.03s , 0.32 \n",
      "6309 , 0.04s , 0.33 \n",
      " \n",
      "lcl\n",
      "1000 , 0.09s , ( 0.34 , 0.63 ) , ( 0.34 , 0.29 ) , ( 0.34 , 0.08 ) \n",
      "1584 , 0.11s , ( 0.36 , 0.60 ) , ( 0.36 , 0.30 ) , ( 0.36 , 0.09 ) \n",
      "2511 , 0.13s , ( 0.29 , 0.59 ) , ( 0.29 , 0.31 ) , ( 0.29 , 0.10 ) \n",
      "3981 , solver timeout\n",
      "6309 , 0.27s , ( 0.33 , 0.55 ) , ( 0.33 , 0.39 ) , ( 0.33 , 0.07 ) \n",
      " \n",
      "glq\n",
      "1000 , 0.14s , 0.34 ( 0.00 ) \n",
      "1584 , 0.13s , 0.36 ( 0.00 ) \n",
      "2511 , 0.13s , 0.29 ( 0.00 ) \n",
      "3981 , 0.18s , 0.32 ( 0.00 ) \n",
      "6309 , 0.21s , 0.33 ( 0.00 ) \n",
      " \n",
      "idl\n",
      "1000 , 0.22s , 0.34 ( 0.00 ) \n",
      "1584 , 0.21s , 0.36 ( 0.00 ) \n",
      "2511 , 0.18s , 0.29 ( 0.00 ) \n",
      "3981 , 0.25s , 0.32 ( 0.00 ) \n",
      "6309 , 0.20s , 0.33 ( 0.00 ) \n",
      " \n",
      "idl\n",
      "1000 , 0.21s , 0.34 ( 0.00 ) \n",
      "1584 , 0.23s , 0.36 ( 0.00 ) \n",
      "2511 , 0.53s , 0.29 ( 0.07 ) \n",
      "3981 , 0.95s , 0.32 ( 0.13 ) \n",
      "6309 , 1.05s , 0.33 ( 0.14 ) \n",
      " \n",
      "idl\n",
      "1000 , 0.55s , 0.35 ( 0.29 ) \n",
      "1584 , 1.04s , 0.38 ( 0.34 ) \n",
      "2511 , 1.41s , 0.29 ( 0.30 ) \n",
      "3981 , 0.76s , 0.32 ( 0.26 ) \n",
      "6309 , 2.08s , 0.33 ( 0.21 ) \n",
      " \n",
      "idl\n",
      "1000 , 1.01s , 0.40 ( 0.63 ) \n",
      "1584 , 1.03s , 0.39 ( 0.46 ) \n",
      "2511 , 1.30s , 0.29 ( 0.34 ) \n",
      "3981 , 1.57s , 0.33 ( 0.28 ) \n",
      "6309 , 0.84s , 0.33 ( 0.21 ) \n",
      " \n"
     ]
    }
   ],
   "source": [
    "\n",
    "mdls = [\n",
    "    { 'code' : 'mnl' , 'data' : None } , \n",
    "    { 'code' : 'lcl' , 'data' : { 'C' : 3 } } , \n",
    "    { 'code' : 'glq' , 'data' : None } , \n",
    "    { 'code' : 'idl' , 'data' : None } , \n",
    "    { 'code' : 'idl' , 'data' : { 'Lambda1' : 10.0 , 'Lambda2' : 0.0 } } , \n",
    "    { 'code' : 'idl' , 'data' : { 'Lambda1' :  1.0 , 'Lambda2' : 0.0 } } , \n",
    "    { 'code' : 'idl' , 'data' : { 'Lambda1' :  0.1 , 'Lambda2' : 0.0 } } , \n",
    "]\n",
    "fitr = ModelFitExp( I=100 , dgp=mnl_dgp , timeout=60 , models=mdls )\n",
    "fitr.set_obsnums( 3 , 4 , 5 )\n",
    "fitr.print_models()\n",
    "\n",
    "fitr.run()\n",
    "fitr.print_results()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
